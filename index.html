<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://example.com">
  <title>Hongtu Liu</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hongtu Liu">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hongtu Liu">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hongtu Liu">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="Hongtu Liu" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(200deg,#a0cfe4,#e8c37e);
    }
  </style>
  

  

<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/"></a></h1>
		</hgroup>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">Home</a></li>
	        
				<li><a href="/categories">Categories</a></li>
	        
				<li><a href="/tags/%E9%9A%8F%E7%AC%94/">随笔</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">All Items</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">Links</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">About Me</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="#" title="github"><i class="icon-github"></i></a>
		        
					<a class="weibo" target="_blank" href="#" title="weibo"><i class="icon-weibo"></i></a>
		        
					<a class="rss" target="_blank" href="#" title="rss"><i class="icon-rss"></i></a>
		        
					<a class="zhihu" target="_blank" href="#" title="zhihu"><i class="icon-zhihu"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author"></h1>
			</hgroup>
			
			
			
				
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="#" title="github"><i class="icon-github"></i></a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo"><i class="icon-weibo"></i></a>
			        
						<a class="rss" target="_blank" href="#" title="rss"><i class="icon-rss"></i></a>
			        
						<a class="zhihu" target="_blank" href="#" title="zhihu"><i class="icon-zhihu"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 70%">
				
				
					<li style="width: 33.333333333333336%"><a href="/">Home</a></li>
		        
					<li style="width: 33.333333333333336%"><a href="/categories">Categories</a></li>
		        
					<li style="width: 33.333333333333336%"><a href="/tags/%E9%9A%8F%E7%AC%94/">随笔</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            
  
    <article id="post-20220324" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/03/24/20220324/">20220324</a>
    </h1>
  

        
        <a href="/2022/03/24/20220324/" class="archive-article-date">
  	<time datetime="2022-03-23T16:00:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>20220324</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
    

      
<div id="toc" class="toc-article">
  <strong class="toc-title">Contents</strong>
  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Working-directories"><span class="toc-number">1.</span> <span class="toc-text">Working directories</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Deepin"><span class="toc-number">2.</span> <span class="toc-text">Deepin</span></a></li></ol>
</div>

        <h1 id="Working-directories"><a href="#Working-directories" class="headerlink" title="Working directories"></a>Working directories</h1><h1 id="Deepin"><a href="#Deepin" class="headerlink" title="Deepin"></a>Deepin</h1><p>为了QQ实在不胜其烦。尤其是Deepin的那些东西。只好删除，使用原生的Linux QQ。</p>
<p><code>sudo apt purge *deepin-wine*</code></p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/Log//" class="article-tag-list-link color4">Log</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2022/03/24/20220324/">Expand all items >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-20220320" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/03/20/20220320/">20220320</a>
    </h1>
  

        
        <a href="/2022/03/20/20220320/" class="archive-article-date">
  	<time datetime="2022-03-19T16:00:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>20220320</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
    

      
<div id="toc" class="toc-article">
  <strong class="toc-title">Contents</strong>
  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Working-directories"><span class="toc-number">1.</span> <span class="toc-text">Working directories</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#GoldenDict"><span class="toc-number">1.1.</span> <span class="toc-text">GoldenDict</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#img2pdf-with-xargs"><span class="toc-number">2.</span> <span class="toc-text">img2pdf with xargs</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#command-line-Convert-a-directory-of-JPEG-files-to-a-single-PDF-document-Ask-Ubuntu"><span class="toc-number">2.1.</span> <span class="toc-text">command line - Convert a directory of JPEG files to a single PDF document - Ask Ubuntu</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#img2pdf-get-pages-in-good-order"><span class="toc-number">2.2.</span> <span class="toc-text">img2pdf: get pages in good order</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ref"><span class="toc-number">2.3.</span> <span class="toc-text">Ref</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Telomere-measurement"><span class="toc-number">3.</span> <span class="toc-text">Telomere measurement</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#BLAST%E7%BB%93%E6%9E%9C%E4%B8%8B%E8%BD%BD%E8%A6%81%E6%B1%82"><span class="toc-number">4.</span> <span class="toc-text">BLAST结果下载要求</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#BLAST%E7%BB%93%E6%9E%9C%E6%B1%87%E6%80%BB%E6%96%87%E4%BB%B6"><span class="toc-number">4.1.</span> <span class="toc-text">BLAST结果汇总文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8%E8%AF%B4%E6%98%8E%E6%96%87%E4%BB%B6%E4%B8%AD%E7%AE%80%E8%BF%B0%E5%A6%82%E4%B8%8B%E4%BF%A1%E6%81%AF%EF%BC%9A"><span class="toc-number">4.2.</span> <span class="toc-text">在说明文件中简述如下信息：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Tips-of-less"><span class="toc-number">5.</span> <span class="toc-text">Tips of less</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#How-to-Use-the-less-Command-on-Linux"><span class="toc-number">6.</span> <span class="toc-text">How to Use the less Command on Linux</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#The-History-of-less"><span class="toc-number">6.1.</span> <span class="toc-text">The History of less</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Why-less-Is-Better-Than-more"><span class="toc-number">6.2.</span> <span class="toc-text">Why less Is Better Than more</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reading-a-File-With-less"><span class="toc-number">6.3.</span> <span class="toc-text">Reading a File With less</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Displaying-Line-Numbers"><span class="toc-number">6.4.</span> <span class="toc-text">Displaying Line Numbers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Searching-in-less"><span class="toc-number">6.5.</span> <span class="toc-text">Searching in less</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Opening-a-File-With-a-Search-Term"><span class="toc-number">6.6.</span> <span class="toc-text">Opening a File With a Search Term</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Navigating-in-Less-The-Most-Useful-Keys"><span class="toc-number">6.7.</span> <span class="toc-text">Navigating in Less: The Most Useful Keys</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Squeeze-Blank-Lines"><span class="toc-number">6.8.</span> <span class="toc-text">Squeeze Blank Lines</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Viewing-Multiple-Files"><span class="toc-number">6.9.</span> <span class="toc-text">Viewing Multiple Files</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Using-Marks"><span class="toc-number">6.10.</span> <span class="toc-text">Using Marks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Using-Piped-Input-with-Less"><span class="toc-number">6.11.</span> <span class="toc-text">Using Piped Input with Less</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Editing-Files-With-less"><span class="toc-number">6.12.</span> <span class="toc-text">Editing Files With less</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#In-Summary"><span class="toc-number">6.13.</span> <span class="toc-text">In Summary</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#QGIS"><span class="toc-number">7.</span> <span class="toc-text">QGIS</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sequin-from-NCBI"><span class="toc-number">8.</span> <span class="toc-text">sequin from NCBI</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CRAN-%E2%80%9Cdevtools%E2%80%9D-installation"><span class="toc-number">9.</span> <span class="toc-text">CRAN “devtools” installation</span></a></li></ol>
</div>

        <h1 id="Working-directories"><a href="#Working-directories" class="headerlink" title="Working directories"></a>Working directories</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># GoldenDict installation in Ubuntu</span><br><span class="line">sudo apt-get install goldendict goldendict-wordnet</span><br><span class="line">sudo apt-get install [ubuntu](https:&#x2F;&#x2F;so.csdn.net&#x2F;so&#x2F;search?q&#x3D;ubuntu&amp;spm&#x3D;1001.2101.3001.7020)-restricted-addons</span><br><span class="line">sudo apt-get install ubuntu-restricted-extras</span><br><span class="line"># </span><br><span class="line">wget -rnp https:&#x2F;&#x2F;downloads.freemdict.com&#x2F;100G_Super_Big_Collection&#x2F;俄语&#x2F;</span><br><span class="line"># to be organized</span><br><span class="line">cd &#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Work&#x2F;References&#x2F;Tech&#x2F;DNA_Amplification&#x2F;RPA&#x2F;Ref&#x2F;1</span><br><span class="line"># </span><br></pre></td></tr></table></figure>

<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo aptitude upgrade --full-resolver</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">open: 40003; closed: 28954; defer: 51; conflict: 191                           oUnable to resolve dependencies for the upgrade: no solution found.</span><br><span class="line">Unable to safely resolve dependencies, try running with --full-resolver.</span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">A_Ligation_Recombinase_Polymerase_Amplification_Assay_for_Rapid_Detection_of_SARS_CoV−2.pdf</span><br><span class="line">Can_we_predict_the_limits_of_SARS_CoV_2_variants_and_their_phenotypic_consequences.pdf</span><br><span class="line">Combining_recombinase_polymerase_amplification_and_DNA_templated_reaction_for_SARS_CoV_2_sensing_with_dual_fluorescence_and_lateral_flow_assay_output_1.pdf</span><br><span class="line">Combining_recombinase_polymerase_amplification_and_DNA_templated_reaction_for_SARS_CoV_2_sensing_with_dual_fluorescence_and_lateral_flow_assay_output.pdf</span><br><span class="line">Covid_19_Researcher_blows_the_whistle_on_data_integrity_issues_in_Pfizer_s_vaccine_trial.pdf</span><br><span class="line">Covid_19_vaccines_In_the_rush_for_regulatory_approval_do_we_need_more_data.pdf</span><br><span class="line">Development_of_a_Directly_Visualized_Recombinase_Polymerase_Amplification–SYBR_Green_I_Method_for_the_Rapid_Detection_of_African_Swine_Fever_Virus.pdf</span><br><span class="line">Development_of_Isothermal_RPA_Assay_for_Rapid_Detection_of_Porcine_Circovirus_Type_2.pdf</span><br><span class="line">Differential_Diagnosis_of_Entamoeba_spp_in_Clinical_Stool_Samples_Using_SYBR_Green_Real_Time_Polymerase_Chain_Reaction.pdf</span><br><span class="line">Editorial_Board_2021_Journal_of_Food_Composition_and_Analysis.pdf</span><br><span class="line">flashcenter_pp_ax_install_cn.exe</span><br><span class="line">Isothermal_Recombinase_Polymerase_Amplification_RPA_of_E_coli_gDNA_in_Commercially_Fabricated_PCB_Based_Microfluidic_Platforms.pdf</span><br><span class="line">Multiple_recombinase_polymerase_amplification_and_low_cost_array_technology_for_the_screening_of_genetically_modified_organisms.pdf</span><br><span class="line">NFX1_Plays_a_Role_in_Human_Papillomavirus_Type_16_E6_Activation_of_NFκB_Activity.pdf</span><br><span class="line">Prospective_postmortem_evaluation_of_735_consecutive_SARS_CoV_2_associated_death_cases.pdf</span><br><span class="line">r0_0_4000_2248_w1200_h678_fmax.jpg</span><br><span class="line">Rapid_isothermal_duplex_real_time_RPA_assay_for_the_diagnosis_of_equine_piroplasmosis.pdf</span><br><span class="line">RPA_PCR_couple_an_approach_to_expedite_plant_diagnostics_and_overcome_PCR_inhibitors.pdf</span><br><span class="line">Triterpenoids_phenolic_compounds_macro__and_microelements_in_anatomical_parts_of_sea_buckthorn_Hippopha_̈e_rhamnoides_L_berries_branches_and_leaves.pdf</span><br></pre></td></tr></table></figure>
<h2 id="GoldenDict"><a href="#GoldenDict" class="headerlink" title="GoldenDict"></a>GoldenDict</h2><p><code>magnet:?xt=urn:btih:477f73217801f21bf8ed499aa431eb9641ba789e&amp;dn=GoldenDict</code></p>
<h1 id="img2pdf-with-xargs"><a href="#img2pdf-with-xargs" class="headerlink" title="img2pdf with xargs"></a>img2pdf with xargs</h1><h2 id="command-line-Convert-a-directory-of-JPEG-files-to-a-single-PDF-document-Ask-Ubuntu"><a href="#command-line-Convert-a-directory-of-JPEG-files-to-a-single-PDF-document-Ask-Ubuntu" class="headerlink" title="command line - Convert a directory of JPEG files to a single PDF document - Ask Ubuntu"></a><a target="_blank" rel="noopener" href="https://askubuntu.com/questions/246647/convert-a-directory-of-jpeg-files-to-a-single-pdf-document">command line - Convert a directory of JPEG files to a single PDF document - Ask Ubuntu</a></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">convert *.jpg -auto-orient pictures.pdf</span><br></pre></td></tr></table></figure>
<p>Unfortunately, <strong><code>convert</code></strong> changes the image quality before “packing it” into the PDF. So, to have minimal loss of quality, is better to put the original <code>jpg</code>, (<em><strong>works with <code>.png</code> too</strong></em>) into the PDF, you need to use <code>img2pdf</code>.</p>
<p>I use these commands:</p>
<p>A shorter <strong>one liner solution</strong> also using <code>img2pdf</code> as suggested in the comments**</p>
<ol>
<li><p>Make PDF</p>
<p> <code>img2pdf *.jp* --output combined.pdf</code></p>
</li>
<li><p>(optional) OCR the output PDF</p>
<p> <code>ocrmypdf combined.pdf combined_ocr.pdf</code></p>
</li>
</ol>
<hr>
<p>Below is the <strong>original</strong> answer commands with more command and more tools needed:</p>
<hr>
<ol>
<li><p>This command is to make a <code>pdf</code> file out of every <code>jpg</code> image without loss of either resolution or quality:</p>
<p> <code>ls -1 ./*jpg | xargs -L1 -I &#123;&#125; img2pdf &#123;&#125; -o &#123;&#125;.pdf</code></p>
</li>
<li><p>This command will concatenate the <code>pdf</code>pages into one document:</p>
<p> <code>pdftk *.pdf cat output combined.pdf</code></p>
</li>
<li><p>And finally, I add an OCRed text layer that doesn’t change the quality of the scan in the pdfs so that they can be searchable:</p>
<p> <code>pypdfocr combined.pdf</code></p>
</li>
</ol>
<p>An alternative to using <code>pypdfocr</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#96;ocrmypdf combined.pdf combined_ocr.pdf&#96;  </span><br></pre></td></tr></table></figure>
<h2 id="img2pdf-get-pages-in-good-order"><a href="#img2pdf-get-pages-in-good-order" class="headerlink" title="img2pdf: get pages in good order"></a><a target="_blank" rel="noopener" href="https://getridbug.com/unix-linux/img2pdf-get-pages-in-good-order/">img2pdf: get pages in good order</a></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img2pdf *.tif -o out.pdf</span><br></pre></td></tr></table></figure>
<h2 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h2><ul>
<li><a target="_blank" rel="noopener" href="https://askubuntu.com/tags">Tags - Ask Ubuntu</a></li>
<li><a target="_blank" rel="noopener" href="https://getridbug.com/unix-linux/img2pdf-get-pages-in-good-order/">img2pdf: get pages in good order</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/8955425/how-can-i-convert-a-series-of-images-to-a-pdf-from-the-command-line-on-linux">bash - How can I convert a series of images to a PDF from the command line on linux? - Stack Overflow</a></li>
</ul>
<h1 id="Telomere-measurement"><a href="#Telomere-measurement" class="headerlink" title="Telomere measurement"></a>Telomere measurement</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/labs/pmc/articles/PMC4292845/">Telomere Length: A Review of Methods for Measurement</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/labs/pmc/articles/PMC6363640/">Telomere length measurement by qPCR – Summary of critical factors and recommendations for assay design</a></li>
<li><a target="_blank" rel="noopener" href="https://biologicalproceduresonline.biomedcentral.com/articles/10.1186/1480-9222-13-3">A quantitative PCR method for measuring absolute telomere length | Biological Procedures Online | Full Text</a></li>
<li><a target="_blank" rel="noopener" href="https://www.frontiersin.org/articles/10.3389/fcell.2020.00493/full">Frontiers | Telomere Length Measurement by Molecular Combing | Cell and Developmental Biology</a></li>
<li><a target="_blank" rel="noopener" href="https://royalsocietypublishing.org/doi/10.1098/rstb.2016.0451">Comparison of telomere length measurement methods | Philosophical Transactions of the Royal Society B: Biological Sciences</a></li>
<li><a target="_blank" rel="noopener" href="https://pubs.acs.org/doi/10.1021/acsomega.0c03246#">One-Step High-Throughput Telomerase Activity Measurement of Cell Populations, Single Cells, and Single-Enzyme Complexes | ACS Omega</a></li>
</ul>
<h1 id="BLAST结果下载要求"><a href="#BLAST结果下载要求" class="headerlink" title="BLAST结果下载要求"></a>BLAST结果下载要求</h1><p>比对结果汇总需要包括2类文件</p>
<ol>
<li>说明文件，如README.txt等</li>
<li>BLAST结果汇总文件</li>
</ol>
<h2 id="BLAST结果汇总文件"><a href="#BLAST结果汇总文件" class="headerlink" title="BLAST结果汇总文件"></a>BLAST结果汇总文件</h2><ul>
<li>Text格式结果，该格式的文件头具有较好的信息汇总，需要下载。同时，Traditional画面的Text格式结果也需要下载。</li>
<li>ASN.1格式结果</li>
<li>CSV格式的Hit-Table汇总结果</li>
<li>SAM格式结果</li>
</ul>
<p>以上共计4种格式，5个文件必须下载。</p>
<hr>
<h2 id="在说明文件中简述如下信息："><a href="#在说明文件中简述如下信息：" class="headerlink" title="在说明文件中简述如下信息："></a>在说明文件中简述如下信息：</h2><ul>
<li>RID号码</li>
<li>Query数据</li>
<li>Subject，即数据库侧，特别是是否指定了数据范围，例如，是否限定了人类相关数据库。</li>
</ul>
<h1 id="Tips-of-less"><a href="#Tips-of-less" class="headerlink" title="Tips of less"></a>Tips of <code>less</code></h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.howtogeek.com/444233/how-to-use-the-less-command-on-linux/">https://www.howtogeek.com/444233/how-to-use-the-less-command-on-linux/</a></li>
</ul>
<h1 id="How-to-Use-the-less-Command-on-Linux"><a href="#How-to-Use-the-less-Command-on-Linux" class="headerlink" title="How to Use the less Command on Linux"></a><a target="_blank" rel="noopener" href="https://www.howtogeek.com/444233/how-to-use-the-less-command-on-linux/" title="How to Use the less Command on Linux">How to Use the less Command on Linux</a></h1><p><a target="_blank" rel="noopener" href="https://www.howtogeek.com/author/davidmckay/">DAVE MCKAY</a></p>
<p><a target="_blank" rel="noopener" href="https://www.howtogeek.com/author/davidmckay/"></a></p>
<p><a target="_blank" rel="noopener" href="https://www.howtogeek.com/about"></a></p>
<p>  <a target="_blank" rel="noopener" href="https://twitter.com/TheGurkha">@thegurkha</a>  </p>
<p>OCT 23, 2019, 8:00 AM EST | 7 MIN READ</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/09/stock-lede-linux-see-attribution.png?width=1198&trim=1,1&bg-color=000&pad=1,1" alt="A Linux terminal on a Ubuntu-style desktop."></p>
<p><a target="_blank" rel="noopener" href="https://www.shutterstock.com/image-vector/linux-interface-screen-notebook-world-map-321627716">Fatmawati Achmad Zaenuri/Shutterstock</a></p>
<p>The <code>less</code> command lets you page through a text file, displaying a screenful of text each time. It seems like one of the simplest Linux commands at first glance, but there’s a lot more to less than meets the eye.</p>
<h2 id="The-History-of-less"><a href="#The-History-of-less" class="headerlink" title="The History of less"></a>The History of less</h2><p>Everything in Linux–and Unix—has a history, no pun intended. The <code>less</code> program is based on the <code>more</code> program, which was initially released in 1978 in version 3.0 of the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Berkeley_Software_Distribution">Berkeley Software Distribution Unix</a> (3.0BSD). <code>more</code> allowed you to progressively page through a text file, displaying a screenful of text at a time.</p>
<p><a target="_blank" rel="noopener" href="https://www.howtogeek.com/757339/10-basic-linux-commands-for-beginners/" title="10 Basic Linux Commands for Beginners"><img src="https://www.howtogeek.com/wp-content/uploads/2021/06/linux-laptop.png?width=120&height=80&fit=crop&trim=2,2,2,2" alt="10 Basic Linux Commands for Beginners"></a></p>
<p><strong>RELATED</strong><a target="_blank" rel="noopener" href="https://www.howtogeek.com/757339/10-basic-linux-commands-for-beginners/">10 Basic Linux Commands for Beginners</a></p>
<p>Necessity being the mother of invention, it was the inability of early versions of <code>more</code> to scroll backward through a file that prompted Mark Nudelman to develop <code>less</code> and to overcome that specific problem. That work started in 1983, and the first version was released outside of the company he worked for in 1985. As of October 2019, he is <a target="_blank" rel="noopener" href="http://www.greenwoodsoftware.com/less/faq.html#mail">still the maintainer</a> of <code>less</code>.</p>
<p>I wonder if there is a Linux user who hasn’t used <code>less</code>? Even if they haven’t used it to page through a chosen text file, chances are they’ve used the <code>man</code> command. And <code>man</code> calls <code>less</code> behind the scenes to display the <code>man</code> pages.</p>
<p>This is a command with a lot of tricks up its sleeve.</p>
<h2 id="Why-less-Is-Better-Than-more"><a href="#Why-less-Is-Better-Than-more" class="headerlink" title="Why less Is Better Than more"></a>Why less Is Better Than more</h2><p><code>less</code> has been added to steadily over the years. It has a staggering amount of command-line options and in-application command keystrokes. Do a quick comparison of the <a target="_blank" rel="noopener" href="http://man7.org/linux/man-pages/man1/less.1.html">man page</a> for less and the <a target="_blank" rel="noopener" href="http://man7.org/linux/man-pages/man1/more.1.html">man page</a> for <code>more</code> , and you’ll start to see how <code>less</code> absolutely towers over <code>more</code>.</p>
<p><code>more</code> has overcome its initial failing of not being able to page backward through text, but only for files. It cannot page backward through piped input. You can do that with <code>less</code>.</p>
<p>With its flexibility in navigating files, viewing multiple files, searching for text, dropping and returning to bookmarks, and dealing with piped input, <code>less</code> wins hands down. Use <code>less</code> instead of the <code>more</code> .</p>
<h2 id="Reading-a-File-With-less"><a href="#Reading-a-File-With-less" class="headerlink" title="Reading a File With less"></a>Reading a File With less</h2><p>To load a file into <code>less</code>, provide the name of the file on the command line:</p>
<p>less Dr-Jekyll-and-Mr-Hyde-001.txt</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/1-9.png?trim=1,1&bg-color=000&pad=1,1"></p>
<p>The file is loaded and displayed. The top (or “start”) of the file is shown in the terminal window. You can use the scroll wheel of your mouse to scroll forward and backward through the text.</p>
<p>On the keyboard, use the Space bar or Page Down key to move forward through the text one screenful of text at a time.</p>
<p>Page Up will move backward through the file (towards the “start” of the file.) The Home and End keys will take you directly to the start and end of the text file, respectively.</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/2-9.png?trim=1,1&bg-color=000&pad=1,1"></p>
<p>The name of the file is displayed in the bottom-left corner of the display. When you start to move around in the file, the bottom line is cleared. It is used to display messages to you, and for you to enter commands in.</p>
<p>Press “q” to quit <code>less</code>.</p>
<h2 id="Displaying-Line-Numbers"><a href="#Displaying-Line-Numbers" class="headerlink" title="Displaying Line Numbers"></a>Displaying Line Numbers</h2><p>To have the lines of the text file numbered for you, use the <code>-N</code> (line numbers) option.</p>
<p>less -N Dr-Jekyll-and-Mr-Hyde-001.txt</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/4-3.png?trim=1,1&bg-color=000&pad=1,1"></p>
<p>The line numbers can be useful to guide you back to specific lines or sections within log files and other files that are not written in standard prose.</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/5-4.png?trim=1,1&bg-color=000&pad=1,1"></p>
<h2 id="Searching-in-less"><a href="#Searching-in-less" class="headerlink" title="Searching in less"></a>Searching in less</h2><p>To search through the text of the file, press “/” and then type your search phrase. The search is case-sensitive. Your search phrase is displayed on the bottom line of the display. Hit “Enter” to perform the search.</p>
<p>In this example, the search term is “Enfield,” and this can be seen at the bottom of the display.</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/6-4.png?trim=1,1&bg-color=000&pad=1,1" alt="Searching for &quot;Enfield&quot; in less"></p>
<p>The search takes place from the current page to the end of the text file. To search the entire file, move to the top of the file before you search.</p>
<p>You will be told if there are no matches. If a match is found, the display moves to display the found item.</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/7-4.png?trim=1,1&bg-color=000&pad=1,1" alt="less displaying a matching search item"></p>
<p>To find the next matching item, press “n”. To search for the previous matching item, press “N”.</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/8-4.png?trim=1,1&bg-color=000&pad=1,1" alt="less with two matching search items"></p>
<p>To search <em>backward</em> from your current position in the file toward the start of the file, press the “?” key and type your search term. To find the next matching item, press “n”. To search for the previous matching item, press “N”.</p>
<p>Note that, when you’re searching backward, the next matching item (found with “n”) is the next one nearer to the <em>top</em> of the file, and the “N” for the previous matching item looks for a matching item closer to the <em>bottom</em> of the file. in other words, “n” and “N” reverse their search direction when you search backward.</p>
<h2 id="Opening-a-File-With-a-Search-Term"><a href="#Opening-a-File-With-a-Search-Term" class="headerlink" title="Opening a File With a Search Term"></a>Opening a File With a Search Term</h2><p>You can use the <code>-p</code> (pattern) option to cause <code>less</code> to search through the text file and find the first matching item. It will then display the page with the matching search item in it, instead of the first page of the file. Unless, of course, the search item is found on the first page of the file.</p>
<p>Note there is no space between the <code>-p</code> and the search term.</p>
<p>less -pEnfield Dr-Jekyll-and-Mr-Hyde-001.txt</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/9-4.png?trim=1,1&bg-color=000&pad=1,1"></p>
<p>The file is displayed with the first matching search term highlighted.</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/10-5.png?trim=1,1&bg-color=000&pad=1,1" alt="less displaying a file with the first matching search item highlighted"></p>
<h2 id="Navigating-in-Less-The-Most-Useful-Keys"><a href="#Navigating-in-Less-The-Most-Useful-Keys" class="headerlink" title="Navigating in Less: The Most Useful Keys"></a>Navigating in Less: The Most Useful Keys</h2><p>Use these keys to move and search through the text file.</p>
<ul>
<li>  Move <strong>forward</strong> one <strong>line</strong>: Down Arrow, Enter, e, or j</li>
<li>  Move <strong>backward</strong> one <strong>line</strong>: Up Arrow, y, or k</li>
<li>  Move <strong>forward</strong> one <strong>page</strong>: Space bar or Page Down</li>
<li>  Move <strong>backward</strong> one <strong>page</strong>: Page Up or b</li>
<li>  <strong>Scroll</strong> to the <strong>right</strong>: Right Arrow</li>
<li>  <strong>Scroll</strong> to the <strong>left</strong>: Left Arrow</li>
<li>  <strong>Jump</strong> to the <strong>top</strong> of the file: Home or g</li>
<li>  <strong>Jump</strong> to the <strong>end</strong> of the file: End or G</li>
<li>  <strong>Jump</strong> to a specific <strong>line</strong>: Type the line number  and then hit “g”</li>
<li>  <strong>Jump</strong> to a <strong>percentage</strong> way through the file: Type the percentage and then hit “p” or “%.” (You can even enter decimal values, so to jump to the point 27.2 percent through the file, type “27.2” and then hit “p” or “%.”  Why would you want to use decimals? I honestly have no idea.)</li>
<li>  <strong>Search forward</strong>: Hit “/” and type your search, like “/Jekyll”, and press Enter</li>
<li>  <strong>Search backward</strong>: Hit “?” and type your search, like “/Hyde”, and press Enter</li>
<li>  <strong>Next</strong> matching <strong>search item</strong>: n</li>
<li>  <strong>Previous</strong> matching <strong>search item</strong>: N</li>
<li>  <strong>Quit</strong>: q</li>
</ul>
<h2 id="Squeeze-Blank-Lines"><a href="#Squeeze-Blank-Lines" class="headerlink" title="Squeeze Blank Lines"></a>Squeeze Blank Lines</h2><p>The <code>-s</code> (squeeze blank lines) option removes a series of blanks lines and replaces them with a single blank line.</p>
<p>There are a couple of consecutive blank lines in our example file, let’s see how <code>less</code> treats them when we use the <code>-s</code> option:</p>
<p>less -s Dr-Jekyll-and-Mr-Hyde-001.txt</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/13-4.png?trim=1,1&bg-color=000&pad=1,1"></p>
<p>All of the double (or more) blank lines have been replaced by a single blank line in each case.</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/14-4.png?trim=1,1&bg-color=000&pad=1,1" alt="Less with no sequences of multiple blank lines being displayed"></p>
<h2 id="Viewing-Multiple-Files"><a href="#Viewing-Multiple-Files" class="headerlink" title="Viewing Multiple Files"></a>Viewing Multiple Files</h2><p><code>less</code> can open multiple files for you. You can hop back and forth from file to file. <code>less</code> will remember your position in each file.</p>
<p>less Dr-Jekyll-and-Mr-Hyde-001.txt Dr-Jekyll-and-Mr-Hyde-002.txt</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/15-3.png?trim=1,1&bg-color=000&pad=1,1"></p>
<p>The files are opened, and the first file is displayed. You are shown which file you are looking at, and how many files have been loaded. This is highlighted below.</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/16-4.png?trim=1,1&bg-color=000&pad=1,1" alt="less with two files loaded"></p>
<p>To view the next file, press “:” and then hit “n”.</p>
<p>Your display will change to show the second file, and the information on the bottom line is updated to show you are viewing the second file. This is highlighted below.</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/17-3.png?trim=1,1&bg-color=000&pad=1,1" alt="viewing the second file in less"></p>
<p>To move to the previous file, type “:” and then hit “p.”</p>
<h2 id="Using-Marks"><a href="#Using-Marks" class="headerlink" title="Using Marks"></a>Using Marks</h2><p><code>less</code> lets you drop a marker so that you can easily return to a marked passage. Each marker is represented by a letter. To drop a mark on the top-most displayed line, press “m” and then hit the letter you wish to use, such as “a”.</p>
<p>When you press “m”, the bottom line of the display shows a prompt as it waits for you to press a letter key.</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/18-3.png?trim=1,1&bg-color=000&pad=1,1" alt="less prompting for a mark"></p>
<p>As soon as you press a letter, the prompt is removed.</p>
<p>From any other location within the file, you can easily return to a mark by pressing the apostrophe (or single quote) “‘” and then pressing the letter of the mark you wish to return to. When you press the “‘” key, you are prompted for the mark you wish to go to.</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/19-2.png?trim=1,1&bg-color=000&pad=1,1" alt="less prompting for a mark to return to"></p>
<p>Press the letter of the mark you wish to return to, and that section of the text file is displayed for you.</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/20.png?trim=1,1&bg-color=000&pad=1,1" alt="less returning to a mark"></p>
<h2 id="Using-Piped-Input-with-Less"><a href="#Using-Piped-Input-with-Less" class="headerlink" title="Using Piped Input with Less"></a>Using Piped Input with Less</h2><p><code>less</code> can display information that comes as a stream of piped text, just as easily as if it were a file.</p>
<p>The <code>dmesg</code> command displays the <a target="_blank" rel="noopener" href="http://man7.org/linux/man-pages/man1/dmesg.1.html">kernel ring buffer messages</a>. We can pipe the output from <code>dmesg</code>  into <code>less</code> using the following command:</p>
<p>dmesg | less</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/21.png?trim=1,1&bg-color=000&pad=1,1"></p>
<p>The output from <code>dmesg</code> is displayed.</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/22.png?trim=1,1&bg-color=000&pad=1,1" alt="The output from dmesg in less"></p>
<p>You can page and search through the piped input just as though it were a file. To see the most recent messages, hit “End” to go to the bottom of the file.</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/23.png?trim=1,1&bg-color=000&pad=1,1" alt="Tme most recent dmesg messages at the bottom of the file in less"></p>
<p>As new messages arrive, you must keep pressing “End” to force <code>less</code> to display the bottom of the file. This isn’t very convenient. To have <code>less</code> always show the bottom of the text, even when new data is being added, use the <code>+F</code> (forward) option. Note the use of <code>+</code> and not <code>-</code> as the option flag.</p>
<p>dmesg | less +F</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/24.png?trim=1,1&bg-color=000&pad=1,1"></p>
<p>The <code>+</code> option flag tells <code>less</code> to treat the option as though you had used that command <em>inside</em> <code>less</code>. So if you forgot to use the <code>+F</code> option, press “F” inside <code>less</code> .</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/25.png?trim=1,1&bg-color=000&pad=1,1" alt="less awaiting new input from dmesg"></p>
<p><code>less</code> displays the bottom of the text, which shows the most recent messages from <code>dmesg</code>. It displays a message that it is waiting for more data. When more kernel messages appear, the display scrolls so that you can always see the newest messages.</p>
<p>You can’t scroll or page in this mode; it is devoted to displaying the bottom of the piped text. To exit from of his mode, press Ctrl+c, and you will be returned to the usual <code>less</code> interactive mode.</p>
<h2 id="Editing-Files-With-less"><a href="#Editing-Files-With-less" class="headerlink" title="Editing Files With less"></a>Editing Files With less</h2><p>You can edit files with <code>less</code>—well, sort of. This command can’t edit files, but if you type “v” when you are viewing a file, the file is transferred to your default editor. When you leave the editor, you are returned to <code>less</code>.</p>
<p>Hit “v” when viewing a file in <code>less</code>:</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/11-4.png?trim=1,1&bg-color=000&pad=1,1" alt="file displayed in less"></p>
<p>The file is loaded into the default editor, in this case <code>nano</code>:</p>
<p><img src="https://www.howtogeek.com/wp-content/uploads/2019/10/12-4.png?trim=1,1&bg-color=000&pad=1,1" alt="file loaded in the nano editor"></p>
<p>When you close the editor, you are turned to <code>less</code>.</p>
<h2 id="In-Summary"><a href="#In-Summary" class="headerlink" title="In Summary"></a>In Summary</h2><p>As counterintuitive as it may seem, in this case<code>less</code> &gt; <code>more</code>.</p>
<p>Linux Commands</p>
<p>Files</p>
<p><a target="_blank" rel="noopener" href="https://www.howtogeek.com/248780/how-to-compress-and-extract-files-using-the-tar-command-on-linux/">tar</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/428654/how-to-monitor-the-progress-of-linux-commands-with-pv-and-progress/">pv</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/278599/how-to-combine-text-files-using-the-cat-command-in-linux/">cat</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/424234/how-to-use-the-linux-cat-and-tac-commands/">tac</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/437958/how-to-use-the-chmod-command-on-linux/">chmod</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/496056/how-to-use-the-grep-command-on-linux/">grep</a> <strong>·</strong>  <a target="_blank" rel="noopener" href="https://www.howtogeek.com/410532/how-to-compare-two-text-files-in-the-linux-terminal/">diff</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/666395/how-to-use-the-sed-command-on-linux/">sed</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/427086/how-to-use-linuxs-ar-command-to-create-static-libraries/">ar</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/663440/how-to-use-linuxs-man-command-hidden-secrets-and-basics/">man</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/659146/how-to-use-pushd-and-popd-on-linux/">pushd</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/659146/how-to-use-pushd-and-popd-on-linux/">popd</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/745921/how-to-use-the-fsck-command-on-linux/">fsck</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/700310/how-to-recover-deleted-files-on-linux-with-testdisk/">testdisk</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/693549/how-to-use-the-seq-command-on-linux/">seq</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/682244/how-to-use-the-fd-command-on-linux/">fd</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/678022/how-to-use-pandoc-to-convert-files-on-the-linux-command-line/">pandoc</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/666127/how-to-use-the-cd-command-on-linux/">cd</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/658904/how-to-add-a-directory-to-your-path-in-linux/">$PATH</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/562941/how-to-use-the-awk-command-on-linux/">awk</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/542677/how-to-use-the-join-command-on-linux/">join</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/529219/how-to-parse-json-files-on-the-linux-command-line-with-jq/">jq</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/538778/how-to-use-the-fold-command-on-linux/">fold</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/533406/how-to-use-the-uniq-command-on-linux/">uniq</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/499623/how-to-use-journalctl-to-read-linux-system-logs/">journalctl</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/481766/how-to-use-the-tail-command-on-linux/">tail</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/451022/how-to-use-the-stat-command-on-linux/">stat</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/448446/how-to-use-the-ls-command-on-linux/">ls</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/444814/how-to-write-an-fstab-file-on-linux/">fstab</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/446071/how-to-use-the-echo-command-on-linux/">echo</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/444233/how-to-use-the-less-command-on-linux/">less</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/439500/how-to-use-the-chgrp-command-on-linux/">chgrp</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/438435/how-to-use-the-chown-command-on-linux/">chown</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/434180/how-to-use-the-rev-command-on-linux/">rev</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/428742/how-to-use-the-look-command-on-linux/">look</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/427805/how-to-use-the-strings-command-on-linux/">strings</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/426014/how-to-use-the-linux-type-command/">type</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/423214/how-to-use-the-rename-command-on-linux/">rename</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/414082/how-to-zip-or-unzip-files-from-the-linux-terminal/">zip</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/414082/how-to-zip-or-unzip-files-from-the-linux-terminal/">unzip</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/414082/how-to-zip-or-unzip-files-from-the-linux-terminal/">mount</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/414082/how-to-zip-or-unzip-files-from-the-linux-terminal/">umount</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/411366/how-to-copy-files-with-the-install-command-on-linux/">install</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/106873/how-to-use-fdisk-to-manage-partitions-on-linux/">fdisk</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/443342/how-to-use-the-mkfs-command-on-linux/">mkfs</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/409115/how-to-delete-files-and-directories-in-the-linux-terminal/">rm</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/409115/how-to-delete-files-and-directories-in-the-linux-terminal/">rmdir</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/427480/how-to-back-up-your-linux-system/">rsync</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/409611/how-to-view-free-disk-space-and-disk-usage-from-the-linux-terminal/">df</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/427982/how-to-encrypt-and-decrypt-files-with-gpg-on-linux/">gpg</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/102468/a-beginners-guide-to-editing-text-files-with-vi/">vi</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/howto/42980/the-beginners-guide-to-nano-the-linux-command-line-text-editor/">nano</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/275069/how-to-create-multiple-subdirectories-with-one-linux-command/">mkdir</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/450366/how-to-get-the-size-of-a-file-or-directory-in-linux/">du</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/287014/how-to-create-and-use-symbolic-links-aka-symlinks-on-linux/">ln</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/415442/how-to-apply-a-patch-to-a-file-and-create-patches-in-linux/">patch</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/109369/how-to-quickly-resize-convert-modify-images-from-the-linux-terminal/">convert</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/451262/how-to-use-rclone-to-back-up-to-google-drive-on-linux/">rclone</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/425232/how-to-securely-delete-files-on-linux/">shred</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/425232/how-to-securely-delete-files-on-linux/">srm</a></p>
<p>Processes</p>
<p><a target="_blank" rel="noopener" href="https://www.howtogeek.com/439736/how-to-create-aliases-and-shell-functions-on-linux/">alias</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/662422/how-to-use-linuxs-screen-command/">screen</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/668986/how-to-use-the-linux-top-command-and-understand-its-output/">top</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/411979/how-to-set-process-priorities-with-the-nice-and-renice-commands-in-linux/">nice</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/411979/how-to-set-process-priorities-with-the-nice-and-renice-commands-in-linux/">renice</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/428654/how-to-monitor-the-progress-of-linux-commands-with-pv-and-progress/">progress</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/732736/how-to-use-strace-to-monitor-linux-system-calls/">strace</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/687970/how-to-run-a-linux-program-at-startup-with-systemd/">systemd</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/671422/how-to-use-tmux-on-linux-and-why-its-better-than-screen/">tmux</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/669835/how-to-change-your-default-shell-on-linux-with-chsh/">chsh</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/465243/how-to-use-the-history-command-on-linux/">history</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/451386/how-to-use-at-and-batch-on-linux-to-launch-processes/">at</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/451386/how-to-use-at-and-batch-on-linux-to-launch-processes/">batch</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/456943/how-to-use-the-free-command-on-linux/">free</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/450894/how-to-use-the-which-command-on-linux/">which</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/449335/how-to-use-the-dmesg-command-on-linux/">dmesg</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/449160/how-to-change-user-data-with-chfn-and-usermod-on-linux/">chfn</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/449160/how-to-change-user-data-with-chfn-and-usermod-on-linux/">usermod</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/448271/how-to-use-the-ps-command-to-monitor-linux-processes/">ps</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/441534/how-to-use-the-chroot-command-on-linux/">chroot</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/435164/how-to-use-the-xargs-command-on-linux/">xargs</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/428174/what-is-a-tty-on-linux-and-how-to-use-the-tty-command/">tty</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/427004/how-to-use-the-pinky-command-on-linux/">pinky</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/426031/how-to-use-the-linux-lsof-command/">lsof</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/424334/how-to-use-the-vmstat-command-on-linux/">vmstat</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/423286/how-to-use-the-timeout-command-on-linux/">timeout</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/415914/how-to-use-the-wall-command-on-linux/">wall</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/415535/how-to-use-the-yes-command-on-linux/">yes</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/413213/how-to-kill-processes-from-the-linux-terminal/">kill</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/410299/how-to-pause-a-bash-script-with-the-linux-sleep-command/">sleep</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/111479/htg-explains-whats-the-difference-between-sudo-su/">sudo</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/111479/htg-explains-whats-the-difference-between-sudo-su/">su</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/415977/how-to-use-the-time-command-on-linux/">time</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/50787/add-a-user-to-a-group-or-second-group-on-linux/">groupadd</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/50787/add-a-user-to-a-group-or-second-group-on-linux/">usermod</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/howto/ubuntu/see-which-groups-your-linux-user-belongs-to/">groups</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/508993/how-to-check-which-gpu-is-installed-on-linux/">lshw</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/411925/how-to-reboot-or-shut-down-linux-using-the-command-line/">shutdown</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/411925/how-to-reboot-or-shut-down-linux-using-the-command-line/">reboot</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/411925/how-to-reboot-or-shut-down-linux-using-the-command-line/">halt</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/411925/how-to-reboot-or-shut-down-linux-using-the-command-line/">poweroff</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/447443/how-to-change-account-passwords-on-linux/">passwd</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/198615/how-to-check-if-your-linux-system-is-32-bit-or-64-bit/">lscpu</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/101288/how-to-schedule-tasks-on-linux-an-introduction-to-crontab-files/">crontab</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/410442/how-to-display-the-date-and-time-in-the-linux-terminal-and-use-it-in-bash-scripts/">date</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/440848/how-to-run-and-control-background-processes-on-linux/">bg</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/440848/how-to-run-and-control-background-processes-on-linux/">fg</a></p>
<p>Networking</p>
<p><a target="_blank" rel="noopener" href="https://www.howtogeek.com/513003/how-to-use-netstat-on-linux/">netstat</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/355664/how-to-use-ping-to-test-your-network/">ping</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/657780/how-to-use-the-traceroute-command-on-linux/">traceroute</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/657911/how-to-use-the-ip-command-on-linux/">ip</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/681468/how-to-use-the-ss-command-on-linux/">ss</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/680086/how-to-use-the-whois-command-on-linux/">whois</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/675010/how-to-secure-your-linux-computer-with-fail2ban/">fail2ban</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/664589/how-to-use-bmon-to-monitor-network-bandwidth-on-linux/">bmon</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/663056/how-to-use-the-dig-command-on-linux/">dig</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/440391/how-to-use-the-finger-command-on-linux/">finger</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/423709/how-to-see-all-devices-on-your-network-with-nmap-on-linux/">nmap</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/412626/how-to-use-the-ftp-command-on-linux/">ftp</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/447033/how-to-use-curl-to-download-files-from-the-linux-command-line/">curl</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/281663/how-to-use-wget-the-ultimate-command-line-downloading-tool/">wget</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/410423/how-to-determine-the-current-user-account-in-linux/">who</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/410423/how-to-determine-the-current-user-account-in-linux/">whoami</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/410423/how-to-determine-the-current-user-account-in-linux/">w</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/177621/the-beginners-guide-to-iptables-the-linux-firewall/">iptables</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/424510/how-to-create-and-install-ssh-keys-from-the-linux-shell/">ssh-keygen</a> <strong>·</strong> <a target="_blank" rel="noopener" href="https://www.howtogeek.com/115116/how-to-configure-ubuntus-built-in-firewall/">ufw</a></p>
<p><strong>RELATED:</strong> <em><strong><a target="_blank" rel="noopener" href="https://www.howtogeek.com/748445/best-linux-laptops/">Best Linux Laptops for Developers and Enthusiasts</a></strong></em></p>
<h1 id="QGIS"><a href="#QGIS" class="headerlink" title="QGIS"></a>QGIS</h1><ul>
<li><a target="_blank" rel="noopener" href="https://qgis.org/en/site/forusers/alldownloads.html#debian-ubuntu">QGIS Installers</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install qgis qgis-plugin-grass</span><br></pre></td></tr></table></figure>
<h1 id="sequin-from-NCBI"><a href="#sequin-from-NCBI" class="headerlink" title="sequin from NCBI"></a>sequin from NCBI</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install ncbi-tools-x11</span><br></pre></td></tr></table></figure>
<h1 id="CRAN-“devtools”-installation"><a href="#CRAN-“devtools”-installation" class="headerlink" title="CRAN “devtools” installation"></a>CRAN “devtools” installation</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1: In install.packages(&quot;devtools&quot;, dependencies &#x3D; TRUE) :</span><br><span class="line">  installation of package ‘credentials’ had non-zero exit status</span><br><span class="line">2: In install.packages(&quot;devtools&quot;, dependencies &#x3D; TRUE) :</span><br><span class="line">  installation of package ‘gert’ had non-zero exit status</span><br><span class="line">3: In install.packages(&quot;devtools&quot;, dependencies &#x3D; TRUE) :</span><br><span class="line">  installation of package ‘usethis’ had non-zero exit status</span><br><span class="line">4: In install.packages(&quot;devtools&quot;, dependencies &#x3D; TRUE) :</span><br><span class="line">  installation of package ‘devtools’ had non-zero exit status</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">install.packages(c(&quot;caret&quot;, &quot;coin&quot;, &quot;Cubist&quot;, &quot;digest&quot;, &quot;e1071&quot;, &quot;fastICA&quot;, &quot;glue&quot;, &quot;gower&quot;, &quot;ipred&quot;, &quot;jsonlite&quot;, &quot;knitr&quot;, &quot;libcoin&quot;, &quot;lme4&quot;, &quot;lubridate&quot;, &quot;magrittr&quot;, &quot;mime&quot;, &quot;nloptr&quot;, &quot;party&quot;, &quot;RcppArmadillo&quot;, &quot;readr&quot;, &quot;recipes&quot;, &quot;rlang&quot;, &quot;stringi&quot;, &quot;tidyr&quot;, &quot;tinytex&quot;, &quot;tzdb&quot;, &quot;vroom&quot;, &quot;xfun&quot;, &quot;yaml&quot;))</span><br><span class="line"># </span><br><span class="line">&gt; install.packages(c(&quot;caret&quot;, &quot;coin&quot;, &quot;Cubist&quot;, &quot;digest&quot;, &quot;e1071&quot;, &quot;fastICA&quot;, &quot;glue&quot;, &quot;gower&quot;, &quot;ipred&quot;, &quot;jsonlite&quot;, &quot;knitr&quot;, &quot;libcoin&quot;, &quot;lme4&quot;, &quot;lubridate&quot;, &quot;magrittr&quot;, &quot;mime&quot;, &quot;nloptr&quot;, &quot;party&quot;, &quot;RcppArmadillo&quot;, &quot;readr&quot;, &quot;recipes&quot;, &quot;rlang&quot;, &quot;stringi&quot;, &quot;tidyr&quot;, &quot;tinytex&quot;, &quot;tzdb&quot;, &quot;vroom&quot;, &quot;xfun&quot;, &quot;yaml&quot;))</span><br><span class="line"># </span><br><span class="line">In install.packages(c(&quot;caret&quot;, &quot;coin&quot;, &quot;Cubist&quot;, &quot;digest&quot;, &quot;e1071&quot;,  :</span><br><span class="line">  installation of package ‘nloptr’ had non-zero exit status</span><br><span class="line"># </span><br><span class="line">library(c(&quot;caret&quot;, &quot;coin&quot;, &quot;Cubist&quot;, &quot;digest&quot;, &quot;e1071&quot;, &quot;fastICA&quot;, &quot;glue&quot;, &quot;gower&quot;, &quot;ipred&quot;, &quot;jsonlite&quot;, &quot;knitr&quot;, &quot;libcoin&quot;, &quot;lme4&quot;, &quot;lubridate&quot;, &quot;magrittr&quot;, &quot;mime&quot;, &quot;nloptr&quot;, &quot;party&quot;, &quot;RcppArmadillo&quot;, &quot;readr&quot;, &quot;recipes&quot;, &quot;rlang&quot;, &quot;stringi&quot;, &quot;tidyr&quot;, &quot;tinytex&quot;, &quot;tzdb&quot;, &quot;vroom&quot;, &quot;xfun&quot;, &quot;yaml&quot;))</span><br><span class="line"># </span><br><span class="line">require(c(&quot;caret&quot;, &quot;coin&quot;, &quot;Cubist&quot;, &quot;digest&quot;, &quot;e1071&quot;, &quot;fastICA&quot;, &quot;glue&quot;, &quot;gower&quot;, &quot;ipred&quot;, &quot;jsonlite&quot;, &quot;knitr&quot;, &quot;libcoin&quot;, &quot;lme4&quot;, &quot;lubridate&quot;, &quot;magrittr&quot;, &quot;mime&quot;, &quot;nloptr&quot;, &quot;party&quot;, &quot;RcppArmadillo&quot;, &quot;readr&quot;, &quot;recipes&quot;, &quot;rlang&quot;, &quot;stringi&quot;, &quot;tidyr&quot;, &quot;tinytex&quot;, &quot;tzdb&quot;, &quot;vroom&quot;, &quot;xfun&quot;, &quot;yaml&quot;))</span><br><span class="line"></span><br><span class="line">nloptr</span><br></pre></td></tr></table></figure>
      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/Log//" class="article-tag-list-link color4">Log</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2022/03/20/20220320/">Expand all items >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-20220308" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/03/08/20220308/">20220308</a>
    </h1>
  

        
        <a href="/2022/03/08/20220308/" class="archive-article-date">
  	<time datetime="2022-03-07T16:00:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>20220308</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
    

      
<div id="toc" class="toc-article">
  <strong class="toc-title">Contents</strong>
  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Working-directories"><span class="toc-number">1.</span> <span class="toc-text">Working directories</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Protein-Interaction-PPI"><span class="toc-number">2.</span> <span class="toc-text">Protein Interaction&#x2F;PPI</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Drive5-com"><span class="toc-number">3.</span> <span class="toc-text">Drive5.com</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#AlphaFold-Protein-Structure-Database"><span class="toc-number">4.</span> <span class="toc-text">AlphaFold Protein Structure Database</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Home"><span class="toc-number">4.1.</span> <span class="toc-text">Home</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Background"><span class="toc-number">4.1.1.</span> <span class="toc-text">Background</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Find-out-more"><span class="toc-number">4.1.2.</span> <span class="toc-text">Find out more</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#What%E2%80%99s-next"><span class="toc-number">4.1.3.</span> <span class="toc-text">What’s next?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#License-and-attribution"><span class="toc-number">4.1.4.</span> <span class="toc-text">License and attribution</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Downloads"><span class="toc-number">4.2.</span> <span class="toc-text">Downloads</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Compressed-prediction-files-for-model-organism-proteomes"><span class="toc-number">4.2.1.</span> <span class="toc-text">Compressed prediction files for model organism proteomes:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Compressed-prediction-files-for-global-health-proteomes"><span class="toc-number">4.2.2.</span> <span class="toc-text">Compressed prediction files for global health proteomes:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Compressed-prediction-files-for-Swiss-Prot"><span class="toc-number">4.2.3.</span> <span class="toc-text">Compressed prediction files for Swiss-Prot:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#License-and-attribution-1"><span class="toc-number">4.2.4.</span> <span class="toc-text">License and attribution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Feedback"><span class="toc-number">4.2.5.</span> <span class="toc-text">Feedback</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Disclaimer"><span class="toc-number">4.2.6.</span> <span class="toc-text">Disclaimer</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Protein-Structure-Prediction"><span class="toc-number">5.</span> <span class="toc-text">Protein Structure Prediction</span></a></li></ol>
</div>

        <h1 id="Working-directories"><a href="#Working-directories" class="headerlink" title="Working directories"></a>Working directories</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Book of &quot;Protein Structure Prediction&quot;</span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Work&#x2F;References&#x2F;eBooks&#x2F;Important_Protocols&#x2F;Protein_Structure_Prediction.pdf; \</span><br><span class="line">setsid okular $&#123;i&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Protein-Interaction-PPI"><a href="#Protein-Interaction-PPI" class="headerlink" title="Protein Interaction/PPI"></a>Protein Interaction/PPI</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.thermofisher.cn/cn/zh/home/references/gibco-cell-culture-basics/cell-culture-protocols/cell-culture-useful-numbers.html">Useful Numbers for Cell Culture | Thermo Fisher Scientific - CN</a></li>
<li><a target="_blank" rel="noopener" href="https://www.sigmaaldrich.cn/CN/zh/search/zebra?focus=products&page=1&perpage=30&sort=relevance&term=zebra&type=product">Zebra | Sigma-Aldrich</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bing.com/search?q=%E7%BB%93%E5%90%88%E8%9B%8B%E7%99%BD%E9%A2%84%E6%B5%8B&go=Search&qs=n&form=QBRE&sp=-1&pq=%E7%BB%93%E5%90%88%E8%9B%8B%E7%99%BD%E9%A2%84%E6%B5%8B&sc=1-6&sk=&cvid=5F120510CC3348C8A91DAA8B035E3810">结合蛋白预测 - Bing</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.sciencenet.cn/blog-1509670-972342.html">科学网—PPA-Pred—-蛋白互作预测的工具 - 熊朝亮的博文</a></li>
<li><a target="_blank" rel="noopener" href="https://new.qq.com/omn/20190903/20190903A0IIMN00.html">这大概是我最想推荐的蛋白结构预测网站了：Phyre2</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/a769a3e8bf47">值得收藏——10个蛋白结构分析预测和相关信号转导网站 - 简书</a></li>
<li><a target="_blank" rel="noopener" href="https://www.researchgate.net/post/How_to_find_out_the_binding_sites_in_a_given_protein">How to find out the binding sites in a given protein?</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi">NCBI Conserved Domain Search</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/Structure/lexington/lexington.cgi?cachecdqs=NC-data_cache-0-NCID_1_5180717_130.14.18.128_9149_1646988775_1372240227_0MetA0__S_NC_CDDCache">Welcome to NCBI Domain architecture search</a></li>
<li><a target="_blank" rel="noopener" href="https://bmcstructbiol.biomedcentral.com/track/pdf/10.1186/1472-6807-14-18.pdf">1472-6807-14-18.pdf</a></li>
<li><a target="_blank" rel="noopener" href="http://molbiol-tools.ca/Motifs.htm">Online Analysis Tools - Motifs</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/sites/ppmc/articles/PMC2845582/">Large-scale prediction of protein-protein interactions from structures</a></li>
<li><a target="_blank" rel="noopener" href="https://www.frontiersin.org/articles/10.3389/fbinf.2021.731345/full">Frontiers | Large-Scale Protein Interactions Prediction by Multiple Evidence Analysis Associated With an In-Silico Curation Strategy | Bioinformatics</a></li>
<li><a target="_blank" rel="noopener" href="https://zhanggroup.org/cgi-bin/BSpred.pl">BSpred On-line Server</a></li>
<li><a target="_blank" rel="noopener" href="https://yanglab.nankai.edu.cn/PepBind/output/PB000795/">PepBind results</a></li>
<li><a target="_blank" rel="noopener" href="https://yanglab.nankai.edu.cn/PepBind/output/PB000001/">PepBind results</a></li>
<li><a target="_blank" rel="noopener" href="https://jcheminf.biomedcentral.com/articles/10.1186/s13321-016-0149-z#:~:text=Protein-binding%20sites%20prediction%20lays%20a%20foundation%20for%20functional,becomes%20the%20dominant%20approach%20for%20protein-binding%20sites%20prediction.">bSiteFinder, an improved protein-binding sites prediction server based on structural alignment: more accurate and less time-consuming | Journal of Cheminformatics | Full Text</a></li>
<li><a target="_blank" rel="noopener" href="https://cas.shmtu.edu.cn/cas/login?service=http://ng.shmtu.edu.cn/wengine-auth/login?cas_login=true">上海海事大学单点登录系统</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bing.com/search?q=protein%20interaction%20spectrum%20prediction&qs=n&form=QBRE&=%25eManage%20Your%20Search%20History%25E&sp=-1&pq=protein%20interaction%20spectrum%20prediction&sc=1-39&sk=&cvid=5BFD95131BC84C97A734C820A47246FB&ajf=60&mkt=zh-CN">protein interaction spectrum prediction - Bing</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ebi.ac.uk/intact/search?query=nfx1">IntAct - Search Results</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bing.com/search?q=%E5%AF%BB%E6%89%BE%E7%BB%93%E5%90%88%E8%9B%8B%E7%99%BD&qs=n&%25eManage+Your+Search+History%25E&sp=-1&pq=%E5%AF%BB%E6%89%BEjiehe&sc=1-7&sk=&cvid=431CF47D2BAE4AE7923AFE99B7CD6711&first=11&FORM=PORE">寻找结合蛋白 - Bing</a></li>
<li><a target="_blank" rel="noopener" href="https://www.biotech-pack.com/resources/tech-protein-identification-58.html">结合蛋白质谱分析_质谱分析寻找结合蛋白_分析结合蛋白质谱|百泰派克生物科技</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jingege.wang/2020/05/29/%E8%BF%99%E5%9B%9B%E4%B8%AA%E7%BD%91%E7%AB%99%E4%BA%86%E8%A7%A3%E4%B8%80%E4%B8%8B%EF%BC%8C%E8%AE%A9%E4%BD%A0%E8%BD%BB%E6%9D%BE%E9%A2%84%E6%B5%8B%E7%9B%B8%E4%BA%92%E4%BD%9C%E7%94%A8%E8%9B%8B%E7%99%BD/">这四个网站了解一下，让你轻松预测相互作用蛋白！ – 王进的个人网站</a></li>
<li><a target="_blank" rel="noopener" href="https://www.uniprot.org/uniprot/P28799">GRN - Progranulin precursor - Homo sapiens (Human) - GRN gene &amp; protein</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bing.com/search?q=string&search=&form=QBLHCN">string - Bing</a></li>
<li><a target="_blank" rel="noopener" href="https://string-db.org/cgi/network?taskId=bzrVKebOgGt6&sessionId=bRNWwSL260d0">NFX1 protein (human) - STRING interaction network</a></li>
<li><a target="_blank" rel="noopener" href="https://www.thermofisher.cn/cn/zh/home/references/gibco-cell-culture-basics/cell-culture-protocols/cell-culture-useful-numbers.html">Useful Numbers for Cell Culture | Thermo Fisher Scientific - CN</a></li>
<li><a target="_blank" rel="noopener" href="https://www.sigmaaldrich.cn/CN/zh/search/zebra?focus=products&page=1&perpage=30&sort=relevance&term=zebra&type=product">Zebra | Sigma-Aldrich</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bing.com/search?q=%E7%BB%93%E5%90%88%E8%9B%8B%E7%99%BD%E9%A2%84%E6%B5%8B&go=Search&qs=n&form=QBRE&sp=-1&pq=%E7%BB%93%E5%90%88%E8%9B%8B%E7%99%BD%E9%A2%84%E6%B5%8B&sc=1-6&sk=&cvid=5F120510CC3348C8A91DAA8B035E3810">结合蛋白预测 - Bing</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.sciencenet.cn/blog-1509670-972342.html">科学网—PPA-Pred—-蛋白互作预测的工具 - 熊朝亮的博文</a></li>
<li><a target="_blank" rel="noopener" href="https://new.qq.com/omn/20190903/20190903A0IIMN00.html">这大概是我最想推荐的蛋白结构预测网站了：Phyre2</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/a769a3e8bf47">值得收藏——10个蛋白结构分析预测和相关信号转导网站 - 简书</a></li>
<li><a target="_blank" rel="noopener" href="https://www.researchgate.net/post/How_to_find_out_the_binding_sites_in_a_given_protein">How to find out the binding sites in a given protein?</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi">NCBI Conserved Domain Search</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/Structure/lexington/lexington.cgi?cachecdqs=NC-data_cache-0-NCID_1_5180717_130.14.18.128_9149_1646988775_1372240227_0MetA0__S_NC_CDDCache">Welcome to NCBI Domain architecture search</a></li>
<li><a target="_blank" rel="noopener" href="https://bmcstructbiol.biomedcentral.com/track/pdf/10.1186/1472-6807-14-18.pdf">1472-6807-14-18.pdf</a></li>
<li><a target="_blank" rel="noopener" href="http://molbiol-tools.ca/Motifs.htm">Online Analysis Tools - Motifs</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/sites/ppmc/articles/PMC2845582/">Large-scale prediction of protein-protein interactions from structures</a></li>
<li><a target="_blank" rel="noopener" href="https://www.frontiersin.org/articles/10.3389/fbinf.2021.731345/full">Frontiers | Large-Scale Protein Interactions Prediction by Multiple Evidence Analysis Associated With an In-Silico Curation Strategy | Bioinformatics</a></li>
<li><a target="_blank" rel="noopener" href="https://zhanggroup.org/cgi-bin/BSpred.pl">BSpred On-line Server</a></li>
<li><a target="_blank" rel="noopener" href="https://yanglab.nankai.edu.cn/PepBind/output/PB000795/">PepBind results</a></li>
<li><a target="_blank" rel="noopener" href="https://yanglab.nankai.edu.cn/PepBind/output/PB000001/">PepBind results</a></li>
<li><a target="_blank" rel="noopener" href="https://jcheminf.biomedcentral.com/articles/10.1186/s13321-016-0149-z#:~:text=Protein-binding%20sites%20prediction%20lays%20a%20foundation%20for%20functional,becomes%20the%20dominant%20approach%20for%20protein-binding%20sites%20prediction.">bSiteFinder, an improved protein-binding sites prediction server based on structural alignment: more accurate and less time-consuming | Journal of Cheminformatics | Full Text</a></li>
<li><a target="_blank" rel="noopener" href="https://cas.shmtu.edu.cn/cas/login?service=http://ng.shmtu.edu.cn/wengine-auth/login?cas_login=true">上海海事大学单点登录系统</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bing.com/search?q=protein%20interaction%20spectrum%20prediction&qs=n&form=QBRE&=%25eManage%20Your%20Search%20History%25E&sp=-1&pq=protein%20interaction%20spectrum%20prediction&sc=1-39&sk=&cvid=5BFD95131BC84C97A734C820A47246FB&ajf=60&mkt=zh-CN">protein interaction spectrum prediction - Bing</a></li>
</ul>
<h1 id="Drive5-com"><a href="#Drive5-com" class="headerlink" title="Drive5.com"></a><a target="_blank" rel="noopener" href="https://drive5.com/software.html">Drive5.com</a></h1><p><a target="_blank" rel="noopener" href="https://drive5.com/usearch">USEARCH &gt;</a><br>Extreme high-throughput sequence analysis. Orders of magnitude faster than BLAST.</p>
<p> </p>
<p><a target="_blank" rel="noopener" href="https://drive5.com/muscle">MUSCLE &gt;</a><br>Multiple sequence alignment. Faster and more accurate than CLUSTALW.</p>
<p> </p>
<p><a target="_blank" rel="noopener" href="https://drive5.com/uparse">UPARSE &gt;</a><br>OTU clustering for 16S and other marker genes. Highly accurate OTU sequences and improved diversity measures.  </p>
<p> </p>
<p><a target="_blank" rel="noopener" href="https://www.drive5.com/usearch/manual/uchime_algo.html">UCHIME &gt;</a><br>Chimeric sequence detection.</p>
<p> </p>
<p><a target="_blank" rel="noopener" href="https://drive5.com/piler">PILER &gt;</a><br>De novo genome repeat finder.  </p>
<p> </p>
<p><a target="_blank" rel="noopener" href="https://drive5.com/pilercr">PILER-CR &gt;</a><br>Detection of CRISPR repeats in bacterial genomes.</p>
<p> </p>
<p><a target="_blank" rel="noopener" href="https://drive5.com/urmap">URMAP</a>  <strong>&gt;&gt; new</strong><br>Ultra-fast read mapper.</p>
<p> </p>
<p><a target="_blank" rel="noopener" href="https://drive5.com/pals">PALS &gt;</a><br>Whole-genome alignment.  </p>
<p> </p>
<p><a target="_blank" rel="noopener" href="https://drive5.com/muscle/prefab.htm">PREFAB &gt;</a><br>Protein Reference Alignment Database.</p>
<p> </p>
<p><a target="_blank" rel="noopener" href="https://drive5.com/bench">MSA benchmark collection &gt;</a><br>Selected multiple alignment benchmarks in a standardized FASTA format.</p>
<h1 id="AlphaFold-Protein-Structure-Database"><a href="#AlphaFold-Protein-Structure-Database" class="headerlink" title="AlphaFold Protein Structure Database"></a><a target="_blank" rel="noopener" href="https://www.alphafold.ebi.ac.uk/">AlphaFold Protein Structure Database</a></h1><h2 id="Home"><a href="#Home" class="headerlink" title="Home"></a>Home</h2><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p><a target="_blank" rel="noopener" href="https://deepmind.com/blog/article/putting-the-power-of-alphafold-into-the-worlds-hands">AlphaFold</a> is an AI system developed by <a target="_blank" rel="noopener" href="https://deepmind.com/">DeepMind</a> that predicts a protein’s 3D structure from its amino acid sequence. It regularly achieves accuracy competitive with experiment.</p>
<p>DeepMind and EMBL’s European Bioinformatics Institute (<a target="_blank" rel="noopener" href="http://www.ebi.ac.uk/">EMBL-EBI</a>) have partnered to create AlphaFold DB to make these predictions freely available to the scientific community. The first release covered the human proteome and the proteomes of several other <a target="_blank" rel="noopener" href="https://www.alphafold.ebi.ac.uk/download">key organisms</a>, while the second release added the majority of manually curated UniProt entries (<a target="_blank" rel="noopener" href="https://www.expasy.org/resources/uniprotkb-swiss-prot">Swiss-Prot</a>). In 2022 we plan to expand the database to cover a large proportion of all catalogued proteins (the over 100 million in <a target="_blank" rel="noopener" href="https://www.uniprot.org/help/uniref">UniRef90</a>).</p>
<p><img src="https://www.alphafold.ebi.ac.uk/assets/img/Q8I3H7_1.png"></p>
<p>Q8I3H7: May protect the malaria parasite against attack by the immune system. Mean pLDDT 85.57.</p>
<p><a target="_blank" rel="noopener" href="https://www.alphafold.ebi.ac.uk/entry/Q8I3H7">View protein</a></p>
<p>In <a target="_blank" rel="noopener" href="https://predictioncenter.org/casp14/zscores_final.cgi">CASP14</a>, AlphaFold was the top-ranked protein structure prediction method by a large margin, producing predictions with <a target="_blank" rel="noopener" href="https://www.alphafold.ebi.ac.uk/faq#faq-12">high accuracy</a>. While the system still has some <a target="_blank" rel="noopener" href="https://www.alphafold.ebi.ac.uk/faq#faq-8">limitations</a>, the CASP results suggest AlphaFold has immediate potential to help us understand the structure of proteins and advance biological research.</p>
<p>Let us know how the AlphaFold Protein Structure Database has been useful in your research, or if you have any questions not covered in the <a target="_blank" rel="noopener" href="https://alphafold.ebi.ac.uk/faq">FAQs,</a> at <a href="mailto:alphafold@deepmind.com">alphafold@deepmind.com</a>.</p>
<p><img src="https://www.alphafold.ebi.ac.uk/assets/img/Q8W3K0.png"></p>
<p>Q8W3K0: A potential plant disease resistance protein. Mean pLDDT 82.24.</p>
<p><a target="_blank" rel="noopener" href="https://www.alphafold.ebi.ac.uk/entry/Q8W3K0">View protein</a></p>
<h3 id="Find-out-more"><a href="#Find-out-more" class="headerlink" title="Find out more"></a>Find out more</h3><p><a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41586-021-03819-2">Methodology</a><a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41586-021-03828-1">Human proteome predictions</a><a target="_blank" rel="noopener" href="https://www.alphafold.ebi.ac.uk/download">Downloads</a></p>
<p><a target="_blank" rel="noopener" href="https://www.alphafold.ebi.ac.uk/about">About AlphaFold DB</a><a target="_blank" rel="noopener" href="https://deepmind.com/">DeepMind</a><a target="_blank" rel="noopener" href="https://www.ebi.ac.uk/about">EMBL-EBI</a></p>
<h3 id="What’s-next"><a href="#What’s-next" class="headerlink" title="What’s next?"></a>What’s next?</h3><p>AlphaFold DB will continue to expand in the coming months, so if you can’t find what you’re looking for right now, please follow <a target="_blank" rel="noopener" href="https://twitter.com/DeepMind">DeepMind</a> and <a target="_blank" rel="noopener" href="https://twitter.com/emblebi">EMBL-EBI</a>’s social channels for updates. In the meantime the AlphaFold <a target="_blank" rel="noopener" href="https://github.com/deepmind/alphafold/">source code</a> and <a target="_blank" rel="noopener" href="https://bit.ly/alphafoldcolab">Colab notebook</a> can be used to predict the structures of proteins not yet in AlphaFold DB. Both resources were recently updated to support multimer prediction.</p>
<h3 id="License-and-attribution"><a href="#License-and-attribution" class="headerlink" title="License and attribution"></a>License and attribution</h3><p>All of the data provided is freely available for both academic and commercial use under Creative Commons Attribution 4.0 (<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a>) license terms. If you use this resource, please cite the following papers:</p>
<p>Jumper, J <em>et al</em>. Highly accurate protein structure prediction with AlphaFold. <em>Nature</em> (2021).</p>
<p>Varadi, M <em>et al</em>. AlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space with high-accuracy models. <em>Nucleic Acids Research</em> (2021).</p>
<p>The structures provided in this resource are predictions with <a target="_blank" rel="noopener" href="https://www.alphafold.ebi.ac.uk/faq#faq-5">varying levels of confidence</a> and should be interpreted carefully.</p>
<h2 id="Downloads"><a href="#Downloads" class="headerlink" title="Downloads"></a>Downloads</h2><ul>
<li><p><a target="_blank" rel="noopener" href="https://www.alphafold.ebi.ac.uk/download">AlphaFold Protein Structure Database</a></p>
</li>
<li><p>  <a target="_blank" rel="noopener" href="https://www.alphafold.ebi.ac.uk/download#proteomes-section">Model organism proteomes</a></p>
</li>
<li><p>  <a target="_blank" rel="noopener" href="https://www.alphafold.ebi.ac.uk/download#global-health-section">Global health proteomes</a></p>
</li>
<li><p>  <a target="_blank" rel="noopener" href="https://www.alphafold.ebi.ac.uk/download#swissprot-section">Swiss-Prot</a></p>
</li>
</ul>
<p>AlphaFold DB currently provides predicted structures for the organisms listed below, as well as the majority of <a target="_blank" rel="noopener" href="https://www.expasy.org/resources/uniprotkb-swiss-prot">Swiss-Prot.</a>  </p>
<p>You can download a prediction for an individual UniProt accession by visiting the corresponding structure page (example: <a target="_blank" rel="noopener" href="https://www.alphafold.ebi.ac.uk/entry/F4HVG8">https://www.alphafold.ebi.ac.uk/entry/F4HVG8</a>).  </p>
<p>For downloading all predictions for a given species, use the download links below. Note that this option is only available on the desktop version of the site.  </p>
<p>These uncompressed archive files (.tar) contain all the available compressed PDB and mmCIF files (.gz) for a reference proteome. In the case of proteins longer than 2700 amino acids (aa), AlphaFold provides 1400aa long, overlapping fragments. For example, Titin has predicted fragment structures named as Q8WZ42-F1 (residues 1–1400), Q8WZ42-F2 (residues 201–1600), etc. These fragments are currently only available in these proteome archive files, not on the website.  </p>
<p>For downloading all predictions for all species, visit the FTP site: <a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold">ftp://ftp.ebi.ac.uk/pub/databases/alphafold</a></p>
<h3 id="Compressed-prediction-files-for-model-organism-proteomes"><a href="#Compressed-prediction-files-for-model-organism-proteomes" class="headerlink" title="Compressed prediction files for model organism proteomes:"></a>Compressed prediction files for model organism proteomes:</h3><p>Species</p>
<p>Common Name</p>
<p>Reference Proteome</p>
<p>Predicted Structures</p>
<p>Download</p>
<p>Arabidopsis thaliana</p>
<p>Arabidopsis</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000006548">UP000006548</a> </p>
<p>27,434</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000006548_3702_ARATH_v2.tar">Download (3,678 MB)</a></p>
<p>Caenorhabditis elegans</p>
<p>Nematode worm</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000001940">UP000001940</a> </p>
<p>19,694</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000001940_6239_CAEEL_v2.tar">Download (2,626 MB)</a></p>
<p>Candida albicans</p>
<p>C. albicans</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000000559">UP000000559</a> </p>
<p>5,974</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000000559_237561_CANAL_v2.tar">Download (974 MB)</a></p>
<p>Danio rerio</p>
<p>Zebrafish</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000000437">UP000000437</a> </p>
<p>24,664</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000000437_7955_DANRE_v2.tar">Download (4,180 MB)</a></p>
<p>Dictyostelium discoideum</p>
<p>Dictyostelium</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000002195">UP000002195</a> </p>
<p>12,622</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000002195_44689_DICDI_v2.tar">Download (2,171 MB)</a></p>
<p>Drosophila melanogaster</p>
<p>Fruit fly</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000000803">UP000000803</a> </p>
<p>13,458</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000000803_7227_DROME_v2.tar">Download (2,195 MB)</a></p>
<p>Escherichia coli</p>
<p>E. coli</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000000625">UP000000625</a> </p>
<p>4,363</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000000625_83333_ECOLI_v2.tar">Download (453 MB)</a></p>
<p>Glycine max</p>
<p>Soybean</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000008827">UP000008827</a> </p>
<p>55,799</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000008827_3847_SOYBN_v2.tar">Download (7,211 MB)</a></p>
<p>Homo sapiens</p>
<p>Human</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000005640">UP000005640</a> </p>
<p>23,391</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000005640_9606_HUMAN_v2.tar">Download (4,830 MB)</a></p>
<p>Methanocaldococcus jannaschii</p>
<p>M. jannaschii</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000000805">UP000000805</a> </p>
<p>1,773</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000000805_243232_METJA_v2.tar">Download (172 MB)</a></p>
<p>Mus musculus</p>
<p>Mouse</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000000589">UP000000589</a> </p>
<p>21,615</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000000589_10090_MOUSE_v2.tar">Download (3,581 MB)</a></p>
<p>Oryza sativa</p>
<p>Asian rice</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000059680">UP000059680</a> </p>
<p>43,649</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000059680_39947_ORYSJ_v2.tar">Download (4,461 MB)</a></p>
<p>Rattus norvegicus</p>
<p>Rat</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000002494">UP000002494</a> </p>
<p>21,272</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000002494_10116_RAT_v2.tar">Download (3,437 MB)</a></p>
<p>Saccharomyces cerevisiae</p>
<p>Budding yeast</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000002311">UP000002311</a> </p>
<p>6,040</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000002311_559292_YEAST_v2.tar">Download (969 MB)</a></p>
<p>Schizosaccharomyces pombe</p>
<p>Fission yeast</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000002485">UP000002485</a> </p>
<p>5,128</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000002485_284812_SCHPO_v2.tar">Download (783 MB)</a></p>
<p>Zea mays</p>
<p>Maize</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000007305">UP000007305</a> </p>
<p>39,299</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000007305_4577_MAIZE_v2.tar">Download (5,064 MB)</a></p>
<h3 id="Compressed-prediction-files-for-global-health-proteomes"><a href="#Compressed-prediction-files-for-global-health-proteomes" class="headerlink" title="Compressed prediction files for global health proteomes:"></a>Compressed prediction files for global health proteomes:</h3><p>Species</p>
<p>Common Name</p>
<p>Reference Proteome</p>
<p>Predicted Structures</p>
<p>Download</p>
<p>Ajellomyces capsulatus</p>
<p>Ajellomyces capsulatus</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000001631">UP000001631</a> </p>
<p>9,199</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000001631_447093_AJECG_v2.tar">Download (1,351 MB)</a></p>
<p>Brugia malayi</p>
<p>Brugia malayi</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000006672">UP000006672</a> </p>
<p>8,743</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000006672_6279_BRUMA_v2.tar">Download (1,274 MB)</a></p>
<p>Campylobacter jejuni</p>
<p>C. jejuni</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000000799">UP000000799</a> </p>
<p>1,620</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000000799_192222_CAMJE_v2.tar">Download (173 MB)</a></p>
<p>Cladophialophora carrionii</p>
<p>Cladophialophora carrionii</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000094526">UP000094526</a> </p>
<p>11,170</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000094526_86049_9EURO1_v2.tar">Download (1,716 MB)</a></p>
<p>Dracunculus medinensis</p>
<p>Dracunculus medinensis</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000274756">UP000274756</a> </p>
<p>10,834</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000274756_318479_DRAME_v2.tar">Download (1,351 MB)</a></p>
<p>Enterococcus faecium</p>
<p>Enterococcus faecium</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000325664">UP000325664</a> </p>
<p>2,823</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000325664_1352_ENTFC_v2.tar">Download (285 MB)</a></p>
<p>Fonsecaea pedrosoi</p>
<p>Fonsecaea pedrosoi</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000053029">UP000053029</a> </p>
<p>12,509</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000053029_1442368_9EURO2_v2.tar">Download (1,999 MB)</a></p>
<p>Haemophilus influenzae</p>
<p>H. influenzae</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000000579">UP000000579</a> </p>
<p>1,662</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000000579_71421_HAEIN_v2.tar">Download (173 MB)</a></p>
<p>Helicobacter pylori</p>
<p>H. pylori</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000000429">UP000000429</a> </p>
<p>1,538</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000000429_85962_HELPY_v2.tar">Download (165 MB)</a></p>
<p>Klebsiella pneumoniae</p>
<p>K. pneumoniae</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000007841">UP000007841</a> </p>
<p>5,727</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000007841_1125630_KLEPH_v2.tar">Download (554 MB)</a></p>
<p>Leishmania infantum</p>
<p>L. infantum</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000008153">UP000008153</a> </p>
<p>7,924</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000008153_5671_LEIIN_v2.tar">Download (1,496 MB)</a></p>
<p>Madurella mycetomatis</p>
<p>Madurella mycetomatis</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000078237">UP000078237</a> </p>
<p>9,561</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000078237_100816_9PEZI1_v2.tar">Download (1,525 MB)</a></p>
<p>Mycobacterium leprae</p>
<p>Mycobacterium leprae</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000000806">UP000000806</a> </p>
<p>1,602</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000000806_272631_MYCLE_v2.tar">Download (175 MB)</a></p>
<p>Mycobacterium tuberculosis</p>
<p>M. tuberculosis</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000001584">UP000001584</a> </p>
<p>3,988</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000001584_83332_MYCTU_v2.tar">Download (425 MB)</a></p>
<p>Mycobacterium ulcerans</p>
<p>Mycobacterium ulcerans</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000020681">UP000020681</a> </p>
<p>9,033</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000020681_1299332_MYCUL_v2.tar">Download (575 MB)</a></p>
<p>Neisseria gonorrhoeae</p>
<p>N. gonorrhoeae</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000000535">UP000000535</a> </p>
<p>2,106</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000000535_242231_NEIG1_v2.tar">Download (194 MB)</a></p>
<p>Nocardia brasiliensis</p>
<p>Nocardia brasiliensis</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000006304">UP000006304</a> </p>
<p>8,372</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000006304_1133849_9NOCA1_v2.tar">Download (863 MB)</a></p>
<p>Onchocerca volvulus</p>
<p>Onchocerca volvulus</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000024404">UP000024404</a> </p>
<p>12,047</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000024404_6282_ONCVO_v2.tar">Download (1,607 MB)</a></p>
<p>Paracoccidioides lutzii</p>
<p>Paracoccidioides lutzii</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000002059">UP000002059</a> </p>
<p>8,794</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000002059_502779_PARBA_v2.tar">Download (1,284 MB)</a></p>
<p>Plasmodium falciparum</p>
<p>P. falciparum</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000001450">UP000001450</a> </p>
<p>5,187</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000001450_36329_PLAF7_v2.tar">Download (1,142 MB)</a></p>
<p>Pseudomonas aeruginosa</p>
<p>P. aeruginosa</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000002438">UP000002438</a> </p>
<p>5,556</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000002438_208964_PSEAE_v2.tar">Download (608 MB)</a></p>
<p>Salmonella typhimurium</p>
<p>S. typhimurium</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000001014">UP000001014</a> </p>
<p>4,526</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000001014_99287_SALTY_v2.tar">Download (474 MB)</a></p>
<p>Schistosoma mansoni</p>
<p>Schistosoma mansoni</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000008854">UP000008854</a> </p>
<p>13,865</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000008854_6183_SCHMA_v2.tar">Download (2,524 MB)</a></p>
<p>Shigella dysenteriae</p>
<p>S. dysenteriae</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000002716">UP000002716</a> </p>
<p>3,893</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000002716_300267_SHIDS_v2.tar">Download (370 MB)</a></p>
<p>Sporothrix schenckii</p>
<p>Sporothrix schenckii</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000018087">UP000018087</a> </p>
<p>8,652</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000018087_1391915_SPOS1_v2.tar">Download (1,507 MB)</a></p>
<p>Staphylococcus aureus</p>
<p>S. aureus</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000008816">UP000008816</a> </p>
<p>2,888</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000008816_93061_STAA8_v2.tar">Download (271 MB)</a></p>
<p>Streptococcus pneumoniae</p>
<p>S. pneumoniae</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000000586">UP000000586</a> </p>
<p>2,030</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000000586_171101_STRR6_v2.tar">Download (201 MB)</a></p>
<p>Strongyloides stercoralis</p>
<p>Strongyloides stercoralis</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000035681">UP000035681</a> </p>
<p>12,613</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000035681_6248_STRER_v2.tar">Download (1,878 MB)</a></p>
<p>Trichuris trichiura</p>
<p>Trichuris trichiura</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000030665">UP000030665</a> </p>
<p>9,564</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000030665_36087_TRITR_v2.tar">Download (1,350 MB)</a></p>
<p>Trypanosoma brucei</p>
<p>Trypanosoma brucei</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000008524">UP000008524</a> </p>
<p>8,491</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000008524_185431_TRYB2_v2.tar">Download (1,334 MB)</a></p>
<p>Trypanosoma cruzi</p>
<p>T. cruzi</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000002296">UP000002296</a> </p>
<p>19,036</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000002296_353153_TRYCC_v2.tar">Download (2,934 MB)</a></p>
<p>Wuchereria bancrofti</p>
<p>Wuchereria bancrofti</p>
<p><a target="_blank" rel="noopener" href="https://www.uniprot.org/proteomes/UP000270924">UP000270924</a> </p>
<p>12,721</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000270924_6293_WUCBA_v2.tar">Download (1,404 MB)</a></p>
<h3 id="Compressed-prediction-files-for-Swiss-Prot"><a href="#Compressed-prediction-files-for-Swiss-Prot" class="headerlink" title="Compressed prediction files for Swiss-Prot:"></a>Compressed prediction files for Swiss-Prot:</h3><p>File type</p>
<p>Predicted Structures</p>
<p>Download</p>
<p>Swiss-Prot (CIF files)</p>
<p>542,380</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/swissprot_cif_v2.tar">Download (36,896 MB)</a></p>
<p>Swiss-Prot (PDB files)</p>
<p>542,380</p>
<p><a target="_blank" rel="noopener" href="https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/swissprot_pdb_v2.tar">Download (26,935 MB)</a></p>
<h3 id="License-and-attribution-1"><a href="#License-and-attribution-1" class="headerlink" title="License and attribution"></a>License and attribution</h3><p>Data is available for academic and commercial use, under a <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by/4.0/">CC-BY-4.0</a> license.  </p>
<p>EMBL-EBI expects attribution (e.g. in publications, services or products) for any of its online services, databases or software in accordance with good scientific practice.  </p>
<p>If you make use of an AlphaFold prediction, please cite the following papers:<br><a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41586-021-03819-2">Jumper, J <em>et al</em>. Highly accurate protein structure prediction with AlphaFold. <em>Nature</em> (2021).</a><a target="_blank" rel="noopener" href="https://academic.oup.com/nar/advance-article/doi/10.1093/nar/gkab1061/6430488">Varadi, M <em>et al</em>. AlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space with high-accuracy models. <em>Nucleic Acids Research</em> (2021).</a>  </p>
<p>AlphaFold Data Copyright (2021) DeepMind Technologies Limited.</p>
<h3 id="Feedback"><a href="#Feedback" class="headerlink" title="Feedback"></a>Feedback</h3><p>If you want to share feedback on an AlphaFold structure prediction please contact <a href="mailto:alphafold@deepmind.com">alphafold@deepmind.com</a>. If you have feedback on the website or experience any bugs please contact <a href="mailto:afdbhelp@ebi.ac.uk">afdbhelp@ebi.ac.uk</a>.</p>
<h3 id="Disclaimer"><a href="#Disclaimer" class="headerlink" title="Disclaimer"></a>Disclaimer</h3><p>The AlphaFold Data and other information provided on this site is for theoretical modelling only, caution should be exercised in its use. It is provided ‘as-is’ without any warranty of any kind, whether expressed or implied. For clarity, no warranty is given that use of the information shall not infringe the rights of any third party. The information is not intended to be a substitute for professional medical advice, diagnosis, or treatment, and does not constitute medical or other professional advice.  </p>
<p>Use of the AlphaFold Protein Structure Database is subject to EMBL-EBI <a target="_blank" rel="noopener" href="https://www.ebi.ac.uk/about/terms-of-use/">Terms of Use</a>.</p>
<h1 id="Protein-Structure-Prediction"><a href="#Protein-Structure-Prediction" class="headerlink" title="Protein Structure Prediction"></a>Protein Structure Prediction</h1><ul>
<li><a target="_blank" rel="noopener" href="https://cn.bing.com/search?q=protein+structure+prediction&qs=AS&pq=protein+structure+pre&sc=8-21&cvid=ACF370F7468E49F694B8476B4C49F438&sp=1&ghc=1&first=5&FORM=PORE">protein structure prediction - Bing</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/labs/pmc/articles/PMC6407873/">Protein structure prediction</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/labs/pmc/articles/PMC8387240/">Highly accurate protein structure prediction for the human proteome</a></li>
<li><a target="_blank" rel="noopener" href="http://raptorx.uchicago.edu/StructurePrediction/predict/">Submit a Prediction Job</a></li>
<li><a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41580-019-0163-x">Advances in protein structure prediction and design | Nature Reviews Molecular Cell Biology</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/sites/ppmc/articles/PMC1302447/">A physical approach to protein structure prediction.</a></li>
<li><a target="_blank" rel="noopener" href="https://www.predictioncenter.org/">Home - Prediction Center</a></li>
<li><a target="_blank" rel="noopener" href="https://www.alphafold.ebi.ac.uk/download">AlphaFold Protein Structure Database</a></li>
<li><a target="_blank" rel="noopener" href="https://www.uniprot.org/uniprot/A0A0R4IPD8">znfx1 - Zinc finger, NFX1-type-containing 1 - Danio rerio (Zebrafish) - znfx1 gene &amp; protein</a></li>
<li><a target="_blank" rel="noopener" href="https://www.alphafold.ebi.ac.uk/">AlphaFold Protein Structure Database</a></li>
</ul>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/Log//" class="article-tag-list-link color4">Log</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2022/03/08/20220308/">Expand all items >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-20220306" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/03/06/20220306/">20220306</a>
    </h1>
  

        
        <a href="/2022/03/06/20220306/" class="archive-article-date">
  	<time datetime="2022-03-05T16:00:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>20220306</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
    

      
<div id="toc" class="toc-article">
  <strong class="toc-title">Contents</strong>
  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Working-directories"><span class="toc-number">1.</span> <span class="toc-text">Working directories</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Randomly-select-a-file-from-a-directory"><span class="toc-number">2.</span> <span class="toc-text">Randomly select a file from a directory</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Zinc-finger"><span class="toc-number">3.</span> <span class="toc-text">Zinc finger</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CRAN"><span class="toc-number">4.</span> <span class="toc-text">CRAN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ITIS"><span class="toc-number">5.</span> <span class="toc-text">ITIS</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Integrated-Taxonomic-Information-System-Database-Download"><span class="toc-number">5.1.</span> <span class="toc-text">Integrated Taxonomic Information System - Database Download</span></a></li></ol></li></ol>
</div>

        <h1 id="Working-directories"><a href="#Working-directories" class="headerlink" title="Working directories"></a>Working directories</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">##############################################################</span><br><span class="line"># Journal club for NFX1</span><br><span class="line"># </span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Work&#x2F;Projects&#x2F;PABP&#x2F;Summary&#x2F;PABP_Project.pptx; \</span><br><span class="line">setsid wps $&#123;i&#125;</span><br><span class="line"># NCBI-BLAST+ installation</span><br><span class="line">i&#x3D;https:&#x2F;&#x2F;ftp.ncbi.nlm.nih.gov&#x2F;blast&#x2F;executables&#x2F;blast+&#x2F;LATEST&#x2F;ncbi-blast-2.12.0+-x64-linux.tar.gz</span><br><span class="line">axel $&#123;i&#125;</span><br><span class="line">tar zxvpf *.gz</span><br></pre></td></tr></table></figure>
<h1 id="Randomly-select-a-file-from-a-directory"><a href="#Randomly-select-a-file-from-a-directory" class="headerlink" title="Randomly select a file from a directory"></a>Randomly select a file from a directory</h1><ul>
<li><a target="_blank" rel="noopener" href="https://unix.stackexchange.com/questions/553674/how-to-pick-a-random-file-from-a-folder-without-repetition-using-bash">find - How to pick a random file from a folder without repetition using bash? - Unix &amp; Linux Stack Exchange</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/414164/how-can-i-select-random-files-from-a-directory-in-bash">How can I select random files from a directory in bash? - Stack Overflow</a></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ls |sort -R |tail -<span class="variable">$N</span> |<span class="keyword">while</span> <span class="built_in">read</span> file; <span class="keyword">do</span></span><br><span class="line">    <span class="comment"># Something involving $file, or you can leave</span></span><br><span class="line">    <span class="comment"># off the while to just get the filenames</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ls dirname | shuf -n 1</span><br><span class="line"><span class="comment"># probably faster and more flexible:</span></span><br><span class="line">find dirname -<span class="built_in">type</span> f | shuf -n 1</span><br><span class="line"><span class="comment"># etc..</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line">shuf -ezn 5 * | xargs -0 -n1 <span class="built_in">echo</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line">LENGTH=<span class="variable">$&#123;#ARRAY[@]&#125;</span></span><br><span class="line">RANDOM=<span class="variable">$&#123;a[RANDOM%$LENGTH]&#125;</span></span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a=( * )</span><br><span class="line">randf=( <span class="string">&quot;<span class="variable">$&#123;a[RANDOM%<span class="variable">$&#123;#a[@]&#125;</span>]&quot;&#123;1..42&#125;</span>&quot;</span>&#125;<span class="string">&quot; )</span></span><br></pre></td></tr></table></figure>
<h1 id="Zinc-finger"><a href="#Zinc-finger" class="headerlink" title="Zinc finger"></a>Zinc finger</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;www.spandidos-publications.com&#x2F;10.3892&#x2F;ijo.2019.4860# https:&#x2F;&#x2F;www.uniprot.org&#x2F;uniprot&#x2F;Q9FFK8 https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;Structure&#x2F;cdd&#x2F;cd06008 https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;protein&#x2F;72391068 https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Zinc_finger#Zn2&#x2F;Cys6 https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Conserved_Domain_Database https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;Structure&#x2F;cdd&#x2F;cdd.shtml http:&#x2F;&#x2F;genexplain.com&#x2F;tfclass&#x2F;huTF_classification_Families.html https:&#x2F;&#x2F;www.frontiersin.org&#x2F;articles&#x2F;10.3389&#x2F;fpls.2020.00115&#x2F;full https:&#x2F;&#x2F;academic.oup.com&#x2F;nar&#x2F;article&#x2F;31&#x2F;2&#x2F;532&#x2F;2375953 https:&#x2F;&#x2F;www.annualreviews.org&#x2F;doi&#x2F;pdf&#x2F;10.1146&#x2F;annurev-biochem-010909-095056 https:&#x2F;&#x2F;epigenie.com&#x2F;key-epigenetic-players&#x2F;chromatin-modifying-and-dna-binding-proteins&#x2F;zinc-finger-proteins&#x2F; https:&#x2F;&#x2F;jbiomedsci.biomedcentral.com&#x2F;articles&#x2F;10.1186&#x2F;s12929-016-0269-9 https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;cddiscovery201771 https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;gene&#x2F;4799</span><br><span class="line"></span><br><span class="line">https:&#x2F;&#x2F;journals.plos.org&#x2F;plosone&#x2F;article?id&#x3D;10.1371&#x2F;journal.pone.0042578 https:&#x2F;&#x2F;www.ebi.ac.uk&#x2F;interpro&#x2F;entry&#x2F;InterPro&#x2F;IPR000967&#x2F;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li><a target="_blank" rel="noopener" href="https://pubmed.ncbi.nlm.nih.gov/31777944/">CDD/SPARCLE: the conserved domain database in 2020 - PubMed</a>, Database</li>
</ul>
<ul>
<li><a target="_blank" rel="noopener" href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0042578">Comparative Analysis of Zinc Finger Proteins Involved in Plant Disease Resistance</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ebi.ac.uk/interpro/entry/InterPro/IPR000967/">Zinc finger, NF-X1-type (IPR000967) - InterPro entry - InterPro</a></li>
<li><a target="_blank" rel="noopener" href="https://www.google.com/search?q=nf-x1-type+zinc+finger+protein+dna+sequence&newwindow=1&client=ubuntu&bih=528&biw=1178&hl=en&sxsrf=APq-WBsjftcGZ8GQFc6ccd1U5lmgxVJyLQ:1646396571433&ei=mwQiYrj4GcOTwPAPl9WI-AU&oq=nf-x1-type+zinc+finger+protein+dna+&gs_lcp=Cgdnd3Mtd2l6EAMYAEoECEEYAEoECEYYAFAAWABgAGgAcAB4AIABAIgBAJIBAJgBAA&sclient=gws-wiz">nf-x1-type zinc finger protein dna sequence - Google Search</a></li>
<li><a target="_blank" rel="noopener" href="https://www.spandidos-publications.com/10.3892/ijo.2019.4860#">Characterization of the human zinc finger nfx‑1‑type containing 1 encoding ZNFX1 gene and its response to 12‑O‑tetradecanoyl‑13‑acetate in HL‑60 cells</a></li>
<li><a target="_blank" rel="noopener" href="https://www.uniprot.org/uniprot/Q9FFK8">NFXL2 - NF-X1-type zinc finger protein NFXL2 - Arabidopsis thaliana (Mouse-ear cress) - NFXL2 gene &amp; protein</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/Structure/cdd/cd06008">CDD Conserved Protein Domain Family: NF-X1-zinc-finger</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/protein/72391068">hypothetical protein, conserved [Trypanosoma brucei brucei TREU927] - Protein - NCBI</a></li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Zinc_finger#Zn2/Cys6">Zinc finger - Wikipedia</a></li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Conserved_Domain_Database">Conserved Domain Database - Wikipedia</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml">Conserved Domains Database (CDD) and Resources</a></li>
<li><a target="_blank" rel="noopener" href="http://genexplain.com/tfclass/huTF_classification_Families.html">Classification of human transcription factors - TFClass</a></li>
<li><a target="_blank" rel="noopener" href="https://www.frontiersin.org/articles/10.3389/fpls.2020.00115/full">Frontiers | C2H2 Zinc Finger Proteins: Master Regulators of Abiotic Stress Responses in Plants | Plant Science</a></li>
<li><a target="_blank" rel="noopener" href="https://academic.oup.com/nar/article/31/2/532/2375953">Structural classification of zinc fingers | Nucleic Acids Research | Oxford Academic</a></li>
<li><a target="_blank" rel="noopener" href="https://www.annualreviews.org/doi/pdf/10.1146/annurev-biochem-010909-095056">The Discovery of Zinc Fingers and Their Applications in Gene Regulation and Genome Manipulation | Annual Review of Biochemistry</a></li>
<li><a target="_blank" rel="noopener" href="https://epigenie.com/key-epigenetic-players/chromatin-modifying-and-dna-binding-proteins/zinc-finger-proteins/">Zinc Finger Proteins Review</a></li>
<li><a target="_blank" rel="noopener" href="https://jbiomedsci.biomedcentral.com/articles/10.1186/s12929-016-0269-9">Zinc finger proteins in cancer progression | Journal of Biomedical Science | Full Text</a></li>
<li><a target="_blank" rel="noopener" href="https://www.nature.com/articles/cddiscovery201771">Zinc-finger proteins in health and disease | Cell Death Discovery</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/gene/4799">NFX1 nuclear transcription factor, X-box binding 1 [Homo sapiens (human)] - Gene - NCBI</a></li>
</ul>
<h1 id="CRAN"><a href="#CRAN" class="headerlink" title="CRAN"></a>CRAN</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># ERROR: dependencies ‘crul’, ‘ape’, ‘bold’, ‘rredlist’, ‘rotl’,&#39; ‘ritis’, ‘worrms’, ‘natserv’, ‘wikitaxa’, ‘phangorn’ are not available for package ‘taxize’</span><br><span class="line"># </span><br><span class="line"># ERROR: dependencies &#39;crul&#39;, &#39;ape&#39;, &#39;bold&#39;, &#39;rredlist&#39;, &#39;rotl&#39;,&#39; &#39;ritis&#39;, &#39;worrms&#39;, &#39;natserv&#39;, &#39;wikitaxa&#39;, &#39;phangorn&#39; are not available for package &#39;taxize&#39;</span><br><span class="line"># </span><br><span class="line">i &lt;- c(&#39;crul&#39;, &#39;ape&#39;, &#39;bold&#39;, &#39;rredlist&#39;, &#39;rotl&#39;, &#39;ritis&#39;, &#39;worrms&#39;, &#39;natserv&#39;, &#39;wikitaxa&#39;, &#39;phangorn&#39;)</span><br><span class="line">install.packages(i, dependencies &#x3D; TRUE)</span><br><span class="line">install.packages(&quot;taxizedb&quot;, dependencies &#x3D; TRUE)</span><br></pre></td></tr></table></figure>
<h1 id="ITIS"><a href="#ITIS" class="headerlink" title="ITIS"></a>ITIS</h1><p><img src="https://www.itis.gov/Static/images/ITIS-Logo.jpg" alt="ITIS"></p>
<h2 id="Integrated-Taxonomic-Information-System-Database-Download"><a href="#Integrated-Taxonomic-Information-System-Database-Download" class="headerlink" title="Integrated Taxonomic Information System - Database Download"></a>Integrated Taxonomic Information System - Database Download</h2><ul>
<li>  <a target="_blank" rel="noopener" href="https://www.itis.gov/">Home</a></li>
<li>  <a target="_blank" rel="noopener" href="https://www.itis.gov/downloads/index.html#">About ITIS</a></li>
<li>  <a target="_blank" rel="noopener" href="https://www.itis.gov/downloads/index.html#">Data Access and Tools</a></li>
<li>  <a target="_blank" rel="noopener" href="https://www.itis.gov/downloads/index.html#">Get ITIS Data</a></li>
<li>  <a target="_blank" rel="noopener" href="https://www.itis.gov/downloads/index.html#">Submit and Update Data</a></li>
</ul>
<p> </p>
<p>ITIS Downloads</p>
<p>The following files are available for download:</p>
<p>ITIS Database Downloads</p>
<p>  <strong>Note:</strong> Database download files are currently from the <strong>28-Feb-2022</strong> data load.</p>
<p>  Full ITIS Data Set (Informix 7) – <a target="_blank" rel="noopener" href="https://www.itis.gov/downloads/itisInformix.tar.gz">itisInformix.tar.gz</a></p>
<p>  Full ITIS Data Set (MS SQL Server) – <a target="_blank" rel="noopener" href="https://www.itis.gov/downloads/itisMSSql.zip">itisMSSql.zip</a></p>
<p>  Full ITIS Data Set (MySQL bulk load) – <a target="_blank" rel="noopener" href="https://www.itis.gov/downloads/itisMySQLBulk.zip">itisMySQLBulk.zip</a></p>
<p>  Full ITIS Data Set (MySQL by table) – <a target="_blank" rel="noopener" href="https://www.itis.gov/downloads/itisMySQLTables.tar.gz">itisMySQLTables.tar.gz</a></p>
<p>  Full ITIS Data Set (PostgreSql) – <a target="_blank" rel="noopener" href="https://www.itis.gov/downloads/itisPostgreSql.zip">itisPostgreSql.zip</a></p>
<p>  Full ITIS Data Set (SQLite) – <a target="_blank" rel="noopener" href="https://www.itis.gov/downloads/itisSqlite.zip">itisSqlite.zip</a></p>
<p>Other ITIS Downloads</p>
<p>  Taxonomic Workbench (TWB) Executable – <a target="_blank" rel="noopener" href="https://www.itis.gov/downloads/itis.exe">TWB Executable</a></p>
<p>  Download Validation Instructions – <a target="_blank" rel="noopener" href="https://www.itis.gov/downloads/MD5_Instr.txt">MD5_Instr.txt</a></p>
<p>  Download Validation File – <a target="_blank" rel="noopener" href="https://www.itis.gov/downloads/MD5SUMS">MD5SUMS</a></p>
<p><img src="https://www.itis.gov/Static/images/spacer.gif"></p>
<p><img src="https://www.itis.gov/Static/images/spacer.gif" alt="spacing image"></p>
<p><img src="https://www.itis.gov/Static/images/spacer.gif" alt="spacing image"></p>
<p><img src="https://www.itis.gov/Static/images/spacer.gif" alt="spacing image"></p>
<p><img src="https://www.itis.gov/Static/images/spacer.gif" alt="spacing image"></p>
<p> </p>
<p> </p>
<p>Please let us know at the <a href="mailto:itiswebmaster@itis.gov">ITIS Webmaster</a> address if you have issues using these database files.</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/Log//" class="article-tag-list-link color4">Log</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2022/03/06/20220306/">Expand all items >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-20220303" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/03/03/20220303/">20220303</a>
    </h1>
  

        
        <a href="/2022/03/03/20220303/" class="archive-article-date">
  	<time datetime="2022-03-02T16:00:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>20220303</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
    

      
<div id="toc" class="toc-article">
  <strong class="toc-title">Contents</strong>
  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Working-directories"><span class="toc-number">1.</span> <span class="toc-text">Working directories</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Movie-sites"><span class="toc-number">2.</span> <span class="toc-text">Movie sites</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Sublime-Text"><span class="toc-number">3.</span> <span class="toc-text">Sublime Text</span></a></li></ol>
</div>

        <h1 id="Working-directories"><a href="#Working-directories" class="headerlink" title="Working directories"></a>Working directories</h1><h1 id="Movie-sites"><a href="#Movie-sites" class="headerlink" title="Movie sites"></a>Movie sites</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.nbys.tv/">泥巴影院</a></li>
<li><a target="_blank" rel="noopener" href="http://silisili.in/">嘶哩嘶哩 silisili.in[S站]</a></li>
<li><a target="_blank" rel="noopener" href="http://jlpcn.net/">纪录片</a></li>
<li><a target="_blank" rel="noopener" href="http://www.520ddtv.com/">闪电影视</a></li>
<li><a target="_blank" rel="noopener" href="https://o8tv.com/vodtype/1.html">555电影网</a></li>
<li><a target="_blank" rel="noopener" href="https://m.88hd.com/">88影视网</a></li>
<li><a target="_blank" rel="noopener" href="https://aidi.tv/">爱迪影视 </a></li>
<li><a target="_blank" rel="noopener" href="https://939394.xyz/">粤正影视</a></li>
</ul>
<h1 id="Sublime-Text"><a href="#Sublime-Text" class="headerlink" title="Sublime Text"></a>Sublime Text</h1><p>Usage: sublime_text [arguments] [files]         edit the given files<br>   or: sublime_text [arguments] [directories]   open the given directories</p>
<p>Arguments:<br>  –project <project>: Load the given project<br>  –command <command>: Run the given command<br>  -n or –new-window:  Open a new window<br>  -a or –add:         Add folders to the current window<br>  -w or –wait:        Wait for the files to be closed before returning<br>  -b or –background:  Don’t activate the application<br>  -h or –help:        Show help (this message) and exit<br>  -v or –version:     Show version and exit</p>
<p>Filenames may be given a :line or :line:column suffix to open at a specific<br>location.</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/Log//" class="article-tag-list-link color4">Log</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2022/03/03/20220303/">Expand all items >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-20220227" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/02/27/20220227/">20220227</a>
    </h1>
  

        
        <a href="/2022/02/27/20220227/" class="archive-article-date">
  	<time datetime="2022-02-26T16:00:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>20220227</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
    

      
<div id="toc" class="toc-article">
  <strong class="toc-title">Contents</strong>
  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Enable-two-finger-scroll-via-Settings-in-Windows-10"><span class="toc-number">1.</span> <span class="toc-text">Enable two-finger scroll via Settings in Windows 10</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#OpenCV-Ref"><span class="toc-number">2.</span> <span class="toc-text">OpenCV Ref</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Extract-faces-from-images"><span class="toc-number">3.</span> <span class="toc-text">Extract faces from images</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Extracting-faces-using-OpenCV-Face-Detection-Neural-Network"><span class="toc-number">3.1.</span> <span class="toc-text">Extracting faces using OpenCV Face Detection Neural Network</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Aim"><span class="toc-number">3.1.1.</span> <span class="toc-text">Aim</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Project"><span class="toc-number">3.1.2.</span> <span class="toc-text">Project</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Import-libraries"><span class="toc-number">3.1.2.1.</span> <span class="toc-text">Import libraries</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Define-paths-and-load-model"><span class="toc-number">3.1.2.2.</span> <span class="toc-text">Define paths and load model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Create-directory"><span class="toc-number">3.1.2.3.</span> <span class="toc-text">Create directory</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Read-images"><span class="toc-number">3.1.2.4.</span> <span class="toc-text">Read images</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Detect-faces"><span class="toc-number">3.1.2.5.</span> <span class="toc-text">Detect faces</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Create-boxes-around-faces"><span class="toc-number">3.1.2.6.</span> <span class="toc-text">1. Create boxes around faces</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Extracting-faces"><span class="toc-number">3.1.2.7.</span> <span class="toc-text">2. Extracting faces</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusion"><span class="toc-number">3.1.3.</span> <span class="toc-text">Conclusion</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#OpenCV-Face-Recognition"><span class="toc-number">3.2.</span> <span class="toc-text">OpenCV Face Recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Looking-for-the-source-code-to-this-post"><span class="toc-number">3.2.0.0.1.</span> <span class="toc-text">Looking for the source code to this post?</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#OpenCV-Face-Recognition-1"><span class="toc-number">3.2.1.</span> <span class="toc-text">OpenCV Face Recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#How-OpenCV%E2%80%99s-face-recognition-works"><span class="toc-number">3.2.1.1.</span> <span class="toc-text">How OpenCV’s face recognition works</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Our-face-recognition-dataset"><span class="toc-number">3.2.1.2.</span> <span class="toc-text">Our face recognition dataset</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Project-structure"><span class="toc-number">3.2.1.3.</span> <span class="toc-text">Project structure</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-1-Extract-embeddings-from-face-dataset"><span class="toc-number">3.2.1.4.</span> <span class="toc-text">Step #1: Extract embeddings from face dataset</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#import-the-necessary-packages"><span class="toc-number">3.3.</span> <span class="toc-text">import the necessary packages</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#construct-the-argument-parser-and-parse-the-arguments"><span class="toc-number">3.4.</span> <span class="toc-text">construct the argument parser and parse the arguments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-detector-from-disk"><span class="toc-number">3.5.</span> <span class="toc-text">load our serialized face detector from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-embedding-model-from-disk"><span class="toc-number">3.6.</span> <span class="toc-text">load our serialized face embedding model from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#grab-the-paths-to-the-input-images-in-our-dataset"><span class="toc-number">3.7.</span> <span class="toc-text">grab the paths to the input images in our dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#initialize-our-lists-of-extracted-facial-embeddings-and"><span class="toc-number">3.8.</span> <span class="toc-text">initialize our lists of extracted facial embeddings and</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#corresponding-people-names"><span class="toc-number">3.9.</span> <span class="toc-text">corresponding people names</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#initialize-the-total-number-of-faces-processed"><span class="toc-number">3.10.</span> <span class="toc-text">initialize the total number of faces processed</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#loop-over-the-image-paths"><span class="toc-number">3.11.</span> <span class="toc-text">loop over the image paths</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dump-the-facial-embeddings-names-to-disk"><span class="toc-number">3.12.</span> <span class="toc-text">dump the facial embeddings + names to disk</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-2-Train-face-recognition-model"><span class="toc-number">3.12.0.1.</span> <span class="toc-text">Step #2: Train face recognition model</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#import-the-necessary-packages-1"><span class="toc-number">3.13.</span> <span class="toc-text">import the necessary packages</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#construct-the-argument-parser-and-parse-the-arguments-1"><span class="toc-number">3.14.</span> <span class="toc-text">construct the argument parser and parse the arguments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-the-face-embeddings"><span class="toc-number">3.15.</span> <span class="toc-text">load the face embeddings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#encode-the-labels"><span class="toc-number">3.16.</span> <span class="toc-text">encode the labels</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#train-the-model-used-to-accept-the-128-d-embeddings-of-the-face-and"><span class="toc-number">3.17.</span> <span class="toc-text">train the model used to accept the 128-d embeddings of the face and</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#then-produce-the-actual-face-recognition"><span class="toc-number">3.18.</span> <span class="toc-text">then produce the actual face recognition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#write-the-actual-face-recognition-model-to-disk"><span class="toc-number">3.19.</span> <span class="toc-text">write the actual face recognition model to disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#write-the-label-encoder-to-disk"><span class="toc-number">3.20.</span> <span class="toc-text">write the label encoder to disk</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-3-Recognize-faces-with-OpenCV"><span class="toc-number">3.20.0.1.</span> <span class="toc-text">Step #3: Recognize faces with OpenCV</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#import-the-necessary-packages-2"><span class="toc-number">3.21.</span> <span class="toc-text">import the necessary packages</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#construct-the-argument-parser-and-parse-the-arguments-2"><span class="toc-number">3.22.</span> <span class="toc-text">construct the argument parser and parse the arguments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-detector-from-disk-1"><span class="toc-number">3.23.</span> <span class="toc-text">load our serialized face detector from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-embedding-model-from-disk-1"><span class="toc-number">3.24.</span> <span class="toc-text">load our serialized face embedding model from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-the-actual-face-recognition-model-along-with-the-label-encoder"><span class="toc-number">3.25.</span> <span class="toc-text">load the actual face recognition model along with the label encoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-the-image-resize-it-to-have-a-width-of-600-pixels-while"><span class="toc-number">3.26.</span> <span class="toc-text">load the image, resize it to have a width of 600 pixels (while</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#maintaining-the-aspect-ratio-and-then-grab-the-image-dimensions"><span class="toc-number">3.27.</span> <span class="toc-text">maintaining the aspect ratio), and then grab the image dimensions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#construct-a-blob-from-the-image"><span class="toc-number">3.28.</span> <span class="toc-text">construct a blob from the image</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#apply-OpenCV%E2%80%99s-deep-learning-based-face-detector-to-localize"><span class="toc-number">3.29.</span> <span class="toc-text">apply OpenCV’s deep learning-based face detector to localize</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#faces-in-the-input-image"><span class="toc-number">3.30.</span> <span class="toc-text">faces in the input image</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#loop-over-the-detections"><span class="toc-number">3.31.</span> <span class="toc-text">loop over the detections</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#show-the-output-image"><span class="toc-number">3.32.</span> <span class="toc-text">show the output image</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#BONUS-Recognize-faces-in-video-streams"><span class="toc-number">3.32.0.1.</span> <span class="toc-text">BONUS: Recognize faces in video streams</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#import-the-necessary-packages-3"><span class="toc-number">3.33.</span> <span class="toc-text">import the necessary packages</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#construct-the-argument-parser-and-parse-the-arguments-3"><span class="toc-number">3.34.</span> <span class="toc-text">construct the argument parser and parse the arguments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-detector-from-disk-2"><span class="toc-number">3.35.</span> <span class="toc-text">load our serialized face detector from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-embedding-model-from-disk-2"><span class="toc-number">3.36.</span> <span class="toc-text">load our serialized face embedding model from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-the-actual-face-recognition-model-along-with-the-label-encoder-1"><span class="toc-number">3.37.</span> <span class="toc-text">load the actual face recognition model along with the label encoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#initialize-the-video-stream-then-allow-the-camera-sensor-to-warm-up"><span class="toc-number">3.38.</span> <span class="toc-text">initialize the video stream, then allow the camera sensor to warm up</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#start-the-FPS-throughput-estimator"><span class="toc-number">3.39.</span> <span class="toc-text">start the FPS throughput estimator</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#loop-over-frames-from-the-video-file-stream"><span class="toc-number">3.40.</span> <span class="toc-text">loop over frames from the video file stream</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#stop-the-timer-and-display-FPS-information"><span class="toc-number">3.41.</span> <span class="toc-text">stop the timer and display FPS information</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#do-a-bit-of-cleanup"><span class="toc-number">3.42.</span> <span class="toc-text">do a bit of cleanup</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Drawbacks-limitations-and-how-to-obtain-higher-face-recognition-accuracy"><span class="toc-number">3.42.0.1.</span> <span class="toc-text">Drawbacks, limitations, and how to obtain higher face recognition accuracy</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#You-may-need-more-data"><span class="toc-number">3.42.0.1.1.</span> <span class="toc-text">You may need more data</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Perform-face-alignment"><span class="toc-number">3.42.0.1.2.</span> <span class="toc-text">Perform face alignment</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Tune-your-hyperparameters"><span class="toc-number">3.42.0.1.3.</span> <span class="toc-text">Tune your hyperparameters</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Use-dlib%E2%80%99s-embedding-model-but-not-it%E2%80%99s-k-NN-for-face-recognition"><span class="toc-number">3.42.0.1.4.</span> <span class="toc-text">Use dlib’s embedding model (but not it’s k-NN for face recognition)</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Did-you-encounter-a-%E2%80%9CUSAGE%E2%80%9D-error-running-today%E2%80%99s-Python-face-recognition-scripts"><span class="toc-number">3.42.0.2.</span> <span class="toc-text">Did you encounter a “USAGE” error running today’s Python face recognition scripts?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Alternative-OpenCV-face-recognition-methods"><span class="toc-number">3.42.0.3.</span> <span class="toc-text">Alternative OpenCV face recognition methods</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#What%E2%80%99s-next-I-recommend-PyImageSearch-University"><span class="toc-number">3.42.0.4.</span> <span class="toc-text">What’s next? I recommend PyImageSearch University.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary"><span class="toc-number">3.42.1.</span> <span class="toc-text">Summary</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Download-the-Source-Code-and-FREE-17-page-Resource-Guide"><span class="toc-number">3.42.1.0.1.</span> <span class="toc-text">Download the Source Code and FREE 17-page Resource Guide</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#About-the-Author"><span class="toc-number">3.42.1.0.2.</span> <span class="toc-text">About the Author</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reader-Interactions"><span class="toc-number">3.42.2.</span> <span class="toc-text">Reader Interactions</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#pip-install-OpenCV"><span class="toc-number">3.42.2.1.</span> <span class="toc-text">pip install OpenCV</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Install-OpenCV-4-on-your-Raspberry-Pi"><span class="toc-number">3.42.2.2.</span> <span class="toc-text">Install OpenCV 4 on your Raspberry Pi</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#359-responses-to-OpenCV-Face-Recognition"><span class="toc-number">3.42.2.3.</span> <span class="toc-text">359 responses to: OpenCV Face Recognition</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#do-a-bit-of-cleanup-1"><span class="toc-number">3.43.</span> <span class="toc-text">do a bit of cleanup</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Comment-section"><span class="toc-number">3.43.0.1.</span> <span class="toc-text">Comment section</span></a></li></ol></li></ol></li></ol></li></ol>
</div>

        <h1 id="Enable-two-finger-scroll-via-Settings-in-Windows-10"><a href="#Enable-two-finger-scroll-via-Settings-in-Windows-10" class="headerlink" title="Enable two-finger scroll via Settings in Windows 10"></a>Enable two-finger scroll via Settings in Windows 10</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.intowindows.com/enable-two-finger-scrolling-windows-8/">How To Enable Two Finger Scrolling In Windows 10</a></li>
</ul>
<p><img src="https://www.intowindows.com/wp-content/uploads/2013/05/enable-two-fingers-to-scroll-in-Windows-10.jpg.webp"></p>
<h1 id="OpenCV-Ref"><a href="#OpenCV-Ref" class="headerlink" title="OpenCV Ref"></a>OpenCV Ref</h1><ul>
<li><a target="_blank" rel="noopener" href="https://docs.opencv.org/3.4/d2/de6/tutorial_py_setup_in_ubuntu.html">OpenCV: Install OpenCV-Python in Ubuntu</a></li>
<li><a target="_blank" rel="noopener" href="https://www.worldscientific.com/doi/10.1142/S0218001408006296">EXTRACTING FACES AND FACIAL FEATURES FROM COLOR IMAGES | International Journal of Pattern Recognition and Artificial Intelligence</a></li>
<li><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/">OpenCV Face Recognition - PyImageSearch</a></li>
<li><a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/cropping-faces-from-images-using-opencv-python/">Cropping Faces from Images using OpenCV - Python - GeeksforGeeks</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/extracting-faces-using-opencv-face-detection-neural-network-475c5cd0c260">Extracting faces using OpenCV Face Detection Neural Network | by Karan Bhanot | Towards Data Science</a></li>
</ul>
<h1 id="Extract-faces-from-images"><a href="#Extract-faces-from-images" class="headerlink" title="Extract faces from images"></a>Extract faces from images</h1><h2 id="Extracting-faces-using-OpenCV-Face-Detection-Neural-Network"><a href="#Extracting-faces-using-OpenCV-Face-Detection-Neural-Network" class="headerlink" title="Extracting faces using OpenCV Face Detection Neural Network"></a>Extracting faces using OpenCV Face Detection Neural Network</h2><ul>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/extracting-faces-using-opencv-face-detection-neural-network-475c5cd0c260">Extracting faces using OpenCV Face Detection Neural Network | by Karan Bhanot | Towards Data Science</a><br><img src="https://miro.medium.com/max/1400/0*25MtyqPCQCSceRx7"></li>
</ul>
<p>Photo by <a target="_blank" rel="noopener" href="https://unsplash.com/@zoner?utm_source=medium&utm_medium=referral">Maxim Dužij</a> on <a target="_blank" rel="noopener" href="https://unsplash.com/?utm_source=medium&utm_medium=referral">Unsplash</a></p>
<p>Recently, I came across the website <a target="_blank" rel="noopener" href="https://www.pyimagesearch.com/">https://www.pyimagesearch.com/</a> which has some of the greatest tutorials on OpenCV. While reading through its numerous articles, I found that OpenCV has its own Face Detection Neural Network with really high accuracy.</p>
<p>So I decided to work on a project using this Neural Network from OpenCV and extract faces from images. Such a process would come handy whenever someone is working with faces and needs to extract them from a number of images.</p>
<p>The complete project is available as a <a target="_blank" rel="noopener" href="https://github.com/kb22/Create-Face-Data-from-Images">GitHub repository</a>. For this article, I’ve taken a picture from my Instagram account.</p>
<p><img src="https://miro.medium.com/max/1400/1*vnewG2OYRUij_b_zm5uNrQ.png"></p>
<p>Image used for extracting face</p>
<h3 id="Aim"><a href="#Aim" class="headerlink" title="Aim"></a>Aim</h3><p>The project has two essential elements:  </p>
<ol>
<li><strong>Box around faces:</strong> Show white boxes around all the faces recognised in the image. The Python file is _data_generator.py_  </li>
<li><strong>Extracted faces:</strong> Extract faces from all images in a folder and save each face into a destination folder to create a handy dataset. The Python file is _face_extractor.py_</li>
</ol>
<p><img src="https://miro.medium.com/max/720/1*Ryd9J1YsNbQPgJkuG6jhSQ.gif"></p>
<p>Face detection and extraction</p>
<p>First, let’s perform the common steps for the two parts, i.e. importing libraries, loading the face detection model, creating output directory, reading images and detecting faces.</p>
<h3 id="Project"><a href="#Project" class="headerlink" title="Project"></a>Project</h3><h4 id="Import-libraries"><a href="#Import-libraries" class="headerlink" title="Import libraries"></a>Import libraries</h4><p>I import <code>os</code> to access various files in the directory. Then, <code>cv2</code> will be used to work with images. <code>numpy</code> helps to easily work with multi-dimensional arrays.</p>
<h4 id="Define-paths-and-load-model"><a href="#Define-paths-and-load-model" class="headerlink" title="Define paths and load model"></a>Define paths and load model</h4><p>The model’s <code>prototxt</code> and <code>caffemodel</code> is provided in the OpenCV repo itself. I used the same and placed them in the <code>model_data</code> directory in my project. <code>prototxt</code> file includes the text description of the network and <code>caffemodel</code> includes the weights. I read the two files and loaded my <code>model</code> using <code>cv2</code>.</p>
<h4 id="Create-directory"><a href="#Create-directory" class="headerlink" title="Create directory"></a>Create directory</h4><p>If the directory where the resultant images will get stored does not exist, I’ll create the directory. The output folder is <strong>updated_images</strong>.</p>
<p>When working with extracting faces, I’ll save the faces into the directory <strong>faces</strong>. If it is not present, I’ll create it.</p>
<h4 id="Read-images"><a href="#Read-images" class="headerlink" title="Read images"></a>Read images</h4><p>I loop through all images inside the <strong>images</strong> folder. After extracting the extension, I check that the files are either of the type <code>.png</code> or <code>.jpg</code> and just operate with those files only.</p>
<h4 id="Detect-faces"><a href="#Detect-faces" class="headerlink" title="Detect faces"></a>Detect faces</h4><p>Using <code>cv2.imread</code>, I read the image, and create a blob using <code>cv2.dnn.blobFromImage</code>. Then, I input this blob into the model and get back the detections from the page using <code>model.forward()</code>.</p>
<p>The common steps are now complete. For the first task, I’ll plot white rectangles around faces and save them in <strong>updated_images</strong> directory. For the second task, I’ll save the extracted faces in <strong>faces</strong> directory.</p>
<h4 id="1-Create-boxes-around-faces"><a href="#1-Create-boxes-around-faces" class="headerlink" title="1. Create boxes around faces"></a>1. Create boxes around faces</h4><p>One by one, I iterate over all of the faces detected in the image and extract their start and end points. Then, I extract the confidence of detection. If the algorithm is more than 50% confident that the detection is a face, I show a rectangle around it.</p>
<p>Then, using <code>cv2.imwrite</code>, I save the image to the <code>updated_images</code> folder with the same name.</p>
<p><img src="https://miro.medium.com/max/1400/1*OOIj2LNRHEFD5bT-FyNbqQ.png"></p>
<p>Image with white rectangle around face</p>
<h4 id="2-Extracting-faces"><a href="#2-Extracting-faces" class="headerlink" title="2. Extracting faces"></a>2. Extracting faces</h4><p>As described above, I iterate all faces, calculate the confidence of detection and if it is more than 50%, I extract the face. Notice the line <code>frame = image[startY:endY, startX:endX]</code>. It extracts the face from the image.</p>
<p>Then, I dump this new image into the <code>faces</code> folder with the name as face number, followed by <code>_</code>, and then the name of the file. If we extracted the first face from an image named <code>sampleImage.png</code>, the name of the face file will be <code>0_sampleImage.png</code>. With each face, I increment the <code>count</code> and after complete execution, I print the count to the console.</p>
<p><img src="https://miro.medium.com/max/508/1*82mRzl-khwb2hBzFi9P4vQ.png"></p>
<p>Extracted face</p>
<p>Finally, the project is ready. You can feed in as many images as possible and generate datasets which can be used for further projects.</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>In this article, I discussed using OpenCV Face Detection Neural Network to detect faces in an image, label them with white rectangles and extract faces into separate images.</p>
<p>As always, I’d love to hear about your thoughts and suggestions.</p>
<p><a target="_blank" rel="noopener" href="https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/towards-data-science/475c5cd0c260&operation=register&redirect=https://towardsdatascience.com/extracting-faces-using-opencv-face-detection-neural-network-475c5cd0c260&user=Karan+Bhanot&userId=10df94b13417&source=post_actions_footer-----475c5cd0c260---------------------clap_footer--------------"></a></p>
<hr>
<h2 id="OpenCV-Face-Recognition"><a href="#OpenCV-Face-Recognition" class="headerlink" title="OpenCV Face Recognition"></a>OpenCV Face Recognition</h2><ul>
<li><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/">OpenCV Face Recognition - PyImageSearch</a></li>
</ul>
<p>by <a target="_blank" rel="noopener" href="https://pyimagesearch.com/author/adrian/">Adrian Rosebrock</a> on September 24, 2018</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#download-the-code">Click here to download the source code to this post</a></p>
<p>Last updated on July 4, 2021.</p>
<p><img src="https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/opencv-face-recognition/opencv_face_reco_animation.gif"></p>
<p><strong>In this tutorial, you will learn how to use OpenCV to perform face recognition.</strong> To build our face recognition system, we’ll first perform face detection, extract face embeddings from each face using deep learning, train a face recognition model on the embeddings, and then finally <strong>recognize faces in both images and video streams with OpenCV.</strong></p>
<p>Today’s tutorial is also a special gift for my fiancée, Trisha (who is now officially my wife). Our wedding was over the weekend, and by the time you’re reading this blog post, we’ll be at the airport preparing to board our flight for the honeymoon.</p>
<p>To celebrate the occasion, and show her how much her support of myself, the PyImageSearch blog, and the PyImageSearch community means to me, I decided to use OpenCV to perform face recognition on a dataset of our faces.</p>
<p><strong>You can swap in your own dataset of faces of course!</strong> All you need to do is follow my directory structure in insert your own face images.</p>
<p>As a bonus, I’ve also included how to label “unknown” faces that cannot be classified with sufficient confidence.</p>
<p><strong>To learn how to perform OpenCV face recognition, <em>just keep reading!</em></strong></p>
<ul>
<li>  <strong>Update July 2021:</strong> Added section on alternative face recognition methods to consider, including how siamese networks can be used for face recognition.</li>
</ul>
<p><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/source-code-icon.png?lossy=1&strip=1&webp=1"></p>
<h5 id="Looking-for-the-source-code-to-this-post"><a href="#Looking-for-the-source-code-to-this-post" class="headerlink" title="Looking for the source code to this post?"></a>Looking for the source code to this post?</h5><p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#download-the-code">JUMP RIGHT TO THE DOWNLOADS SECTION</a> </p>
<h3 id="OpenCV-Face-Recognition-1"><a href="#OpenCV-Face-Recognition-1" class="headerlink" title="OpenCV Face Recognition"></a>OpenCV Face Recognition</h3><p>In today’s tutorial, you will learn how to perform face recognition using the OpenCV library.</p>
<p>You might be wondering how this tutorial is different from the one I wrote a few months back on <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">face recognition with dlib</a>?</p>
<p>Well, keep in mind that the dlib face recognition post relied on two important external libraries:</p>
<ol>
<li> <a target="_blank" rel="noopener" href="http://dlib.net/">dlib</a> (obviously)</li>
<li> <a target="_blank" rel="noopener" href="https://github.com/ageitgey/face_recognition">face_recognition</a> (which is an easy to use set of face recognition utilities that wraps around dlib)</li>
</ol>
<p>While we used OpenCV to <em>facilitate</em> face recognition, OpenCV <em>itself</em> was not responsible for identifying faces.</p>
<p>In today’s tutorial, we’ll learn how we can apply deep learning and OpenCV together (with no other libraries other than scikit-learn) to:</p>
<ol>
<li> Detect faces</li>
<li> Compute 128-d face embeddings to quantify a face</li>
<li> Train a Support Vector Machine (SVM) on top of the embeddings</li>
<li> Recognize faces in images and video streams</li>
</ol>
<p>All of these tasks will be accomplished with OpenCV, enabling us to obtain a “pure” OpenCV face recognition pipeline.</p>
<h4 id="How-OpenCV’s-face-recognition-works"><a href="#How-OpenCV’s-face-recognition-works" class="headerlink" title="How OpenCV’s face recognition works"></a>How OpenCV’s face recognition works</h4><p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_facenet.jpg"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_facenet.jpg?size=800x520&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 1:</strong> An overview of the OpenCV face recognition pipeline. The key step is a CNN feature extractor that generates 128-d facial embeddings. (<a target="_blank" rel="noopener" href="https://cmusatyalab.github.io/openface/">source</a>)</p>
<p>In order to build our OpenCV face recognition pipeline, we’ll be applying deep learning in two key steps:</p>
<ol>
<li> To apply <em>face detection</em>, which detects the <em>presence</em> and location of a face in an image, but does not identify it</li>
<li> To extract the 128-d feature vectors (called “embeddings”) that <em>quantify</em> each face in an image</li>
</ol>
<p>I’ve discussed <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/">how OpenCV’s face detection works</a> previously, so please refer to it if you have not detected faces before.</p>
<p>The model responsible for actually quantifying each face in an image is from the <a target="_blank" rel="noopener" href="https://cmusatyalab.github.io/openface/">OpenFace project</a>, a Python and Torch implementation of face recognition with deep learning. This implementation comes from Schroff et al.’s 2015 CVPR publication, <a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/app/1A_089.pdf"><em>FaceNet: A</em> <em>Unified Embedding for Face Recognition and Clustering</em></a>.</p>
<p>Reviewing the entire FaceNet implementation is outside the scope of this tutorial, but the gist of the pipeline can be seen in <strong>Figure 1</strong> above.</p>
<p>First, we input an image or video frame to our face recognition pipeline. Given the input image, we apply face detection to detect the location of a face in the image.</p>
<p>Optionally we can compute <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/">facial landmarks</a>, enabling us to <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/">preprocess and align the face</a>.</p>
<p>Face alignment, as the name suggests, is the process of (1) identifying the geometric structure of the faces and (2) attempting to obtain a canonical alignment of the face based on translation, rotation, and scale.</p>
<p>While optional, face alignment has been demonstrated to increase face recognition accuracy in some pipelines.</p>
<p>After we’ve (optionally) applied face alignment and cropping, we pass the input face through our deep neural network:</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_training.png"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_training.png?size=600x250&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 2:</strong> How the deep learning face recognition model computes the face embedding.</p>
<p>The FaceNet deep learning model computes a 128-d embedding that quantifies the face itself.</p>
<p>But how does the network actually compute the face embedding?</p>
<p>The answer lies in the training process itself, including:</p>
<ol>
<li> The input data to the network</li>
<li> The triplet loss function</li>
</ol>
<p>To train a face recognition model with deep learning, each input batch of data includes three images:</p>
<ol>
<li> The <em>anchor</em></li>
<li> The <em>positive</em> image</li>
<li> The <em>negative</em> image</li>
</ol>
<p>The anchor is our current face and has identity <em>A</em>.</p>
<p>The second image is our positive image — this image also contains a face of person <em>A</em>.</p>
<p>The negative image, on the other hand, <em><strong>does not have the same identity</strong></em>, and could belong to person <em>B</em>, <em>C</em>, or even <em>Y</em>!</p>
<p>The point is that the anchor and positive image both belong to the same person/face while the negative image does not contain the same face.</p>
<p>The neural network computes the 128-d embeddings for each face and then tweaks the weights of the network (via the triplet loss function) such that:</p>
<ol>
<li> The 128-d embeddings of the anchor and positive image lie closer together</li>
<li> While at the same time, pushing the embeddings for the negative image father away</li>
</ol>
<p>In this manner, the network is able to learn to quantify faces and return highly robust and discriminating embeddings suitable for face recognition.</p>
<p><strong>And furthermore, we can actually</strong> <em>reuse</em> <strong>the OpenFace model for our own applications without having to explicitly train it!</strong></p>
<p>Even though the deep learning model we’re using today has (very likely) <em>never</em> seen the faces we’re about to pass through it, the model will still be able to compute embeddings for each face — ideally, these face embeddings will be sufficiently different such that we can train a “standard” machine learning classifier (SVM, SGD classifier, Random Forest, etc.) on top of the face embeddings, and therefore obtain our OpenCV face recognition pipeline.</p>
<p>If you are interested in learning more about the details surrounding triplet loss and how it can be used to train a face embedding model, be sure to refer to my <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">previous blog post</a> as well as the <a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/app/1A_089.pdf">Schroff et al. publication</a>.</p>
<h4 id="Our-face-recognition-dataset"><a href="#Our-face-recognition-dataset" class="headerlink" title="Our face recognition dataset"></a>Our face recognition dataset</h4><p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_dataset.png"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_dataset.png?size=600x154&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 3:</strong> A small example face dataset for face recognition with OpenCV.</p>
<p>The dataset we are using today contains three people:</p>
<ul>
<li>  Myself</li>
<li>  Trisha (my wife)</li>
<li>  “Unknown”, which is used to represent faces of people we do not know and wish to label as such (here I just sampled faces from the movie <em>Jurassic Park</em> which I used in a <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">previous post</a> — you may want to insert your own “unknown” dataset).</li>
</ul>
<p>As I mentioned in the introduction to today’s face recognition post, I was just married over the weekend, so this post is a “gift” to my new wife ?.</p>
<p>Each class contains a total of six images.</p>
<p>If you are <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/11/how-to-build-a-custom-face-recognition-dataset/">building your own face recognition dataset</a>, ideally, I would suggest having 10-20 images per person you wish to recognize — be sure to refer to the <em>“Drawbacks, limitations, and how to obtain higher face recognition accuracy”</em> section of this blog post for more details.</p>
<h4 id="Project-structure"><a href="#Project-structure" class="headerlink" title="Project structure"></a>Project structure</h4><p>Once you’ve grabbed the zip from the <em><strong>“Downloads”</strong></em> section of this post, go ahead and unzip the archive and navigate into the directory.</p>
<p>From there, you may use the <code>tree</code> command to have the directory structure printed in your terminal:</p>
<p>$ tree –dirsfirst<br>.<br>├── dataset<br>│   ├── adrian [6 images]<br>│   ├── trisha [6 images]<br>│   └── unknown [6 images]<br>├── images<br>│   ├── adrian.jpg<br>│   ├── patrick_bateman.jpg<br>│   └── trisha_adrian.jpg<br>├── face_detection_model<br>│   ├── deploy.prototxt<br>│   └── res10_300x300_ssd_iter_140000.caffemodel<br>├── output<br>│   ├── embeddings.pickle<br>│   ├── le.pickle<br>│   └── recognizer.pickle<br>├── extract_embeddings.py<br>├── openface_nn4.small2.v1.t7<br>├── train_model.py<br>├── recognize.py<br>└── recognize_video.py</p>
<p>7 directories, 31 files</p>
<p>There are quite a few moving parts for this project — <strong>take the time now to carefully read this section so you become familiar with all the files in today’s project.</strong></p>
<p>Our project has four directories in the root folder:</p>
<ul>
<li>  <code>dataset/</code> : Contains our face images organized into subfolders by name.</li>
<li>  <code>images/</code> : Contains three test images that we’ll use to verify the operation of our model.</li>
<li>  <code>face_detection_model/</code> : Contains a pre-trained Caffe deep learning model provided by OpenCV to <em>detect</em> faces. This model <em>detects</em> and <em>localizes</em> faces in an image.</li>
<li><code>output/</code> : Contains my output pickle files. If you’re working with your own dataset, you can store your output files here as well. The output files include:<ul>
<li>  <code>embeddings.pickle</code> : A serialized facial embeddings file. Embeddings have been computed for every face in the dataset and are stored in this file.</li>
<li>  <code>le.pickle</code> : Our label encoder. Contains the name labels for the people that our model can recognize.</li>
<li>  <code>recognizer.pickle</code> : Our Linear Support Vector Machine (SVM) model. This is a machine learning model rather than a deep learning model and it is responsible for actually <em>recognizing</em> faces.</li>
</ul>
</li>
</ul>
<p>Let’s summarize the five files in the root directory:</p>
<ul>
<li>  <code>extract_embeddings.py</code> : We’ll review this file in <strong>Step #1</strong> which is responsible for using a deep learning feature extractor to generate a 128-D vector describing a face. All faces in our dataset will be passed through the neural network to generate embeddings.</li>
<li>  <code>openface_nn4.small2.v1.t7</code> : A Torch deep learning model which produces the 128-D facial embeddings. We’ll be using this deep learning model in <strong>Steps #1, #2, and #3</strong> as well as the <strong>Bonus</strong> section.</li>
<li>  <code>train_model.py</code> : Our Linear SVM model will be trained by this script in <strong>Step #2</strong>. We’ll <em>detect</em> faces, <em>extract</em> embeddings, and <em>fit</em> our SVM model to the embeddings data.</li>
<li>  <code>recognize.py</code> : In <strong>Step #3</strong> and we’ll <em>recognize</em> faces in images. We’ll <em>detect</em> faces, <em>extract</em> embeddings, and <em>query</em> our SVM model to determine <em>who</em> is in an image. We’ll draw boxes around faces and annotate each box with a name.</li>
<li>  <code>recognize_video.py</code> : Our <strong>Bonus</strong> section describes how to <em>recognize who</em> is in frames of a video stream just as we did in <strong>Step #3</strong> on static images.</li>
</ul>
<p>Let’s move on to the first step!</p>
<h4 id="Step-1-Extract-embeddings-from-face-dataset"><a href="#Step-1-Extract-embeddings-from-face-dataset" class="headerlink" title="Step #1: Extract embeddings from face dataset"></a>Step #1: Extract embeddings from face dataset</h4><p>Now that we understand how face recognition works and reviewed our project structure, let’s get started building our OpenCV face recognition pipeline.</p>
<p>Open up the <code>extract_embeddings.py</code> file and insert the following code:</p>
<h2 id="import-the-necessary-packages"><a href="#import-the-necessary-packages" class="headerlink" title="import the necessary packages"></a>import the necessary packages</h2><p>from imutils import paths<br>import numpy as np<br>import argparse<br>import imutils<br>import pickle<br>import cv2<br>import os</p>
<h2 id="construct-the-argument-parser-and-parse-the-arguments"><a href="#construct-the-argument-parser-and-parse-the-arguments" class="headerlink" title="construct the argument parser and parse the arguments"></a>construct the argument parser and parse the arguments</h2><p>ap = argparse.ArgumentParser()<br>ap.add_argument(“-i”, “–dataset”, required=True,<br>    help=”path to input directory of faces + images”)<br>ap.add_argument(“-e”, “–embeddings”, required=True,<br>    help=”path to output serialized db of facial embeddings”)<br>ap.add_argument(“-d”, “–detector”, required=True,<br>    help=”path to OpenCV’s deep learning face detector”)<br>ap.add_argument(“-m”, “–embedding-model”, required=True,<br>    help=”path to OpenCV’s deep learning face embedding model”)<br>ap.add_argument(“-c”, “–confidence”, type=float, default=0.5,<br>    help=”minimum probability to filter weak detections”)<br>args = vars(ap.parse_args())</p>
<p>We import our required packages on <strong>Lines 2-8</strong>. You’ll need to have OpenCV and <code>imutils</code> installed. To install OpenCV, <a target="_blank" rel="noopener" href="https://pyimagesearch.com/opencv-tutorials-resources-guides/">simply follow one of my guides</a> (I recommend OpenCV 3.4.2, so be sure to download the right version while you follow along). My <a target="_blank" rel="noopener" href="https://github.com/jrosebr1/imutils">imutils</a> package can be installed with pip:</p>
<p>$ pip install –upgrade imutils</p>
<p>Next, we process our command line arguments:</p>
<ul>
<li>  <code>--dataset</code> : The path to our input dataset of face images.</li>
<li>  <code>--embeddings</code> : The path to our output embeddings file. Our script will compute face embeddings which we’ll serialize to disk.</li>
<li>  <code>--detector</code> : Path to OpenCV’s Caffe-based deep learning face detector used to actually <em>localize</em> the faces in the images.</li>
<li>  <code>--embedding-model</code> : Path to the OpenCV deep learning Torch embedding model. This model will allow us to <em>extract</em> a 128-D facial embedding vector.</li>
<li>  <code>--confidence</code> : Optional threshold for filtering week face detections.</li>
</ul>
<p>Now that we’ve imported our packages and parsed command line arguments, lets load the face detector and embedder from disk:</p>
<h2 id="load-our-serialized-face-detector-from-disk"><a href="#load-our-serialized-face-detector-from-disk" class="headerlink" title="load our serialized face detector from disk"></a>load our serialized face detector from disk</h2><p>print(“[INFO] loading face detector…”)<br>protoPath = os.path.sep.join([args[“detector”], “deploy.prototxt”])<br>modelPath = os.path.sep.join([args[“detector”],<br>    “res10_300x300_ssd_iter_140000.caffemodel”])<br>detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)</p>
<h2 id="load-our-serialized-face-embedding-model-from-disk"><a href="#load-our-serialized-face-embedding-model-from-disk" class="headerlink" title="load our serialized face embedding model from disk"></a>load our serialized face embedding model from disk</h2><p>print(“[INFO] loading face recognizer…”)<br>embedder = cv2.dnn.readNetFromTorch(args[“embedding_model”])</p>
<p>Here we load the face detector and embedder:</p>
<ul>
<li>  <code>detector</code> : Loaded via <strong>Lines 26-29</strong>. We’re using a Caffe based DL face detector to <em>localize</em> faces in an image.</li>
<li>  <code>embedder</code> : Loaded on <strong>Line 33</strong>. This model is Torch-based and is responsible for <em>extracting</em> facial embeddings via deep learning feature extraction.</li>
</ul>
<p>Notice that we’re using the respective <code>cv2.dnn</code> functions to load the two separate models. The <code>dnn</code> module wasn’t made available like this until <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/08/21/deep-learning-with-opencv/">OpenCV 3.3</a>, but I recommend that you are <a target="_blank" rel="noopener" href="https://pyimagesearch.com/opencv-tutorials-resources-guides/">using OpenCV 3.4.2 or higher</a> for this blog post.</p>
<p>Moving forward, let’s grab our image paths and perform initializations:</p>
<h2 id="grab-the-paths-to-the-input-images-in-our-dataset"><a href="#grab-the-paths-to-the-input-images-in-our-dataset" class="headerlink" title="grab the paths to the input images in our dataset"></a>grab the paths to the input images in our dataset</h2><p>print(“[INFO] quantifying faces…”)<br>imagePaths = list(paths.list_images(args[“dataset”]))</p>
<h2 id="initialize-our-lists-of-extracted-facial-embeddings-and"><a href="#initialize-our-lists-of-extracted-facial-embeddings-and" class="headerlink" title="initialize our lists of extracted facial embeddings and"></a>initialize our lists of extracted facial embeddings and</h2><h2 id="corresponding-people-names"><a href="#corresponding-people-names" class="headerlink" title="corresponding people names"></a>corresponding people names</h2><p>knownEmbeddings = []<br>knownNames = []</p>
<h2 id="initialize-the-total-number-of-faces-processed"><a href="#initialize-the-total-number-of-faces-processed" class="headerlink" title="initialize the total number of faces processed"></a>initialize the total number of faces processed</h2><p>total = 0</p>
<p>The <code>imagePaths</code> list, built on <strong>Line 37</strong>, contains the path to each image in the dataset. I’ve made this easy via my <code>imutils</code> function, <code>paths.list_images</code> .</p>
<p>Our embeddings and corresponding names will be held in two lists: <code>knownEmbeddings</code> and <code>knownNames</code> (<strong>Lines 41 and 42</strong>).</p>
<p>We’ll also be keeping track of how many faces we’ve processed via a variable called <code>total</code> (<strong>Line 45</strong>).</p>
<p>Let’s begin looping over the image paths — this loop will be responsible for extracting embeddings from faces found in each image:</p>
<h2 id="loop-over-the-image-paths"><a href="#loop-over-the-image-paths" class="headerlink" title="loop over the image paths"></a>loop over the image paths</h2><p>for (i, imagePath) in enumerate(imagePaths):<br>    ## extract the person name from the image path<br>    print(“[INFO] processing image {}/{}”.format(i + 1,<br>        len(imagePaths)))<br>    name = imagePath.split(os.path.sep)[-2]</p>
<pre><code>## load the image, resize it to have a width of 600 pixels (while
## maintaining the aspect ratio), and then grab the image
## dimensions
image = cv2.imread(imagePath)
image = imutils.resize(image, width=600)
(h, w) = image.shape[:2]</code></pre>
<p>We begin looping over <code>imagePaths</code> on <strong>Line 48</strong>.</p>
<p>First, we extract the <code>name</code> of the person from the path (<strong>Line 52</strong>). To explain how this works, consider the following example in my Python shell:</p>
<p>$ python</p>
<blockquote>
<blockquote>
<blockquote>
<p>from imutils import paths<br>import os<br>imagePaths = list(paths.list_images(“dataset”))<br>imagePath = imagePaths[0]<br>imagePath<br>‘dataset/adrian/00004.jpg’<br>imagePath.split(os.path.sep)<br>[‘dataset’, ‘adrian’, ‘00004.jpg’]<br>imagePath.split(os.path.sep)[-2]<br>‘adrian’</p>
</blockquote>
</blockquote>
</blockquote>
<p>Notice how by using <code>imagePath.split</code> and providing the split character (the OS path separator — “/” on unix and “\” on Windows), the function produces a list of folder/file names (strings) which walk down the directory tree. We grab the second-to-last index, the persons <code>name</code> , which in this case is <code>&#39;adrian&#39;</code> .</p>
<p>Finally, we wrap up the above code block by loading the <code>image</code> and <code>resize</code> it to a known <code>width</code> (<strong>Lines 57 and 58</strong>).</p>
<p>Let’s detect and localize faces:</p>
<pre><code>## construct a blob from the image
imageBlob = cv2.dnn.blobFromImage(
    cv2.resize(image, (300, 300)), 1.0, (300, 300),
    (104.0, 177.0, 123.0), swapRB=False, crop=False)

## apply OpenCV&#39;s deep learning-based face detector to localize
## faces in the input image
detector.setInput(imageBlob)
detections = detector.forward()</code></pre>
<p>On <strong>Lines 62-64</strong>, we construct a blob. To learn more about this process, please read <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/"><em>Deep learning: How OpenCV’s blobFromImage works</em></a>.</p>
<p>From there we detect faces in the image by passing the <code>imageBlob</code> through the <code>detector</code> network (<strong>Lines 68 and 69</strong>).</p>
<p>Let’s process the <code>detections</code> :</p>
<pre><code>## ensure at least one face was found
if len(detections) &gt; 0:
    ## we&#39;re making the assumption that each image has only ONE
    ## face, so find the bounding box with the largest probability
    i = np.argmax(detections[0, 0, :, 2])
    confidence = detections[0, 0, i, 2]

    ## ensure that the detection with the largest probability also
    ## means our minimum probability test (thus helping filter out
    ## weak detections)
    if confidence &gt; args[&quot;confidence&quot;]:
        ## compute the (x, y)-coordinates of the bounding box for
        ## the face
        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
        (startX, startY, endX, endY) = box.astype(&quot;int&quot;)

        ## extract the face ROI and grab the ROI dimensions
        face = image[startY:endY, startX:endX]
        (fH, fW) = face.shape[:2]

        ## ensure the face width and height are sufficiently large
        if fW &lt; 20 or fH &lt; 20:
            continue</code></pre>
<p>The <code>detections</code> list contains probabilities and coordinates to localize faces in an image.</p>
<p>Assuming we have at least one detection, we’ll proceed into the body of the if-statement (<strong>Line 72</strong>).</p>
<p>We make the assumption that there is only <em>one</em> face in the image, so we extract the detection with the highest <code>confidence</code> and check to make sure that the confidence meets the minimum probability threshold used to filter out weak detections (<strong>Lines 75-81</strong>).</p>
<p>Assuming we’ve met that threshold, we extract the <code>face</code> ROI and grab/check dimensions to make sure the <code>face</code> ROI is sufficiently large (<strong>Lines 84-93</strong>).</p>
<p>From there, we’ll take advantage of our <code>embedder</code> CNN and extract the face embeddings:</p>
<pre><code>        ## construct a blob for the face ROI, then pass the blob
        ## through our face embedding model to obtain the 128-d
        ## quantification of the face
        faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,
            (96, 96), (0, 0, 0), swapRB=True, crop=False)
        embedder.setInput(faceBlob)
        vec = embedder.forward()

        ## add the name of the person + corresponding face
        ## embedding to their respective lists
        knownNames.append(name)
        knownEmbeddings.append(vec.flatten())
        total += 1</code></pre>
<p>We construct another blob, this time from the face ROI (not the whole image as we did before) on <strong>Lines 98 and 99</strong>.</p>
<p>Subsequently, we pass the <code>faceBlob</code> through the embedder CNN (<strong>Lines 100 and 101</strong>). This generates a 128-D vector (<code>vec</code> ) which describes the face. We’ll leverage this data to recognize new faces via machine learning.</p>
<p>And then we simply add the <code>name</code> and embedding <code>vec</code> to <code>knownNames</code> and <code>knownEmbeddings</code> , respectively (<strong>Lines 105 and 106</strong>).</p>
<p>We also can’t forget about the variable we set to track the <code>total</code> number of faces either — we go ahead and increment the value on <strong>Line 107</strong>.</p>
<p>We continue this process of looping over images, detecting faces, and extracting face embeddings for <em>each and every image</em> in our dataset.</p>
<p>All that’s left when the loop finishes is to dump the data to disk:</p>
<h2 id="dump-the-facial-embeddings-names-to-disk"><a href="#dump-the-facial-embeddings-names-to-disk" class="headerlink" title="dump the facial embeddings + names to disk"></a>dump the facial embeddings + names to disk</h2><p>print(“[INFO] serializing {} encodings…”.format(total))<br>data = {“embeddings”: knownEmbeddings, “names”: knownNames}<br>f = open(args[“embeddings”], “wb”)<br>f.write(pickle.dumps(data))<br>f.close()</p>
<p>We add the name and embedding data to a dictionary and then serialize the <code>data</code> in a pickle file on <strong>Lines 110-114</strong>.</p>
<p>At this point we’re ready to extract embeddings by running our script.</p>
<p>To follow along with this face recognition tutorial, use the <em><strong>“Downloads”</strong></em> section of the post to download the source code, OpenCV models, and example face recognition dataset.</p>
<p>From there, open up a terminal and execute the following command to compute the face embeddings with OpenCV:</p>
<p>$ python extract_embeddings.py –dataset dataset <br>    –embeddings output/embeddings.pickle <br>    –detector face_detection_model <br>    –embedding-model openface_nn4.small2.v1.t7<br>[INFO] loading face detector…<br>[INFO] loading face recognizer…<br>[INFO] quantifying faces…<br>[INFO] processing image 1/18<br>[INFO] processing image 2/18<br>[INFO] processing image 3/18<br>[INFO] processing image 4/18<br>[INFO] processing image 5/18<br>[INFO] processing image 6/18<br>[INFO] processing image 7/18<br>[INFO] processing image 8/18<br>[INFO] processing image 9/18<br>[INFO] processing image 10/18<br>[INFO] processing image 11/18<br>[INFO] processing image 12/18<br>[INFO] processing image 13/18<br>[INFO] processing image 14/18<br>[INFO] processing image 15/18<br>[INFO] processing image 16/18<br>[INFO] processing image 17/18<br>[INFO] processing image 18/18<br>[INFO] serializing 18 encodings…</p>
<p>Here you can see that we have extracted 18 face embeddings, one for each of the images (6 per class) in our input face dataset.</p>
<h4 id="Step-2-Train-face-recognition-model"><a href="#Step-2-Train-face-recognition-model" class="headerlink" title="Step #2: Train face recognition model"></a>Step #2: Train face recognition model</h4><p>At this point we have extracted 128-d embeddings for each face — <em>but how do we actually recognize a person based on these embeddings?</em> The answer is that we need to train a “standard” machine learning model (such as an SVM, k-NN classifier, Random Forest, etc.) on top of the embeddings.</p>
<p>In my <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">previous face recognition tutorial</a> we discovered how a modified version of k-NN can be used for face recognition on 128-d embeddings created via the <a target="_blank" rel="noopener" href="http://dlib.net/">dlib</a> and <a target="_blank" rel="noopener" href="https://github.com/ageitgey/face_recognition">face_recognition</a> libraries.</p>
<p>Today, I want to share how we can build a more powerful classifier on top of the embeddings — you’ll be able to use this same method in your dlib-based face recognition pipelines as well if you are so inclined.</p>
<p>Open up the <code>train_model.py</code> file and insert the following code:</p>
<h2 id="import-the-necessary-packages-1"><a href="#import-the-necessary-packages-1" class="headerlink" title="import the necessary packages"></a>import the necessary packages</h2><p>from sklearn.preprocessing import LabelEncoder<br>from sklearn.svm import SVC<br>import argparse<br>import pickle</p>
<h2 id="construct-the-argument-parser-and-parse-the-arguments-1"><a href="#construct-the-argument-parser-and-parse-the-arguments-1" class="headerlink" title="construct the argument parser and parse the arguments"></a>construct the argument parser and parse the arguments</h2><p>ap = argparse.ArgumentParser()<br>ap.add_argument(“-e”, “–embeddings”, required=True,<br>    help=”path to serialized db of facial embeddings”)<br>ap.add_argument(“-r”, “–recognizer”, required=True,<br>    help=”path to output model trained to recognize faces”)<br>ap.add_argument(“-l”, “–le”, required=True,<br>    help=”path to output label encoder”)<br>args = vars(ap.parse_args())</p>
<p>We’ll need <a target="_blank" rel="noopener" href="http://scikit-learn.org/stable/">scikit-learn</a>, a machine learning library, installed in our environment prior to running this script. You can install it via pip:</p>
<p>$ pip install scikit-learn</p>
<p>We import our packages and modules on <strong>Lines 2-5</strong>. We’ll be using scikit-learn’s implementation of Support Vector Machines (SVM), a common machine learning model.</p>
<p>From there we parse our command line arguments:</p>
<ul>
<li>  <code>--embeddings</code> : The path to the serialized embeddings (we exported it by running the previous <code>extract_embeddings.py</code> script).</li>
<li>  <code>--recognizer</code> : This will be our output model that <em>recognizes</em> faces. It is based on SVM. We’ll be saving it so we can use it in the next two recognition scripts.</li>
<li>  <code>--le</code> : Our label encoder output file path. We’ll serialize our label encoder to disk so that we can use it and the recognizer model in our image/video face recognition scripts.</li>
</ul>
<p>Each of these arguments is <em>required</em>.</p>
<p>Let’s load our facial embeddings and encode our labels:</p>
<h2 id="load-the-face-embeddings"><a href="#load-the-face-embeddings" class="headerlink" title="load the face embeddings"></a>load the face embeddings</h2><p>print(“[INFO] loading face embeddings…”)<br>data = pickle.loads(open(args[“embeddings”], “rb”).read())</p>
<h2 id="encode-the-labels"><a href="#encode-the-labels" class="headerlink" title="encode the labels"></a>encode the labels</h2><p>print(“[INFO] encoding labels…”)<br>le = LabelEncoder()<br>labels = le.fit_transform(data[“names”])</p>
<p>Here we load our embeddings from <strong>Step #1</strong> on <strong>Line 19</strong>. We won’t be generating any embeddings in this model training script — we’ll use the embeddings previously generated and serialized.</p>
<p>Then we initialize our scikit-learn <code>LabelEncoder</code> and encode our name <code>labels</code> (<strong>Lines 23 and 24</strong>).</p>
<p>Now it’s time to train our SVM model for recognizing faces:</p>
<h2 id="train-the-model-used-to-accept-the-128-d-embeddings-of-the-face-and"><a href="#train-the-model-used-to-accept-the-128-d-embeddings-of-the-face-and" class="headerlink" title="train the model used to accept the 128-d embeddings of the face and"></a>train the model used to accept the 128-d embeddings of the face and</h2><h2 id="then-produce-the-actual-face-recognition"><a href="#then-produce-the-actual-face-recognition" class="headerlink" title="then produce the actual face recognition"></a>then produce the actual face recognition</h2><p>print(“[INFO] training model…”)<br>recognizer = SVC(C=1.0, kernel=”linear”, probability=True)<br>recognizer.fit(data[“embeddings”], labels)</p>
<p>On <strong>Line 29</strong> we initialize our SVM model, and on <strong>Line 30</strong> we <code>fit</code> the model (also known as “training the model”).</p>
<p>Here we are using a Linear Support Vector Machine (SVM) but you can try experimenting with other machine learning models if you so wish.</p>
<p>After training the model we output the model and label encoder to disk as pickle files.</p>
<h2 id="write-the-actual-face-recognition-model-to-disk"><a href="#write-the-actual-face-recognition-model-to-disk" class="headerlink" title="write the actual face recognition model to disk"></a>write the actual face recognition model to disk</h2><p>f = open(args[“recognizer”], “wb”)<br>f.write(pickle.dumps(recognizer))<br>f.close()</p>
<h2 id="write-the-label-encoder-to-disk"><a href="#write-the-label-encoder-to-disk" class="headerlink" title="write the label encoder to disk"></a>write the label encoder to disk</h2><p>f = open(args[“le”], “wb”)<br>f.write(pickle.dumps(le))<br>f.close()</p>
<p>We write two pickle files to disk in this block — the <em>face recognizer model</em> and the <em>label encoder</em>.</p>
<p>At this point, be sure you executed the code from <strong>Step #1</strong> first. You can grab the zip containing the code and data from the <em><strong>“Downloads”</strong></em> section.</p>
<p>Now that we have finished coding <code>train_model.py</code> as well, let’s apply it to our extracted face embeddings:</p>
<p>$ python train_model.py –embeddings output/embeddings.pickle <br>    –recognizer output/recognizer.pickle <br>    –le output/le.pickle<br>[INFO] loading face embeddings…<br>[INFO] encoding labels…<br>[INFO] training model…<br>$ ls output/<br>embeddings.pickle    le.pickle        recognizer.pickle</p>
<p>Here you can see that our SVM has been trained on the embeddings and both the (1) SVM itself and (2) the label encoding have been written to disk, enabling us to apply them to input images and video.</p>
<h4 id="Step-3-Recognize-faces-with-OpenCV"><a href="#Step-3-Recognize-faces-with-OpenCV" class="headerlink" title="Step #3: Recognize faces with OpenCV"></a>Step #3: Recognize faces with OpenCV</h4><p>We are now ready to perform face recognition with OpenCV!</p>
<p>We’ll start with recognizing faces in images in this section and then move on to recognizing faces in video streams in the following section.</p>
<p>Open up the <code>recognize.py</code> file in your project and insert the following code:</p>
<h2 id="import-the-necessary-packages-2"><a href="#import-the-necessary-packages-2" class="headerlink" title="import the necessary packages"></a>import the necessary packages</h2><p>import numpy as np<br>import argparse<br>import imutils<br>import pickle<br>import cv2<br>import os</p>
<h2 id="construct-the-argument-parser-and-parse-the-arguments-2"><a href="#construct-the-argument-parser-and-parse-the-arguments-2" class="headerlink" title="construct the argument parser and parse the arguments"></a>construct the argument parser and parse the arguments</h2><p>ap = argparse.ArgumentParser()<br>ap.add_argument(“-i”, “–image”, required=True,<br>    help=”path to input image”)<br>ap.add_argument(“-d”, “–detector”, required=True,<br>    help=”path to OpenCV’s deep learning face detector”)<br>ap.add_argument(“-m”, “–embedding-model”, required=True,<br>    help=”path to OpenCV’s deep learning face embedding model”)<br>ap.add_argument(“-r”, “–recognizer”, required=True,<br>    help=”path to model trained to recognize faces”)<br>ap.add_argument(“-l”, “–le”, required=True,<br>    help=”path to label encoder”)<br>ap.add_argument(“-c”, “–confidence”, type=float, default=0.5,<br>    help=”minimum probability to filter weak detections”)<br>args = vars(ap.parse_args())</p>
<p>We <code>import</code> our required packages on <strong>Lines 2-7</strong>. At this point, you should have each of these packages installed.</p>
<p>Our six command line arguments are parsed on <strong>Lines 10-23</strong>:</p>
<ul>
<li>  <code>--image</code> : The path to the input image. We will attempt to recognize the faces in this image.</li>
<li>  <code>--detector</code> : The path to OpenCV’s deep learning face detector. We’ll use this model to <em>detect</em> where in the image the face ROIs are.</li>
<li>  <code>--embedding-model</code> : The path to OpenCV’s deep learning face embedding model. We’ll use this model to <em>extract</em> the 128-D face embedding from the face ROI — we’ll feed the data into the recognizer.</li>
<li>  <code>--recognizer</code> : The path to our recognizer model. We trained our SVM recognizer in <strong>Step #2</strong>. This is what will actually <em>determine who</em> a face is.</li>
<li>  <code>--le</code> : The path to our label encoder. This contains our face labels such as <code>&#39;adrian&#39;</code> or <code>&#39;trisha&#39;</code> .</li>
<li>  <code>--confidence</code> : The optional threshold to filter weak face <em>detections</em>.</li>
</ul>
<p>Be sure to study these command line arguments — it is important to know the difference between the two deep learning models and the SVM model. If you find yourself confused later in this script, you should refer back to here.</p>
<p>Now that we’ve handled our imports and command line arguments, let’s load the three models from disk into memory:</p>
<h2 id="load-our-serialized-face-detector-from-disk-1"><a href="#load-our-serialized-face-detector-from-disk-1" class="headerlink" title="load our serialized face detector from disk"></a>load our serialized face detector from disk</h2><p>print(“[INFO] loading face detector…”)<br>protoPath = os.path.sep.join([args[“detector”], “deploy.prototxt”])<br>modelPath = os.path.sep.join([args[“detector”],<br>    “res10_300x300_ssd_iter_140000.caffemodel”])<br>detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)</p>
<h2 id="load-our-serialized-face-embedding-model-from-disk-1"><a href="#load-our-serialized-face-embedding-model-from-disk-1" class="headerlink" title="load our serialized face embedding model from disk"></a>load our serialized face embedding model from disk</h2><p>print(“[INFO] loading face recognizer…”)<br>embedder = cv2.dnn.readNetFromTorch(args[“embedding_model”])</p>
<h2 id="load-the-actual-face-recognition-model-along-with-the-label-encoder"><a href="#load-the-actual-face-recognition-model-along-with-the-label-encoder" class="headerlink" title="load the actual face recognition model along with the label encoder"></a>load the actual face recognition model along with the label encoder</h2><p>recognizer = pickle.loads(open(args[“recognizer”], “rb”).read())<br>le = pickle.loads(open(args[“le”], “rb”).read())</p>
<p>We load three models in this block. At the risk of being redundant, I want to explicitly remind you of the differences among the models:</p>
<ol>
<li> <code>detector</code> : A <em>pre-trained</em> Caffe DL model to <em>detect where in the image the faces are</em> (<strong>Lines 27-30</strong>).</li>
<li> <code>embedder</code> : A <em>pre-trained</em> Torch DL model to <em>calculate our 128-D face embeddings</em> (<strong>Line 34</strong>).</li>
<li> <code>recognizer</code> : Our Linear SVM <em>face recognition</em> model (<strong>Line 37</strong>). We trained this model in <strong>Step 2</strong>.</li>
</ol>
<p>Both 1 &amp; 2 are <em>pre-trained</em> meaning that they are provided to you as-is by OpenCV. They are buried in the OpenCV project on GitHub, but I’ve included them for your convenience in the <em><strong>“Downloads”</strong></em> section of today’s post. I’ve also numbered the models in the order that we’ll apply them to recognize faces with OpenCV.</p>
<p>We also load our label encoder which holds the names of the people our model can recognize (<strong>Line 38</strong>).</p>
<p>Now let’s load our image and <em>detect</em> faces:</p>
<h2 id="load-the-image-resize-it-to-have-a-width-of-600-pixels-while"><a href="#load-the-image-resize-it-to-have-a-width-of-600-pixels-while" class="headerlink" title="load the image, resize it to have a width of 600 pixels (while"></a>load the image, resize it to have a width of 600 pixels (while</h2><h2 id="maintaining-the-aspect-ratio-and-then-grab-the-image-dimensions"><a href="#maintaining-the-aspect-ratio-and-then-grab-the-image-dimensions" class="headerlink" title="maintaining the aspect ratio), and then grab the image dimensions"></a>maintaining the aspect ratio), and then grab the image dimensions</h2><p>image = cv2.imread(args[“image”])<br>image = imutils.resize(image, width=600)<br>(h, w) = image.shape[:2]</p>
<h2 id="construct-a-blob-from-the-image"><a href="#construct-a-blob-from-the-image" class="headerlink" title="construct a blob from the image"></a>construct a blob from the image</h2><p>imageBlob = cv2.dnn.blobFromImage(<br>    cv2.resize(image, (300, 300)), 1.0, (300, 300),<br>    (104.0, 177.0, 123.0), swapRB=False, crop=False)</p>
<h2 id="apply-OpenCV’s-deep-learning-based-face-detector-to-localize"><a href="#apply-OpenCV’s-deep-learning-based-face-detector-to-localize" class="headerlink" title="apply OpenCV’s deep learning-based face detector to localize"></a>apply OpenCV’s deep learning-based face detector to localize</h2><h2 id="faces-in-the-input-image"><a href="#faces-in-the-input-image" class="headerlink" title="faces in the input image"></a>faces in the input image</h2><p>detector.setInput(imageBlob)<br>detections = detector.forward()</p>
<p>Here we:</p>
<ul>
<li>  Load the image into memory and construct a blob (<strong>Lines 42-49</strong>). Learn about <code>cv2.dnn.blobFromImage</code> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/">here</a>.</li>
<li>  Localize faces in the image via our <code>detector</code> (<strong>Lines 53 and 54</strong>).</li>
</ul>
<p>Given our new <code>detections</code> , let’s recognize faces in the image. But first we need to filter weak <code>detections</code> and extract the <code>face</code> ROI:</p>
<h2 id="loop-over-the-detections"><a href="#loop-over-the-detections" class="headerlink" title="loop over the detections"></a>loop over the detections</h2><p>for i in range(0, detections.shape[2]):<br>    ## extract the confidence (i.e., probability) associated with the<br>    ## prediction<br>    confidence = detections[0, 0, i, 2]</p>
<pre><code>## filter out weak detections
if confidence &gt; args[&quot;confidence&quot;]:
    ## compute the (x, y)-coordinates of the bounding box for the
    ## face
    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
    (startX, startY, endX, endY) = box.astype(&quot;int&quot;)

    ## extract the face ROI
    face = image[startY:endY, startX:endX]
    (fH, fW) = face.shape[:2]

    ## ensure the face width and height are sufficiently large
    if fW &lt; 20 or fH &lt; 20:
        continue</code></pre>
<p>You’ll recognize this block from <strong>Step #1</strong>. I’ll explain it here once more:</p>
<ul>
<li>  We loop over the <code>detections</code> on <strong>Line 57</strong> and extract the <code>confidence</code> of each on <strong>Line 60</strong>.</li>
<li>  Then we compare the <code>confidence</code> to the minimum probability detection threshold contained in our command line <code>args</code> dictionary, ensuring that the computed probability is larger than the minimum probability (<strong>Line 63</strong>).</li>
<li>  From there, we extract the <code>face</code> ROI (<strong>Lines 66-70</strong>) as well as ensure it’s spatial dimensions are sufficiently large (<strong>Lines 74 and 75</strong>).</li>
</ul>
<p>Recognizing the name of the <code>face</code> ROI requires just a few steps:</p>
<pre><code>    ## construct a blob for the face ROI, then pass the blob
    ## through our face embedding model to obtain the 128-d
    ## quantification of the face
    faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96),
        (0, 0, 0), swapRB=True, crop=False)
    embedder.setInput(faceBlob)
    vec = embedder.forward()

    ## perform classification to recognize the face
    preds = recognizer.predict_proba(vec)[0]
    j = np.argmax(preds)
    proba = preds[j]
    name = le.classes_[j]</code></pre>
<p>First, we construct a <code>faceBlob</code> (from the <code>face</code> ROI) and pass it through the <code>embedder</code> to generate a 128-D vector which describes the face (<strong>Lines 80-83</strong>)</p>
<p>Then, we pass the <code>vec</code> through our SVM recognizer model (<strong>Line 86</strong>), the result of which is our predictions for <em>who</em> is in the face ROI.</p>
<p>We take the highest probability index (<strong>Line 87</strong>) and query our label encoder to find the <code>name</code> (<strong>Line 89</strong>). In between, I extract the probability on <strong>Line 88</strong>.</p>
<p><em><strong>Note:</strong> You cam further filter out weak face recognitions by applying an additional threshold test on the probability. For example, inserting <code>if proba &lt; T</code> (where <code>T</code> is a variable you define) can provide an additional layer of filtering to ensure there are less false-positive face recognitions.</em></p>
<p>Now, let’s display OpenCV face recognition results:</p>
<pre><code>    ## draw the bounding box of the face along with the associated
    ## probability
    text = &quot;&#123;&#125;: &#123;:.2f&#125;%&quot;.format(name, proba * 100)
    y = startY - 10 if startY - 10 &gt; 10 else startY + 10
    cv2.rectangle(image, (startX, startY), (endX, endY),
        (0, 0, 255), 2)
    cv2.putText(image, text, (startX, y),
        cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)</code></pre>
<h2 id="show-the-output-image"><a href="#show-the-output-image" class="headerlink" title="show the output image"></a>show the output image</h2><p>cv2.imshow(“Image”, image)<br>cv2.waitKey(0)</p>
<p>For every face we recognize in the loop (including the “unknown”) people:</p>
<ul>
<li>  We construct a <code>text</code> string containing the <code>name</code> and probability on <strong>Line 93</strong>.</li>
<li>  And then we draw a rectangle around the face and place the text above the box (<strong>Lines 94-98</strong>).</li>
</ul>
<p>And then finally we visualize the results on the screen until a key is pressed (<strong>Lines 101 and 102</strong>).</p>
<p>It is time to recognize faces in images with OpenCV!</p>
<p>To apply our OpenCV face recognition pipeline to my provided images (or your own dataset + test images), make sure you use the <em><strong>“Downloads”</strong></em> section of the blog post to download the code, trained models, and example images.</p>
<p>From there, open up a terminal and execute the following command:</p>
<p>$ python recognize.py –detector face_detection_model <br>    –embedding-model openface_nn4.small2.v1.t7 <br>    –recognizer output/recognizer.pickle <br>    –le output/le.pickle <br>    –image images/adrian.jpg<br>[INFO] loading face detector…<br>[INFO] loading face recognizer…</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_result01.jpg"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_result01.jpg?size=500x663&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 4:</strong> OpenCV face recognition has recognized <em>me</em> at the <a target="_blank" rel="noopener" href="https://www.imdb.com/title/tt4881806/"><em>Jurassic World: Fallen Kingdom</em></a> movie showing.</p>
<p>Here you can see me sipping on a beer and sporting one of my favorite <em>Jurassic Park</em> shirts, along with a special <em>Jurassic World</em> pint glass and commemorative book. My face prediction only has 47.15% confidence; however, that confidence is higher than the <em>“Unknown”</em> class.</p>
<p>Let’s try another OpenCV face recognition example:</p>
<p>$ python recognize.py –detector face_detection_model <br>    –embedding-model openface_nn4.small2.v1.t7 <br>    –recognizer output/recognizer.pickle <br>    –le output/le.pickle <br>    –image images/trisha_adrian.jpg<br>[INFO] loading face detector…<br>[INFO] loading face recognizer…</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_result02.jpg"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_result02.jpg?size=500x663&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 5:</strong> My wife, Trisha, and I are recognized in a selfie picture on an airplane with OpenCV + deep learning facial recognition.</p>
<p>Here are Trisha and I, ready to start our vacation!</p>
<p>In a final example, let’s look at what happens when our model is unable to recognize the actual face:</p>
<p>$ python recognize.py –detector face_detection_model <br>    –embedding-model openface_nn4.small2.v1.t7 <br>    –recognizer output/recognizer.pickle <br>    –le output/le.pickle <br>    –image images/patrick_bateman.jpg<br>[INFO] loading face detector…<br>[INFO] loading face recognizer…</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_result03.jpg"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_result03.jpg?size=500x516&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 6:</strong> Facial recognition with OpenCV has determined that this person is “unknown”.</p>
<p>The third image is an example of an “unknown” person who is actually Patrick Bateman from <a target="_blank" rel="noopener" href="https://www.imdb.com/title/tt0144084/"><em>American Psycho</em></a> — believe me, this is not a person you would want to see show up in your images or video streams!</p>
<h4 id="BONUS-Recognize-faces-in-video-streams"><a href="#BONUS-Recognize-faces-in-video-streams" class="headerlink" title="BONUS: Recognize faces in video streams"></a>BONUS: Recognize faces in video streams</h4><p>As a bonus, I decided to include a section dedicated to OpenCV face recognition in video streams!</p>
<p>The actual pipeline itself is near identical to recognizing faces in images, with only a few updates which we’ll review along the way.</p>
<p>Open up the <code>recognize_video.py</code> file and let’s get started:</p>
<h2 id="import-the-necessary-packages-3"><a href="#import-the-necessary-packages-3" class="headerlink" title="import the necessary packages"></a>import the necessary packages</h2><p>from imutils.video import VideoStream<br>from imutils.video import FPS<br>import numpy as np<br>import argparse<br>import imutils<br>import pickle<br>import time<br>import cv2<br>import os</p>
<h2 id="construct-the-argument-parser-and-parse-the-arguments-3"><a href="#construct-the-argument-parser-and-parse-the-arguments-3" class="headerlink" title="construct the argument parser and parse the arguments"></a>construct the argument parser and parse the arguments</h2><p>ap = argparse.ArgumentParser()<br>ap.add_argument(“-d”, “–detector”, required=True,<br>    help=”path to OpenCV’s deep learning face detector”)<br>ap.add_argument(“-m”, “–embedding-model”, required=True,<br>    help=”path to OpenCV’s deep learning face embedding model”)<br>ap.add_argument(“-r”, “–recognizer”, required=True,<br>    help=”path to model trained to recognize faces”)<br>ap.add_argument(“-l”, “–le”, required=True,<br>    help=”path to label encoder”)<br>ap.add_argument(“-c”, “–confidence”, type=float, default=0.5,<br>    help=”minimum probability to filter weak detections”)<br>args = vars(ap.parse_args())</p>
<p>Our imports are the same as the <strong>Step #3</strong> section above, except for <strong>Lines 2 and 3</strong> where we use the <code>imutils.video</code> module. We’ll use <code>VideoStream</code> to capture frames from our camera and <code>FPS</code> to calculate frames per second statistics.</p>
<p>The command line arguments are also the same except we aren’t passing a path to a static image via the command line. Rather, we’ll grab a reference to our webcam and then process the video. Refer to <strong>Step #3</strong> if you need to review the arguments.</p>
<p>Our three models and label encoder are loaded here:</p>
<h2 id="load-our-serialized-face-detector-from-disk-2"><a href="#load-our-serialized-face-detector-from-disk-2" class="headerlink" title="load our serialized face detector from disk"></a>load our serialized face detector from disk</h2><p>print(“[INFO] loading face detector…”)<br>protoPath = os.path.sep.join([args[“detector”], “deploy.prototxt”])<br>modelPath = os.path.sep.join([args[“detector”],<br>    “res10_300x300_ssd_iter_140000.caffemodel”])<br>detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)</p>
<h2 id="load-our-serialized-face-embedding-model-from-disk-2"><a href="#load-our-serialized-face-embedding-model-from-disk-2" class="headerlink" title="load our serialized face embedding model from disk"></a>load our serialized face embedding model from disk</h2><p>print(“[INFO] loading face recognizer…”)<br>embedder = cv2.dnn.readNetFromTorch(args[“embedding_model”])</p>
<h2 id="load-the-actual-face-recognition-model-along-with-the-label-encoder-1"><a href="#load-the-actual-face-recognition-model-along-with-the-label-encoder-1" class="headerlink" title="load the actual face recognition model along with the label encoder"></a>load the actual face recognition model along with the label encoder</h2><p>recognizer = pickle.loads(open(args[“recognizer”], “rb”).read())<br>le = pickle.loads(open(args[“le”], “rb”).read())</p>
<p>Here we load face <code>detector</code> , face <code>embedder</code> model, face <code>recognizer</code> model (Linear SVM), and label encoder.</p>
<p>Again, be sure to refer to <strong>Step #3</strong> if you are confused about the three models or label encoder.</p>
<p>Let’s initialize our video stream and begin processing frames:</p>
<h2 id="initialize-the-video-stream-then-allow-the-camera-sensor-to-warm-up"><a href="#initialize-the-video-stream-then-allow-the-camera-sensor-to-warm-up" class="headerlink" title="initialize the video stream, then allow the camera sensor to warm up"></a>initialize the video stream, then allow the camera sensor to warm up</h2><p>print(“[INFO] starting video stream…”)<br>vs = VideoStream(src=0).start()<br>time.sleep(2.0)</p>
<h2 id="start-the-FPS-throughput-estimator"><a href="#start-the-FPS-throughput-estimator" class="headerlink" title="start the FPS throughput estimator"></a>start the FPS throughput estimator</h2><p>fps = FPS().start()</p>
<h2 id="loop-over-frames-from-the-video-file-stream"><a href="#loop-over-frames-from-the-video-file-stream" class="headerlink" title="loop over frames from the video file stream"></a>loop over frames from the video file stream</h2><p>while True:<br>    ## grab the frame from the threaded video stream<br>    frame = vs.read()</p>
<pre><code>## resize the frame to have a width of 600 pixels (while
## maintaining the aspect ratio), and then grab the image
## dimensions
frame = imutils.resize(frame, width=600)
(h, w) = frame.shape[:2]

## construct a blob from the image
imageBlob = cv2.dnn.blobFromImage(
    cv2.resize(frame, (300, 300)), 1.0, (300, 300),
    (104.0, 177.0, 123.0), swapRB=False, crop=False)

## apply OpenCV&#39;s deep learning-based face detector to localize
## faces in the input image
detector.setInput(imageBlob)
detections = detector.forward()</code></pre>
<p>Our <code>VideoStream</code> object is initialized and started on <strong>Line 43</strong>. We wait for the camera sensor to warm up on <strong>Line 44</strong>.</p>
<p>We also initialize our frames per second counter (<strong>Line 47</strong>) and begin looping over frames on <strong>Line 50</strong>. We grab a <code>frame</code> from the webcam on <strong>Line 52</strong>.</p>
<p>From here everything is the same as <strong>Step 3</strong>. We <code>resize</code> the frame (<strong>L**</strong>ine 57**) and then we construct a blob from the frame + detect where the faces are (<strong>Lines 61-68</strong>).</p>
<p>Now let’s process the detections:</p>
<pre><code>## loop over the detections
for i in range(0, detections.shape[2]):
    ## extract the confidence (i.e., probability) associated with
    ## the prediction
    confidence = detections[0, 0, i, 2]

    ## filter out weak detections
    if confidence &gt; args[&quot;confidence&quot;]:
        ## compute the (x, y)-coordinates of the bounding box for
        ## the face
        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
        (startX, startY, endX, endY) = box.astype(&quot;int&quot;)

        ## extract the face ROI
        face = frame[startY:endY, startX:endX]
        (fH, fW) = face.shape[:2]

        ## ensure the face width and height are sufficiently large
        if fW &lt; 20 or fH &lt; 20:
            continue</code></pre>
<p>Just as in the previous section, we begin looping over <code>detections</code> and filter out weak ones (<strong>Lines 71-77</strong>). Then we extract the <code>face</code> ROI as well as ensure the spatial dimensions are sufficiently large enough for the next steps (<strong>Lines 84-89</strong>).</p>
<p>Now it’s time to perform OpenCV face recognition:</p>
<pre><code>        ## construct a blob for the face ROI, then pass the blob
        ## through our face embedding model to obtain the 128-d
        ## quantification of the face
        faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,
            (96, 96), (0, 0, 0), swapRB=True, crop=False)
        embedder.setInput(faceBlob)
        vec = embedder.forward()

        ## perform classification to recognize the face
        preds = recognizer.predict_proba(vec)[0]
        j = np.argmax(preds)
        proba = preds[j]
        name = le.classes_[j]

        ## draw the bounding box of the face along with the
        ## associated probability
        text = &quot;&#123;&#125;: &#123;:.2f&#125;%&quot;.format(name, proba * 100)
        y = startY - 10 if startY - 10 &gt; 10 else startY + 10
        cv2.rectangle(frame, (startX, startY), (endX, endY),
            (0, 0, 255), 2)
        cv2.putText(frame, text, (startX, y),
            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)

## update the FPS counter
fps.update()</code></pre>
<p>Here we:</p>
<ul>
<li>  Construct the <code>faceBlob</code> (<strong>Lines 94 and 95</strong>) and calculate the facial embeddings via deep learning (<strong>Lines 96 and 97</strong>).</li>
<li>  Recognize the most-likely <code>name</code> of the face while calculating the probability (<strong>Line 100-103</strong>).</li>
<li>  Draw a bounding box around the face and the person’s <code>name</code> + probability (<strong>Lines 107 -112</strong>).</li>
</ul>
<p>Our <code>fps</code> counter is updated on <strong>Line 115</strong>.</p>
<p>Let’s display the results and clean up:</p>
<pre><code>## show the output frame
cv2.imshow(&quot;Frame&quot;, frame)
key = cv2.waitKey(1) &amp; 0xFF

## if the `q` key was pressed, break from the loop
if key == ord(&quot;q&quot;):
    break</code></pre>
<h2 id="stop-the-timer-and-display-FPS-information"><a href="#stop-the-timer-and-display-FPS-information" class="headerlink" title="stop the timer and display FPS information"></a>stop the timer and display FPS information</h2><p>fps.stop()<br>print(“[INFO] elasped time: {:.2f}”.format(fps.elapsed()))<br>print(“[INFO] approx. FPS: {:.2f}”.format(fps.fps()))</p>
<h2 id="do-a-bit-of-cleanup"><a href="#do-a-bit-of-cleanup" class="headerlink" title="do a bit of cleanup"></a>do a bit of cleanup</h2><p>cv2.destroyAllWindows()<br>vs.stop()</p>
<p>To close out the script, we:</p>
<ul>
<li>  Display the annotated <code>frame</code> (<strong>Line 118</strong>) and wait for the “q” key to be pressed at which point we break out of the loop (<strong>Lines 119-123</strong>).</li>
<li>  Stop our <code>fps</code> counter and print statistics in the terminal (<strong>Lines 126-128</strong>).</li>
<li>  Cleanup by closing windows and releasing pointers (<strong>Lines 131 and 132</strong>).</li>
</ul>
<p>To execute our OpenCV face recognition pipeline on a video stream, open up a terminal and execute the following command:</p>
<p>$ python recognize_video.py –detector face_detection_model <br>    –embedding-model openface_nn4.small2.v1.t7 <br>    –recognizer output/recognizer.pickle <br>    –le output/le.pickle<br>[INFO] loading face detector…<br>[INFO] loading face recognizer…<br>[INFO] starting video stream…<br>[INFO] elasped time: 12.52<br>[INFO] approx. FPS: 16.13</p>
<p><a target="_blank" rel="noopener" href="https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/opencv-face-recognition/opencv_face_reco_animation.gif"><img src="https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/opencv-face-recognition/opencv_face_reco_animation.gif"></a></p>
<p><strong>Figure 7:</strong> Face recognition in video with OpenCV.</p>
<p>As you can see, both Trisha and my face are correctly identified! Our OpenCV face recognition pipeline is also obtaining ~16 FPS on my iMac. On my MacBook Pro I was getting ~14 FPS throughput rate.</p>
<h4 id="Drawbacks-limitations-and-how-to-obtain-higher-face-recognition-accuracy"><a href="#Drawbacks-limitations-and-how-to-obtain-higher-face-recognition-accuracy" class="headerlink" title="Drawbacks, limitations, and how to obtain higher face recognition accuracy"></a>Drawbacks, limitations, and how to obtain higher face recognition accuracy</h4><p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_misclassification.jpg"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_misclassification.jpg?size=500x663&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 8:</strong> All face recognition systems are error-prone. There will never be a 100% accurate face recognition system.</p>
<p>Inevitably, you’ll run into a situation where OpenCV does not recognize a face correctly.</p>
<p>What do you do in those situations?</p>
<p>And how do you improve your OpenCV face recognition accuracy? In this section, I’ll detail a few of the suggested methods to increase the accuracy of your face recognition pipeline</p>
<h5 id="You-may-need-more-data"><a href="#You-may-need-more-data" class="headerlink" title="You may need more data"></a>You may need more data</h5><p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_more_data.jpg"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_more_data.jpg?size=450x338&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 9:</strong> Most people aren’t training their OpenCV face recognition models with enough data. (<a target="_blank" rel="noopener" href="http://chenlab.ece.cornell.edu/projects/KinshipVerification/">image source</a>)</p>
<p>My first suggestion is likely the most obvious one, but it’s worth sharing.</p>
<p>In my <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">previous tutorial on face recognition</a>, a handful of PyImageSearch readers asked why their face recognition accuracy was low and faces were being misclassified — the conversation went something like this (paraphrased):</p>
<blockquote>
<p><strong>Them:</strong> Hey Adrian, I am trying to perform face recognition on a dataset of my classmate’s faces, but the accuracy is really low. What can I do to increase face recognition accuracy?</p>
<p><strong>Me:</strong> How many face images do you have per person?</p>
<p><strong>Them:</strong> Only one or two.</p>
<p><strong>Me:</strong> Gather more data.</p>
</blockquote>
<p>I get the impression that most readers already know they need more face images when they only have one or two example faces per person, but I suspect they are hoping for me to pull a computer vision technique out of my bag of tips and tricks to solve the problem.</p>
<p>It doesn’t work like that.</p>
<p>If you find yourself with low face recognition accuracy and only have a few example faces per person, gather more data — there are no “computer vision tricks” that will save you from the data gathering process.</p>
<p><strong>Invest in your data and you’ll have a better OpenCV face recognition pipeline.</strong> In general, I would recommend a <strong>minimum of 10-20 faces per person.</strong></p>
<p><em><strong>Note:</strong> You may be thinking, “But Adrian, you only gathered 6 images per person in today’s post!” Yes, you are right — and I did that to prove a point. The OpenCV face recognition system we discussed here today worked but can always be improved. There are times when smaller datasets will give you your desired results, and there’s nothing wrong with trying a small dataset — but when you don’t achieve your desired accuracy you’ll want to gather more data.</em></p>
<h5 id="Perform-face-alignment"><a href="#Perform-face-alignment" class="headerlink" title="Perform face alignment"></a>Perform face alignment</h5><p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_alignment.jpg"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_alignment.jpg?size=500x388&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 9:</strong> Performing face alignment for OpenCV facial recognition can dramatically improve face recognition performance.</p>
<p>The face recognition model OpenCV uses to compute the 128-d face embeddings comes from the <a target="_blank" rel="noopener" href="https://cmusatyalab.github.io/openface/">OpenFace project</a>.</p>
<p>The OpenFace model will perform better on faces that have been aligned.</p>
<p>Face alignment is the process of:</p>
<ol>
<li> Identifying the geometric structure of faces in images.</li>
<li> Attempting to obtain a canonical alignment of the face based on translation, rotation, and scale.</li>
</ol>
<p>As you can see from <strong>Figure 9</strong> at the top of this section, I have:</p>
<ol>
<li> Detected a faces in the image and extracted the ROIs (based on the bounding box coordinates).</li>
<li> Applied <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/">facial landmark detection</a> to extract the coordinates of the eyes.</li>
<li> Computed the centroid for each respective eye along with the midpoint between the eyes.</li>
<li> And based on these points, applied an affine transform to resize the face to a fixed size and dimension.</li>
</ol>
<p>If we apply face alignment to every face in our dataset, then in the output coordinate space, all faces should:</p>
<ol>
<li> Be centered in the image.</li>
<li> Be rotated such the eyes lie on a horizontal line (i.e., the face is rotated such that the eyes lie along the same <em>y</em>-coordinates).</li>
<li> Be scaled such that the size of the faces is approximately identical.</li>
</ol>
<p>Applying face alignment to our OpenCV face recognition pipeline was outside the scope of today’s tutorial, but if you would like to further increase your face recognition accuracy using OpenCV and OpenFace, I would recommend you apply face alignment.</p>
<p>Check out my blog post, <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/"><em>Face Alignment with OpenCV and Python</em></a>.</p>
<h5 id="Tune-your-hyperparameters"><a href="#Tune-your-hyperparameters" class="headerlink" title="Tune your hyperparameters"></a>Tune your hyperparameters</h5><p>My second suggestion is for you to attempt to tune your hyperparameters on whatever machine learning model you are using (i.e., the model trained on top of the extracted face embeddings).</p>
<p>For this tutorial, we used a Linear SVM; however, we did not tune the <code>C</code> value, which is typically the most important value of an SVM to tune.</p>
<p>The <code>C</code> value is a “strictness” parameter and controls how much you want to avoid misclassifying each data point in the training set.</p>
<p>Larger values of <code>C</code> will be more strict and try harder to classify every input data point correctly, even at the risk of overfitting.</p>
<p>Smaller values of <code>C</code> will be more “soft”, allowing some misclassifications in the training data, but ideally generalizing better to testing data.</p>
<p>It’s interesting to note that according to one of the classification examples in the <a target="_blank" rel="noopener" href="https://github.com/cmusatyalab/openface">OpenFace GitHub</a>, they actually recommend to <em>not</em> tune the hyperparameters, as, from their experience, they found that setting <code>C=1</code> obtains satisfactory face recognition results in most settings.</p>
<p>Still, if your face recognition accuracy is not sufficient, it may be worth the extra effort and computational cost of tuning your hyperparameters via either a grid search or random search.</p>
<h5 id="Use-dlib’s-embedding-model-but-not-it’s-k-NN-for-face-recognition"><a href="#Use-dlib’s-embedding-model-but-not-it’s-k-NN-for-face-recognition" class="headerlink" title="Use dlib’s embedding model (but not it’s k-NN for face recognition)"></a>Use dlib’s embedding model (but not it’s k-NN for face recognition)</h5><p>In my experience using both OpenCV’s face recognition model along with <a target="_blank" rel="noopener" href="http://blog.dlib.net/2017/02/high-quality-face-recognition-with-deep.html">dlib’s face recognition model</a>, I’ve found that dlib’s face embeddings are more discriminative, especially for smaller datasets.</p>
<p>Furthermore, I’ve found that dlib’s model is less dependent on:</p>
<ol>
<li> Preprocessing such as face alignment</li>
<li> Using a more powerful machine learning model on top of extracted face embeddings</li>
</ol>
<p>If you take a look at <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">my original face recognition tutorial</a>, you’ll notice that we utilized a simple k-NN algorithm for face recognition (with a small modification to throw out nearest neighbor votes whose distance was above a threshold).</p>
<p>The k-NN model worked extremely well, but as we know, more powerful machine learning models exist.</p>
<p>To improve accuracy further, you may want to use <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">dlib’s embedding model</a>, and then instead of applying k-NN, follow <strong>Step #2</strong> from today’s post and train a more powerful classifier on the face embeddings.</p>
<h4 id="Did-you-encounter-a-“USAGE”-error-running-today’s-Python-face-recognition-scripts"><a href="#Did-you-encounter-a-“USAGE”-error-running-today’s-Python-face-recognition-scripts" class="headerlink" title="Did you encounter a “USAGE” error running today’s Python face recognition scripts?"></a>Did you encounter a “USAGE” error running today’s Python face recognition scripts?</h4><p>Each week I receive emails that (paraphrased) go something like this:</p>
<blockquote>
<p>Hi Adrian, I can’t run the code from the blog post.</p>
<p>My error looks like this:</p>
</blockquote>
<p>usage: extract_embeddings.py [-h] -i DATASET -e EMBEDDINGS<br>    -d DETECTOR -m EMBEDDING_MODEL [-c CONFIDENCE]<br>extract_embeddings.py: error: the following arguments are required:<br>    -i/–dataset, -e/–embeddings, -d/–detector, -m/–embedding-model</p>
<p>Or this:</p>
<blockquote>
<p>I’m using Spyder IDE to run the code. It isn’t running as I encounter a “usage” message in the command box.</p>
</blockquote>
<p>There are three separate Python scripts in this tutorial, and furthermore, each of them requires that you (correctly) supply the respective command line arguments.</p>
<p><strong>If you’re new to command line arguments, that’s fine, <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/03/12/python-argparse-command-line-arguments/">but you need to read up on how Python, argparse, and command line arguments work</a> <em>before</em> you try to run these scripts!</strong></p>
<p>I’ll be honest with you — face recognition is an <em>advanced</em> technique. Command line arguments are a <em>very beginner/novice</em> concept. Make sure you walk before you run, otherwise you will trip up. <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/03/12/python-argparse-command-line-arguments/">Take the time now to educate yourself on how command line arguments.</a></p>
<p>Secondly, I always include the <em>exact</em> command you can copy and paste into your terminal or command line and run the script. You might want to modify the command line arguments to accommodate your own image or video data, <em>but essentially I’ve done the work for you</em>. With a knowledge of command line arguments you can update the arguments to <em>point to your own data</em>, <em>without having to modify a single line of code.</em></p>
<p>For the readers that want to use an IDE like Spyder or PyCharm my recommendation is that you learn how to use command line arguments in the command line/terminal <em><strong>first</strong></em>. Program in the IDE, but use the command line to execute your scripts.</p>
<p>I also recommend that you don’t bother trying to configure your IDE for command line arguments until you understand how they work by typing them in first. In fact, <em>you’ll probably learn to love the command line as it is faster than clicking through a GUI menu</em> to input the arguments each time you want to change them. Once you have a good handle on how command line arguments work, you can then configure them separately in your IDE.</p>
<p>From a quick search through my inbox, I see that I’ve answered over 500-1,000 of command line argument-related questions. I’d estimate that I’d answered another 1,000+ such questions replying to comments on the blog.</p>
<p><em>Don’t let me discourage you from commenting on a post or emailing me for assistance — please do.</em> <strong>But if you are new to programming, I urge you to read and try the concepts discussed in my <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/03/12/python-argparse-command-line-arguments/">command line arguments blog post</a></strong> as that will be the tutorial I’ll link you to if you need help.</p>
<h4 id="Alternative-OpenCV-face-recognition-methods"><a href="#Alternative-OpenCV-face-recognition-methods" class="headerlink" title="Alternative OpenCV face recognition methods"></a><strong>Alternative OpenCV face recognition methods</strong></h4><p>In this tutorial, you learned how to perform face recognition using OpenCV and a pre-trained FaceNet model.</p>
<p>Unlike our <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">previous tutorial on deep learning-based face recognition</a>, which utilized two other libraries/packages (dlib and face_recognition), the method covered here today utilizes <em>just</em> OpenCV, therefore removing other dependencies.</p>
<p>However, it’s worth noting that there are other methods that you can utilize when creating your own face recognition systems.</p>
<p><strong>I suggest starting with siamese networks.</strong> Siamese networks are specialized deep learning models that:</p>
<ul>
<li>  Can be successfully trained with very little data</li>
<li>  Learn a similarity score between two images (i.e., how similar two faces are)</li>
<li>  Are the cornerstone of modern face recognition systems</li>
</ul>
<p>I have an entire series of tutorials on siamese networks that I suggest you read to become familiar with them:</p>
<ol>
<li> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2020/11/23/building-image-pairs-for-siamese-networks-with-python/"><em>Building image pairs for siamese networks with Python</em></a></li>
<li> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2020/11/30/siamese-networks-with-keras-tensorflow-and-deep-learning/"><em>Siamese networks with Keras, TensorFlow, and Deep Learning</em></a></li>
<li> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2020/12/07/comparing-images-for-similarity-using-siamese-networks-keras-and-tensorflow/"><em>Comparing images for similarity using siamese networks, Keras, and TensorFlow</em></a></li>
<li> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2021/01/18/contrastive-loss-for-siamese-networks-with-keras-and-tensorflow/"><em>Contrastive Loss for Siamese Networks with Keras and TensorFlow</em></a></li>
</ol>
<p>Additionally, there are non-deep learning-based face recognition methods you may want to consider:</p>
<ul>
<li>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2021/05/03/face-recognition-with-local-binary-patterns-lbps-and-opencv/"><em>Face Recognition with Local Binary Patterns (LBPs) and OpenCV</em></a></li>
<li>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2021/05/10/opencv-eigenfaces-for-face-recognition/"><em>OpenCV Eigenfaces for Face Recognition</em></a></li>
</ul>
<p>These methods are less accurate than their deep learning-based face recognition counterparts, but tend to be much more computationally efficient and will run faster on embedded systems.</p>
<h4 id="What’s-next-I-recommend-PyImageSearch-University"><a href="#What’s-next-I-recommend-PyImageSearch-University" class="headerlink" title="What’s next? I recommend PyImageSearch University."></a>What’s next? I recommend <a target="_blank" rel="noopener" href="https://pyimagesearch.com/pyimagesearch-university/?utm_source=blogPost&utm_medium=bottomBanner&utm_campaign=What%27s%20next?%20I%20recommend">PyImageSearch University</a>.</h4><p><img src="https://fast.wistia.com/embed/medias/kno0cmko2z/swatch"></p>
<p><strong>Course information:</strong><br>30+ total classes • 39h 44m video • Last updated: 12/2021<br>★★★★★ 4.84 (128 Ratings) • 3,000+ Students Enrolled</p>
<p><strong>I strongly believe that if you had the right teacher you could <em>master</em> computer vision and deep learning.</strong></p>
<p>Do you think learning computer vision and deep learning has to be time-consuming, overwhelming, and complicated? Or has to involve complex mathematics and equations? Or requires a degree in computer science?</p>
<p>That’s <em>not</em> the case.</p>
<p>All you need to master computer vision and deep learning is for someone to explain things to you in <em>simple, intuitive</em> terms. <em>And that’s exactly what I do</em>. My mission is to change education and how complex Artificial Intelligence topics are taught.</p>
<p>If you’re serious about learning computer vision, your next stop should be PyImageSearch University, the most comprehensive computer vision, deep learning, and OpenCV course online today. Here you’ll learn how to <em>successfully</em> and <em>confidently</em> apply computer vision to your work, research, and projects. Join me in computer vision mastery.</p>
<p><strong>Inside PyImageSearch University you’ll find:</strong></p>
<ul>
<li>  ✓ <strong>30+ courses</strong> on essential computer vision, deep learning, and OpenCV topics</li>
<li>  ✓ 30+ Certificates of Completion</li>
<li>  ✓ <strong>39h 44m</strong> on-demand video</li>
<li>  ✓ <strong>Brand new courses released <em>every month</em></strong>, ensuring you can keep up with state-of-the-art techniques</li>
<li>  ✓ <strong>Pre-configured Jupyter Notebooks in Google Colab</strong></li>
<li>  ✓ Run all code examples in your web browser — works on Windows, macOS, and Linux (no dev environment configuration required!)</li>
<li>  ✓ Access to <strong>centralized code repos for <em>all</em> 500+ tutorials</strong> on PyImageSearch</li>
<li>  ✓ <strong>Easy one-click downloads</strong> for code, datasets, pre-trained models, etc.</li>
<li>  ✓ Access on mobile, laptop, desktop, etc.</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/pyimagesearch-university/?utm_source=blogPost&utm_medium=bottomBanner&utm_campaign=What%27s%20next?%20I%20recommend">CLICK HERE TO JOIN PYIMAGESEARCH UNIVERSITY</a></p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>In today’s blog post we used OpenCV to perform face recognition.</p>
<p>Our OpenCV face recognition pipeline was created using a four-stage process:</p>
<ol>
<li> Create your dataset of face images</li>
<li> Extract face embeddings for each face in the image (again, using OpenCV)</li>
<li> Train a model on top of the face embeddings</li>
<li> Utilize OpenCV to recognize faces in images and video streams</li>
</ol>
<p>Since I was married over this past weekend, I used photos of myself and Trisha (my now wife) to keep the tutorial fun and festive.</p>
<p>You can, of course, swap in your own face dataset provided you follow the directory structure of the project detailed above.</p>
<p>If you need help gathering your own face dataset, be sure to refer to this post on <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/11/how-to-build-a-custom-face-recognition-dataset/">building a face recognition dataset</a>.</p>
<p>I hope you enjoyed today’s tutorial on OpenCV face recognition!</p>
<p><strong>To download the source code, models, and example dataset for this post (and be notified when future blog posts are published here on PyImageSearch), <em>just enter your email address in the form below!</em></strong></p>
<p><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/cta-source-guide-1.png?lossy=1&strip=1&webp=1"></p>
<h5 id="Download-the-Source-Code-and-FREE-17-page-Resource-Guide"><a href="#Download-the-Source-Code-and-FREE-17-page-Resource-Guide" class="headerlink" title="Download the Source Code and FREE 17-page Resource Guide"></a>Download the Source Code and FREE 17-page Resource Guide</h5><p>Enter your email address below to get a .zip of the code and a <strong>FREE 17-page Resource Guide on Computer Vision, OpenCV, and Deep Learning.</strong> Inside you’ll find my hand-picked tutorials, books, courses, and libraries to help you master CV and DL!</p>
<p>DOWNLOAD THE CODE!</p>
<p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=240&d=mm&r=g"></p>
<h5 id="About-the-Author"><a href="#About-the-Author" class="headerlink" title="About the Author"></a><strong>About the Author</strong></h5><p>Hi there, I’m Adrian Rosebrock, PhD. All too often I see developers, students, and researchers wasting their time, studying the wrong things, and generally struggling to get started with Computer Vision, Deep Learning, and OpenCV. I created this website to show you what I believe is the best possible way to get your start.</p>
<h3 id="Reader-Interactions"><a href="#Reader-Interactions" class="headerlink" title="Reader Interactions"></a>Reader Interactions</h3><p>[</p>
<p>Previous Article:</p>
<h4 id="pip-install-OpenCV"><a href="#pip-install-OpenCV" class="headerlink" title="pip install OpenCV"></a>pip install OpenCV</h4><p>](<a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/19/pip-install-opencv/)[">https://pyimagesearch.com/2018/09/19/pip-install-opencv/)[</a></p>
<p>Next Article:</p>
<h4 id="Install-OpenCV-4-on-your-Raspberry-Pi"><a href="#Install-OpenCV-4-on-your-Raspberry-Pi" class="headerlink" title="Install OpenCV 4 on your Raspberry Pi"></a>Install OpenCV 4 on your Raspberry Pi</h4><p>](<a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/26/install-opencv-4-on-your-raspberry-pi/">https://pyimagesearch.com/2018/09/26/install-opencv-4-on-your-raspberry-pi/</a>)</p>
<h4 id="359-responses-to-OpenCV-Face-Recognition"><a href="#359-responses-to-OpenCV-Face-Recognition" class="headerlink" title="359 responses to: OpenCV Face Recognition"></a>359 responses to: OpenCV Face Recognition</h4><ol>
<li><p><img src="https://secure.gravatar.com/avatar/98d96d0f4b4aa3b1b8b58009396f285f?s=48&d=mm&r=g">Harald Vaessin</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479427">September 24, 2018 at 10:53 am</a></p>
<p> My heartfelt congratulations and best wishes for your future together.<br> And thank you for your wonderful tutorials!<br> HV</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/19ac9a6370cf20163c098cc74d73ecf4?s=48&d=mm&r=g">Jesudas</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479428">September 24, 2018 at 10:56 am</a></p>
<p> Congratulations Adrian on your marriage. Wishing you and Trisha the Very Best in Life !</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/f7883fcf89a17b7716e875f5a48116c2?s=48&d=mm&r=g">Bhavesh kacha</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766619">March 17, 2020 at 5:16 am</a></p>
<p>  Can we live stream that over a network??? If yes, then how ???</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766926">March 19, 2020 at 9:49 am</a></p>
<p>  You can follow <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2019/09/02/opencv-stream-video-to-web-browser-html-page/">this tutorial.</a></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/f0c43c8f73a82effb7435efb53899b36?s=48&d=mm&r=g">Tosho Futami</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479431">September 24, 2018 at 11:09 am</a></p>
<p> I am very appreciated for your weekly new code support. Conglaturation your marriage, please enjoy your forepufule future…</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/b335b3d02b32263040bf7bb83300b4d5?s=48&d=mm&r=g">raj shah</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479564">September 25, 2018 at 3:01 am</a></p>
<p>  hey can u help me to figure out this module (Opencv) ,i m getting an error i know its command line argument can u tell me the configuration parts of ur file.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/3a63fea0fd7f13b5193e941e9c4f72ec?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://ayushpant1998@gmail.com/">Ayush</a></p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479435">September 24, 2018 at 11:22 am</a></p>
<p> Can this be used for detecting and recognising faces in a classroom with many students?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/ce88ea36c4a0d4be49719fc288741c7f?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://twitter.com/drhoffma">David Hoffman</a></p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479461">September 24, 2018 at 2:23 pm</a></p>
<p>  Hi Ayush, potentially it can be used for a classroom. There are several considerations to make:</p>
<ol>
<li> Due to the camera angle, some students’ faces may be obscured if the camera is positioned at the front of the classroom.</li>
<li> Scaling of faces especially for low resolution cameras (depends on camera placement).</li>
<li> Privacy concerns — especially since students/children are involved.</li>
</ol>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/90e8191efee98045d3a6cedc05f629a8?s=48&d=mm&r=g">Huseyn</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498900">February 1, 2019 at 5:42 am</a></p>
<p>  What is the maximum number of people i can trai and this system will work accurately?</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/9b363c50a2c9e981f0068d275f2330fe?s=48&d=mm&r=g">falahgs</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479441">September 24, 2018 at 11:38 am</a></p>
<p> congratulations Adrian ..<br> i like all you are posts in geat blog<br> u really great prof.<br> thanks for this post</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/730b7f99bf79688ef6d6ac224fd33c6a?s=48&d=mm&r=g">Nika</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479445">September 24, 2018 at 12:06 pm</a></p>
<p> Congratulations Adrian and thanks for the tutorial!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/65b6542316cb29ebcc3c1d4c1c153e30?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://none/">mohamed</a></p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479446">September 24, 2018 at 12:17 pm</a></p>
<p> Congratulations Adrian<br> happy Days</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c2fbc71930b000756bd47fd2a295a510?s=48&d=mm&r=g">Jesus Hdz Soberon</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479454">September 24, 2018 at 1:28 pm</a></p>
<p> Congratulations Adrian for you and now for your wife. My best wishes in this new stage of your lives.<br> Best regards from México.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/00947f41f94298b175471fa22a454333?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="https://raypack.ai/">Gary</a></p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479459">September 24, 2018 at 2:04 pm</a></p>
<p> Hello Adrian,<br> I got married in February this year and it feels very good and right 🙂 Nerds like us need great women on our side. Take good care of them and congratulation.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e4d4c96bff48a8f0041eddb3b1e4a07b?s=48&d=mm&r=g">Cyprian</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479460">September 24, 2018 at 2:07 pm</a></p>
<p>Congrats on getting married!<br>Thank you again for this great tutorial on face recognition!<br>Have a nice honeymoon.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/fe01bbd97203f36cdb327306e96b7f81?s=48&d=mm&r=g">Yinon Bloch</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479463">September 24, 2018 at 3:14 pm</a></p>
<p>Congratulations Adrian,<br>I wish you both a happy life together!<br>I read your blog from time to time and enjoy it a lot, I gain a lot of knowledge and ideas from your posts. thank you very much!<br>Regarding your comments about improving the accuracy of the identity, I would like to share with you that I also play a lot with the various libraries of facial identification.<br>I’v tried the code I found in Martin Krasser’s post: <a target="_blank" rel="noopener" href="http://krasserm.github.io/2018/02/07/deep-face-recognition/">http://krasserm.github.io/2018/02/07/deep-face-recognition/</a><br>Which is very similar to what you’ve shown in this post. I would like to know if there are any significant differences between the two.<br>After a lot of poking around and testing I also came to conclusion that the dlib library gives the best results (at least for my needs), but without GPU – we get very slow performance.</p>
<p>I wanted to know if you tried to use the facenet library, which uses a vector of 512D, from my experiments it seems to have the same accuracy as nn4 (more or less), but maybe I’m doing something wrong here.</p>
<p>I would appreciate a response from your experience ,</p>
<p>Great appreciation,<br>Yinon Bloch</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/85b9253a21ac8be53718e2494969eb92?s=48&d=mm&r=g">Horelvis</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479467">September 24, 2018 at 4:23 pm</a></p>
<p>Congratulations Adrian!<br>But now you will don’t have more free time! 😉<br>Enjoy with your wife for all life!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d4f8cc6b1b2f0f24382b7e7bbe6cae4c?s=48&d=mm&r=g">Hossein</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479468">September 24, 2018 at 4:39 pm</a></p>
<p>Congratulations<br>I wish a green life for you.<br>great Thanks</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/aec601bff9ade94745fbc00275df6d43?s=48&d=mm&r=g">Nico</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479469">September 24, 2018 at 4:41 pm</a></p>
<p>Hi Adrian,<br>first of all congrats.</p>
<p>Regarding the code, I tend to agree with Yinon about the fact that the version that uses dlib seems to work better. In particular this version sometimes finds inexistent faces.<br>What is your opinion ?<br>Thanks<br>Nico</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/6689a119b178aaf896631cce3eb6dfa9?s=48&d=mm&r=g">Naser</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479473">September 24, 2018 at 5:33 pm</a></p>
<p>Congratulations Adrian and thank you for good tutorial!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/0ff5404071b72662bf2ac08a1fc0a663?s=48&d=mm&r=g">Hugues</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479477">September 24, 2018 at 6:10 pm</a></p>
<p>Very nice postings, and congratulations on your wedding.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/6e6dbd9e3dbc7353b15a30564b8ff928?s=48&d=mm&r=g">Prateek Xaxa</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479478">September 24, 2018 at 6:50 pm</a></p>
<p>Thanks for the great contents</p>
<p>Wishing Happy Life Together!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/10bca9fead7f94940afca7f0968de452?s=48&d=mm&r=g">Sinh Huynh</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479481">September 24, 2018 at 8:27 pm</a></p>
<p>Wishing you and Trisha all the best in your marriage.<br>Many thanks for your tutorials, they are really great, easy to understand for beginner like me.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/0d4335ebf0a2f3b800ec9f46979e86e2?s=48&d=mm&r=g">kus</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479482">September 24, 2018 at 8:28 pm</a></p>
<p>Congratulations!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/85239fd9da379f3e663b87323b6cc16e?s=48&d=mm&r=g">brett</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479486">September 24, 2018 at 9:23 pm</a></p>
<p>Congratulations, wish you both the best! Thank-you for this post,ill be attempting it in the next few days, great tutorials always worth a read.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/537539638bf494db3bfc9455034bdbd3?s=48&d=mm&r=g">Guanghui Yang</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479487">September 24, 2018 at 9:25 pm</a></p>
<p>Congratulations Adrian！</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/436bc15408c327a1a6c5fb32f773a503?s=48&d=mm&r=g">Tran Tuan Vu</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479494">September 24, 2018 at 10:41 pm</a></p>
<p>Hi Adrian,<br>I have tried on my big dataset (250 persons with ~30 image/person). But when I run recogization scripts, I got very low accuracy? So I think I should not use Linear-SVM for training on the big dataset.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/ce88ea36c4a0d4be49719fc288741c7f?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://twitter.com/drhoffma">David Hoffman</a></p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479654">September 25, 2018 at 11:25 am</a></p>
<p>  Hi Tran, I believe that you need more training data. Thirty images per class isn’t likely enough.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5d28ff93e3eba927761a950a912ffe43?s=48&d=mm&r=g">Keesh</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479501">September 24, 2018 at 11:52 pm</a></p>
<p>Congrats Adrian and Trisha!<br>I hope you have a wonderful Honeymoon and life together.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/831ee93e10e3030b89deab53727173c2?s=48&d=mm&r=g">Emmanuel Girard</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479506">September 25, 2018 at 12:02 am</a></p>
<p>Félicitations. Nous vous souhaitons du bonheur, de la joie, de l’amour et beaucoup de souvenirs. / Congratulations. We wish you happiness, joy, love and many memories.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a8d27b7cb84b4d05c00f1479d802330c?s=48&d=mm&r=g">Namdev</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479513">September 25, 2018 at 12:23 am</a></p>
<p>Many congratulations, Adrian and Trisha</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/300ccf641ac23e91899b047891b106c6?s=48&d=mm&r=g">andreas</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479531">September 25, 2018 at 1:06 am</a></p>
<p>Hi Adrian,<br>Thank you for your tutorial. Could you please point out where non max suppression is solved in this pipeline?<br>Thanks,<br>Andreas</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/15651d7b8e171af94e5bd71bce549cab?s=48&d=mm&r=g">Abhishek Thanki</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479870">September 26, 2018 at 1:23 pm</a></p>
<p>  Hi Andreas,</p>
<p>  There was no non-maxima suppression applied explicitly in the pipeline. Instead, it’s applied by the deep learning based face detector used (which uses a SSD model).</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/2b47ca0cdb231e5b373b064e65d7dd96?s=48&d=mm&r=g">Waheed</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479532">September 25, 2018 at 1:08 am</a></p>
<p>Congratulation Adrian. You deserve it! Thanks for all your posts. I really enjoy them</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/078e1ce95ea0922270c55084008825e8?s=48&d=mm&r=g">Evgeny</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479551">September 25, 2018 at 2:31 am</a></p>
<p>Congratulations Adrian! Thanks for your great post. Wish you a happy life together!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/9fab474cf7b6037cc71c1d55f6fd8710?s=48&d=mm&r=g">Pardis</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479572">September 25, 2018 at 3:31 am</a></p>
<p>Wishing you both a lifetime of love and happiness. And thank you for this great tutorial.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/f3f369c16b2690b20eee3c538876e723?s=48&d=mm&r=g">Chunan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479589">September 25, 2018 at 3:53 am</a></p>
<p>Congratulations! Happy wedding.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e44d6db487716a78d0452bf6d95dda2e?s=48&d=mm&r=g">MD Khan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479604">September 25, 2018 at 5:05 am</a></p>
<p>Congratulations Dr!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/854626a8741852f22600aa0ecf3db113?s=48&d=mm&r=g">siavash</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479629">September 25, 2018 at 6:56 am</a></p>
<p>&lt;3</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/98cca282ebf5cb7f9e06a9a6fcddc6f5?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://www.hash0k.com/">Srinivasan Ramachandran</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479837">September 26, 2018 at 8:00 am</a></p>
<p>Hello Adrian,</p>
<p>Hearty congratulations and best wishes to you and your wife.</p>
<p>Regards,</p>
<p>#0K</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5e6363068241d3a91e4c5adac677f287?s=48&d=mm&r=g">Devkar</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479847">September 26, 2018 at 8:42 am</a></p>
<p>Congratulations….</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/2255d113eedc5ebad187b2c31f7421ab?s=48&d=mm&r=g">Zak Zebrowski</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479871">September 26, 2018 at 1:38 pm</a></p>
<p>Congrats!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/17ea2e2b2e8554ec30f4f869800d9969?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://torun4ever.com/">Murthy Udupa</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479905">September 26, 2018 at 11:08 pm</a></p>
<p>Congratulations Adrian and Trisha. Wish you a wonderful life ahead.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/0b2ff295c34443a97126dbb2f7a9290b?s=48&d=mm&r=g">PFC</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479959">September 27, 2018 at 8:33 am</a></p>
<p>If I want to add a person’s face model, do I just need to add that person’s face data set to the dataset folder?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/ce88ea36c4a0d4be49719fc288741c7f?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://twitter.com/drhoffma">David Hoffman</a></p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479963">September 27, 2018 at 8:58 am</a></p>
<p>  Hi Peng — you’ll need a folder of face pictures for each person in the dataset directory. Then you’ll need to extract embeddings for the dataset and continue with the next steps.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/9a9d21693da7ae487a486721adf881f6?s=48&d=mm&r=g">noura</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-489647">November 28, 2018 at 3:01 pm</a></p>
<p>  how do the extract embedding ?</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/f955dc66abe7cb218dc0a31b44d20b7a?s=48&d=mm&r=g">Arya</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514866">April 26, 2019 at 8:20 am</a></p>
<p>  Tried that. Still shows uknown</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/aa63154f428c44ba2170d6ed3922fb1c?s=48&d=mm&r=g">wayne</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480272">September 30, 2018 at 5:16 am</a></p>
<p>Thanks for your course and congrats!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481269">October 8, 2018 at 12:07 pm</a></p>
<p>  Thanks Wayne, I’m glad you’re enjoying the course 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d539c8d2da5b265526f72a801d994bd8?s=48&d=mm&r=g">Hariprasad</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480399">October 1, 2018 at 2:13 am</a></p>
<p>Happy Married Life</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481252">October 8, 2018 at 10:48 am</a></p>
<p>  Thanks Hariprasad!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c1bc36fed66d402357a9aa23a9aa9c49?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="https://www.caramanual.com/">Cara Manual</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480512">October 2, 2018 at 3:12 am</a></p>
<p>Thank you, this really helped me …</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481238">October 8, 2018 at 10:39 am</a></p>
<p>  Thanks Cara, I’m happy the tutorial has helped you 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c1bc36fed66d402357a9aa23a9aa9c49?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="https://iprint.id/">Jasa Print Kain Jakarta</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480513">October 2, 2018 at 3:13 am</a></p>
<p>Congratulations Adrian and thanks for the tutorial, this is ver usefull…</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481237">October 8, 2018 at 10:38 am</a></p>
<p>  Thank you 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d0de25b5269a380a7808ab999f632509?s=48&d=mm&r=g">Hermy Cruz</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480532">October 2, 2018 at 10:43 am</a></p>
<p>Hi Adrian! First of all Congratulations!!</p>
<p>I have a question, how can I run this at startup if it has command line arguments(crontab).<br>Thank you in advance!!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481230">October 8, 2018 at 10:33 am</a></p>
<p>  I would suggest creating a shell script that calls your Python script. Then call the shell script from the crontab.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c828df8607a337589cb3230037c0e7d3?s=48&d=mm&r=g">Stephen Fischer</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480545">October 2, 2018 at 5:11 pm</a></p>
<p>Congratulations to you and Trisha! Many of your readers got a chance to meet both of you at PyImageConf, and you make a great couple! Here’s to many happy years ahead!</p>
<p>One quick suggestion – I had been receiving an error as follows in the sample code:<br>[INFO] loading face detector…<br>[INFO] loading face recognizer…<br>[INFO] starting video stream…<br>[INFO] elasped time: 8.33<br>[INFO] approx. FPS: 22.09<br>FATAL: exception not rethrown<br>Aborted (core dumped)</p>
<p>I’m wondering if this is related to imutils Bug #86? Anyways, I put a sleep command in and it addressed the “waiting producer/stream issue”:  </p>
<h2 id="do-a-bit-of-cleanup-1"><a href="#do-a-bit-of-cleanup-1" class="headerlink" title="do a bit of cleanup"></a>do a bit of cleanup</h2><p>cv2.destroyAllWindows()<br>time.sleep(1.0)<br>vs.stop()</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481226">October 8, 2018 at 10:32 am</a></p>
<p>  Thanks Stephen 🙂 And yes, I believe the error is due to the threading bug.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a2aa236a27d546893fc2b4f041ff89a1?s=48&d=mm&r=g">tommy</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485415">November 4, 2018 at 10:00 pm</a></p>
<p>  Dear Stephen,</p>
<p>  How about trying to chage code excution order as below?</p>
<p>  vs.stop()<br>  time.sleep(0.5)<br>  cv2.destroyAllWindows()</p>
<p>  It worked for me.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c5a614c796d471aea6466b4e6502813a?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://this-mysterious-world.blogspot.com/">Luis M</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480546">October 2, 2018 at 5:12 pm</a></p>
<p>Congratulations, Adrian! 😀</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481225">October 8, 2018 at 10:32 am</a></p>
<p>  Thank you Luis!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/fb564acbaaf1e4be591c6e3375d94771?s=48&d=mm&r=g">Ravindran</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480591">October 3, 2018 at 1:20 am</a></p>
<p>Congratulations Adrian and Trisha! Happy wedding!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481217">October 8, 2018 at 10:28 am</a></p>
<p>  Thanks so much Ravindran! 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/1a294f2b03ec8daefbac2e9e01023336?s=48&d=mm&r=g">Francisco Rodriguez</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480633">October 3, 2018 at 12:28 pm</a></p>
<p>Hello Adrian, excellent post I want to ask you a question if I follow your course pyimagesearch-gurus or buy the most extensive version of ImageNet Bundle. I could have support and the necessary information to start a project of face-recognition at a distance for example more than 8 meters</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481208">October 8, 2018 at 10:22 am</a></p>
<p>  Hi Francisco, I always do my best to help readers and certainly prioritize customers. I provide the best support I possibly can but do keep in mind that I expect you to put in the hard work, read the books/courses, and run your own experiments. I’m more than happy to keep you going in the right direction but do keep in mind that I cannot do the hard work for you. Keep up the great work! 🙂</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/1a294f2b03ec8daefbac2e9e01023336?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="https://www.linkedin.com/in/francisco-rodr%C3%ADguez-mgs-4772527a/">Francisco Rodriguez</a></p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-487722">November 18, 2018 at 5:32 pm</a></p>
<p>  Thanks Adrian, I know that the effort should be mine, the important thing is to have good bibliography and information, thank you I am very motivated and tis post are of great help especially to developing countries like in which I live</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/3fd87de1199debfb581d1be2991f7035?s=48&d=mm&r=g">Chintan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480678">October 4, 2018 at 1:55 am</a></p>
<p>Congratulations to both of you!!</p>
<p>I want to use this face recognition method in form of a mobile application. Currently I have used <a target="_blank" rel="noopener" href="https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/#0">https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/#0</a> article for developing mobile application from tensorflow for face detection.</p>
<p>Can you suggest me a direction?</p>
<p>Thanks</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/589f35f40cffd5ee7f4a26127c969a1e?s=48&d=mm&r=g">Kalicharan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480728">October 4, 2018 at 12:01 pm</a></p>
<p>I dont have 30+ pictures for each person, can i use the data augmentation tool to create many pictures of the pictures i have by blur, shifting etc</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481196">October 8, 2018 at 10:11 am</a></p>
<p>  Yes, but make sure your data augmentation is realistic of how a face would look. For example, don’t use too much shearing or you’ll overly distort the face.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e9e0d696461f09295c94202e7656e220?s=48&d=mm&r=g">Neleesh</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480814">October 5, 2018 at 3:01 pm</a></p>
<p>Congratulations Adrian, thank you for the tutorial. I am starting to follow you more regularly. I am amazed with the detail in your blogs. I am just curious how long each of these tutorial takes you to plan and author.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481183">October 8, 2018 at 9:50 am</a></p>
<p>  Thanks Neleesh. As far as how long it takes to create each tutorial, it really depends. Some tutorials take less than half a day. Others are larger, on-going projects that can span days to weeks.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/eeb4abbca8c9ae532ef80c699c22f2e5?s=48&d=mm&r=g">Huy Ngo</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480929">October 6, 2018 at 11:58 am</a></p>
<p>Hi Adrian.<br>How to apply this model on my own dataset?<br>Thank you in advance.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481169">October 8, 2018 at 9:41 am</a></p>
<p>  This tutorial actually covers how to build your own face recognition system on your own dataset. Just refer to the directory structure I provided and insert your own images.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/b2907bd098c1c5d9f3a6c86139803361?s=48&d=mm&r=g">dadiouf</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480947">October 6, 2018 at 2:51 pm</a></p>
<p>You both make a lovely couple</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481164">October 8, 2018 at 9:39 am</a></p>
<p>  Thank you 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/bf4ab4f000e5fd158ce3bcade46f10ac?s=48&d=mm&r=g">Q</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480972">October 6, 2018 at 6:52 pm</a></p>
<p>Adrian,<br>Congratulations on your marriage!<br>Take some time off for your honeymoon and enjoy the best time of your life!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481165">October 8, 2018 at 9:39 am</a></p>
<p>  Thank you so much! 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/3a0f2964cc8e6836142088899ef84696?s=48&d=mm&r=g">Rayomond</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481459">October 8, 2018 at 10:59 pm</a></p>
<p>Hearty Congratulations! Wish you both the very best</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481522">October 9, 2018 at 6:05 am</a></p>
<p>  Thanks Rayomond 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/8de1512eb8040c4462e3445f3d93c3c9?s=48&d=mm&r=g">dauey</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481698">October 10, 2018 at 4:21 am</a></p>
<p>have you liveness detection for face recognition systems?its necessary for face recognition systems.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-482190">October 12, 2018 at 9:17 am</a></p>
<p>  I do not have any liveliness detection tutorials but I will try to cover the topic in the future.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/411df6715671a6324f2f03467e5220ed?s=48&d=mm&r=g">Nguyen Anh Tuan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-482842">October 16, 2018 at 11:34 am</a></p>
<p>Congratulation man</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-483332">October 20, 2018 at 8:07 am</a></p>
<p>  Thank you!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c8d6e426dbbe16b0f86529edd3077bed?s=48&d=mm&r=g">Eric</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-483042">October 17, 2018 at 11:17 pm</a></p>
<p>Hi Adrian, Congratulations on the marriage!</p>
<p>Thank you for all the interesting posts!</p>
<p>I wonder if Adrian or anyone else has actually combined the dlib landmarks with the training described in this post? It seems to require additional steps which are not that easy to infer.</p>
<p>I have successfully created embeddings/encodings from the older posts dlib instructions but when I combine them with this posts training 100% of the faces get recognized as the same face with very high accurace despite my dataset containing several different faces. When I changed up the model I saw that it basically only recognized the first name in the dict that is created and then matches every found face to that name (in one case it even matched a backpack).</p>
<p>I spotted a difference between the dicts that get pickled. The one from this post has a text: dtype=float32 at the end of every array but the dlib dict does not have this text. Maybe this is a problem cause? In any case I can’t spot anything else I could change. But I also don’t know how to change that. (Another small difference is that this post uses embeddings in its code and the previous one calls them encodings).</p>
<p>Also, in the text above, shouldn’t it be proba &gt; T?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/6acc6215e646d233541c02622edffe80?s=48&d=mm&r=g">Sebastian</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-522283">June 19, 2019 at 9:26 am</a></p>
<p>  I’m also trying to combine those two. Did you manage to get it to work?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/4c9435780e30fdda2576808807770bef?s=48&d=mm&r=g">Naresh</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-527098">July 24, 2019 at 9:03 am</a></p>
<p>  I was also trying to combine both, Had you done that ?</p>
<p>  Please let me know.</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/ccf0d831f8e806bcaf6d11ebd868dea9?s=48&d=mm&r=g">Varun</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-483637">October 22, 2018 at 9:47 pm</a></p>
<p>Thanks a lot man</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-484505">October 29, 2018 at 2:15 pm</a></p>
<p>  You are welcome, Varun 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/3c8a3e33695ffb8da8b3f0c657b9182f?s=48&d=mm&r=g">Arvand Homer</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-483842">October 24, 2018 at 12:14 pm</a></p>
<p>Hey Adrian, thanks for the tutorial.</p>
<p>We are trying to run the code off an Nvidia Jetson TX2 with a 2.1 mm fisheye lens camera, but the frame rate of our video stream is very low and there is significant lag. Is there any way to resolve these problems?</p>
<p>Best wishes.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/429f49cb2d1764920122269d3dd01e48?s=48&d=mm&r=g">Praveen</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-484249">October 28, 2018 at 4:52 am</a></p>
<p>hi adrian, will this algo is useful for faceliveliness detection..</p>
<p>Thanq</p>
<p>with regards,<br>praveen</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-484447">October 29, 2018 at 1:25 pm</a></p>
<p>  No, face recognition and liveliness detection are two separate subjects. You would need a dedicated liveliness detector.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/da718568eca2d093384447dffcb58749?s=48&d=mm&r=g">Somo</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-484616">October 30, 2018 at 5:48 am</a></p>
<p>Hi Adrian,</p>
<p>First of all thanks for the tutorial.<br>If I were going to use the dlib’s embedding model, but wanting to change from k-NN to SVM how do I do that.</p>
<p>Thanks,<br>Somo</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485044">November 2, 2018 at 8:27 am</a></p>
<p>  You would replace use the model from <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">dlib face recognition tutorial</a> instead of the OpenCV face embedder. Just swap out the models and relevant code. Give it a try!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/7a64e4011aee9b715141b4b32a9f8c51?s=48&d=mm&r=g">akhil alexander</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-484758">October 31, 2018 at 6:21 am</a></p>
<p>Hi Andrian, your posts are always inspiring.Congratulations and wishing you a Happy married life… I invite both of you to my state, you should visit Kerala at least once in your lifetime <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=gpTMhLWUZCQ">https://www.youtube.com/watch?v=gpTMhLWUZCQ</a></p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485019">November 2, 2018 at 7:38 am</a></p>
<p>  Thank you Akhil, I really appreciate your kind words 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/2485595649da960672517be968ed626f?s=48&d=mm&r=g">Zong</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-484846">October 31, 2018 at 10:17 pm</a></p>
<p>hi Adrian，thanks for your tutorial!<br>I’m trying to replace the resnet caffemodel with squeezenet caffemodel. Simply replace the caffemodel file seems not work. How should I rewrite the code?<br>PS: Congratulations on your marriage!<br>Thanks again<br>Zong</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485013">November 2, 2018 at 7:22 am</a></p>
<p>  Hey Zong — which SqueezeNet model are you using? Keep in mind that OpenCV doesn’t support all Caffe models.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/218951b0584347261665a6d23b9bef05?s=48&d=mm&r=g">M O Leong</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485296">November 4, 2018 at 5:19 am</a></p>
<p>Hi Adrian.</p>
<p>Having attempted the 1st few sections of your post (recognize.py), surprisingly, when I run patrack_bateman.jpg it appears to recognise the photo as “adrian”. Did you actually add more photos to your dataset so that “patrick bateman” doesn’t get recognised wrongly?</p>
<p>Yes, I read further down the post that more datasets will eventually lead to much-needed accuracy. But I was just wondering how u got to the part to achieve “patrick bateman’ being ‘unknown’ or unrecognized in your tutorial example. Look forward to your feedback.</p>
<p>Many thanks!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485783">November 6, 2018 at 1:26 pm</a></p>
<p>  That is quite strange. What version of OpenCV, dlib, and scikit-learn are you using?</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/f1fbb43375a9e7805edc2e3c0e9e003f?s=48&d=mm&r=g">Harshpal</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485310">November 4, 2018 at 7:58 am</a></p>
<p>Hi Adrian, Thanks for the informative article on Face Recognition. Loved it!!!</p>
<p>I have a question on this. What if, I already have pre-trained model for face recognition (say FaceNet) and on top of it I want to train the same model for a few more faces. Is it possible to retrain the same model by updating the weights file.</p>
<p>Or how can this be done. Please suggest ideas.</p>
<p>Regards,<br>Harshpal</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485781">November 6, 2018 at 1:25 pm</a></p>
<p>  Yes. What you are referring to is called “fine-tuning” the model and can be used to take a model trained on one dataset and ideally tune the weights to work on another dataset as well.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a2aa236a27d546893fc2b4f041ff89a1?s=48&d=mm&r=g">tommy</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485420">November 4, 2018 at 10:31 pm</a></p>
<p>Hi, Adrian.<br>Always thanks for your wonderful article.</p>
<p>I have tested your code for a week.<br>It was working for small dataset(1~2 people face).<br>But when I increased number of people(upto 10), it looked unstable sometims.</p>
<p>1.In my test, sometimes, face naming was too fluctuated, I mean,<br>real name and other name was switched too frequently.<br>sometimes it worked a bit stable, but sometimes looked very unstable<br>or gave wrong face-name.<br>So as you said before, I added more pictures(more than 30 pieces)<br>to each person’s directory to increase accuracy.<br>After that, face naming seemed to get more stable, but there are<br>still fluctuated output or wrong naming output frequenty.<br>Is there any method to increase accuracy?</p>
<ol start="2">
<li>Is there possibility on a relation-formula of between face landmark points to distinguish each face more accurately? ( I tried ti find ,but I still failed.)</li>
</ol>
<p>Thanks in advance for your advice.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485774">November 6, 2018 at 1:21 pm</a></p>
<ol>
<li><p>Once you start getting more and more people in your dataset this method will start to fail. Keep in mind that we’re leveraging a <em>pre-trained</em> network here to compute the 128-d facial embeddings. Try instead fine-tuning the network itself on the people you want to recognize to increase accuracy.</p>
</li>
<li><p>2D facial landmarks in some cases can be used for face recognition but realistically they aren’t good for face recognition. The models covered in this post will give you better accuracy.</p>
</li>
</ol>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/04522096a7f5637650d858adc5ec3cb8?s=48&d=mm&r=g">Vijay</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485502">November 5, 2018 at 11:30 am</a></p>
<p>What happend if any person other than the one in data set entered in to the frame….</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485765">November 6, 2018 at 1:14 pm</a></p>
<p>  The person would be marked as “unknown”.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/4715a1d3021fab7df69092e063e97add?s=48&d=mm&r=g">Ankita</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-486925">November 13, 2018 at 1:32 pm</a></p>
<p>Hi Adrian,</p>
<p>Firstly, I would like to Congratulate you on your wedding though it’s pretty late!<br>I wish to know do you follow any algorithms, kindly mention, if any?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-486946">November 13, 2018 at 4:10 pm</a></p>
<p>  I’m not sure what you mean by “follow any algorithms” — could you clarify?</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c509c13fe67e9993d8f37067071aa9d7?s=48&d=mm&r=g">Vijay</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-487243">November 15, 2018 at 6:24 am</a></p>
<p>“Try instead fine-tuning the network itself on the people you want to recognize to increase accuracy”</p>
<p>Can u plz tell me how to do that ☺️</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/b859a4a6afb438c9e85e19b31a43bf84?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://none/">Ray</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-487976">November 20, 2018 at 2:37 am</a></p>
<p>Hi Adrian,<br>Thanks for the info.<br>I have 2 questions related to this:</p>
<p>First, How would I use an RTSP stream instead of the webcam as input. My rtsp source is in the following format:<br><a href="rtsp://username:password@IP:port/videoMain">rtsp://username:password@IP:port/videoMain</a></p>
<p>I can see this stream in vlc on any computer on my network, so i should be able to use that as the source in your script</p>
<p>Second, instead of viewing the results on my screen, how can I can Output it in a format so I can watch it from another computer. Example, How can I create a stream that I can feed into a vlc server, so I can watch it from another computer on my network.</p>
<p>Thanks for your guidance</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-488047">November 20, 2018 at 9:07 am</a></p>
<p>  Hey Ray — I don’t have any tutorials on how to display or read an RTSP stream on the Pi but I will be covering it in my upcoming Raspberry Pi + computer vision book.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/ffeb4e0fe00f937bfaae73b90edf970a?s=48&d=mm&r=g">anu</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-488455">November 22, 2018 at 3:32 am</a></p>
<p>thanks a lot for this page…<br>how do we include our own pictures into this to recognize?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-489028">November 25, 2018 at 9:31 am</a></p>
<p>  Refer to the “Project structure” section of the tutorial where I describe the directory structure for adding your images. If you need help actually building the face dataset itself, refer to <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/11/how-to-build-a-custom-face-recognition-dataset/">this tutorial.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/6524061d38516004c8776d1a970eadeb?s=48&d=mm&r=g">Teresa DiMeola</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-488561">November 22, 2018 at 1:27 pm</a></p>
<p>Hi Adrian,</p>
<p>You are so kind and generous…you must be an amazing human being. Thank you for this tutorial. I cannot wait to use it (I’m still learning some python basics…so not quite ready yet).</p>
<p>But I do have a general question for you, which is – well – not off topic entirely, but also something which you may not know of the top of your head, but anyway here goes: Can you guess at or estimate at what camera resolution/focal length one would go from being a “resolved image” to a “low resolution image?” Let’s assume for the sake of the question/answer that it is a cooperative subject.</p>
<p>Thanks again, for all you do!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-489018">November 25, 2018 at 9:22 am</a></p>
<p>  Hi Teresa — each camera will have it’s own specific resolution and focal length so I don’t think there is “one true” resolution that will achieve the best results. The results are entirely dependent on the algorithm and the camera itself.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/3e552c419c9f29c49102e873961035dc?s=48&d=mm&r=g">hendrick</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-488592">November 22, 2018 at 5:49 pm</a></p>
<p>hi adrian. i got this error “File “extract_embeddings.py”, line 62, in<br>(h, w) = image.shape[:2]<br>AttributeError: ‘NoneType’ object has no attribute ‘shape’”. This code i ran in ubuntu. But in my Mac everything was fine. I used the same version python and opencv.<br>Thank you</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-489014">November 25, 2018 at 9:19 am</a></p>
<p>  It’s not an issue with Python and OpenCV, it’s an issue with your input path of images. The path to your input images does not exist on disk. Double-check your images and paths.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/f0ce5819789d41ef1b76f16d26cb7380?s=48&d=mm&r=g">Rob</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-494951">January 6, 2019 at 6:14 pm</a></p>
<p>  Hendrick, I had the same error but it was a problem with the webcam under Ubuntu. Once I set that up correctly everything worked fine.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/77fbe4d8767ea65790520be6c88f661c?s=48&d=mm&r=g">Tim</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-488812">November 24, 2018 at 2:43 am</a></p>
<p>Hi Adrian.<br>Great job u have done~<br>Here is my question.<br>How can I plot the decision boundaries for each class after train_model?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-488997">November 25, 2018 at 9:04 am</a></p>
<p>  The scikit-learn documentation has an <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/auto_examples/svm/plot_iris.html">excellent example</a> of plotting the decision boundaries from the SVM.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d328e41d366bb46b9551236eebdd1a26?s=48&d=mm&r=g">S M Yu</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-488950">November 25, 2018 at 4:54 am</a></p>
<p>What should I do if the camera recognizes a person who is not being trained, does not appear as ‘unknown’, but appears in the name of another person?</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/4486e270fe69802ed1634cb4e5b4eaa4?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://pyimagesearch.com/">Dorra</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-489630">November 28, 2018 at 12:47 pm</a></p>
<p>Hi Doctor Adrian<br>Great job<br>I don’t understand this error ” ValueError: unsupported pickle protocol: 3 ” ?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-490008">November 30, 2018 at 9:06 am</a></p>
<p>  Re-train your face recognition model and serialize it to disk. You are trying to use my pre-trained model and we’re using two different versions of Python, hence the error.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/bfd45482a0c9aae65511db2b481b99a5?s=48&d=mm&r=g">Rico</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-489736">November 29, 2018 at 3:38 am</a></p>
<p>LabelEncoder seems to be reversing the labels. If you try to print knownNames and le.classes_, the results are reversed. So when you call le.classes_[j], incorrect mapping is done. It seems to be causing misidentification on my datasets.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/bfd45482a0c9aae65511db2b481b99a5?s=48&d=mm&r=g">Rico</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-490564">December 3, 2018 at 10:21 pm</a></p>
<p>  This happens when the list of images are not sorted. After adding sorting of the list of dataset images, it works without problem.</p>
<p>  By the way, linear SVM seems to perform bad with few dataset images per person. Using other classification algorithms such as Naive Bayes are better suited few datasets.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-490667">December 4, 2018 at 9:50 am</a></p>
<p>  Thank you for sharing your experience, Rico!</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c0911bfab60c73b1f6b3ecdf9c5a2f91?s=48&d=mm&r=g">Gyho</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-492631">December 16, 2018 at 11:03 pm</a></p>
<p>Is it possible to represent the name in other languages, i.e. Chinese?<br>Thank you very much!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-492815">December 18, 2018 at 9:03 am</a></p>
<p>  You can use whatever names in whatever languages you wish, provided Python and OpenCV can handle the character set.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e169b1cb4aaaa948364488577de5869a?s=48&d=mm&r=g">Shaun</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-493200">December 21, 2018 at 6:39 pm</a></p>
<p>Dear Adrian,</p>
<p>Many thanks for your tutorials. Step by step following your instruction, I have successfully implemented 7 tutorials on my RPi. The most fun part is this opencv face recognition tutorial. I train the model by adding my family members. It works pretty accurate at most time but sometimes either your name or your wife name pops up. LOL Anyway, your professional tutorial makes me feel like a real coder, though I am actually a dummy :). Wish you and Trisha a Merry Christmas and Happy New Year.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-493867">December 27, 2018 at 10:59 am</a></p>
<p>  That’s awesome Shaun, I’m so happy to hear you’ve been able to apply the face recognizer to your own projects 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/52d24164f316a4beb3868371ef8b8a10?s=48&d=mm&r=g">Igor</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-493328">December 23, 2018 at 10:04 am</a></p>
<p>Adrian. Great job. Please tell me how to write a file to the file? Thank.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-493853">December 27, 2018 at 10:47 am</a></p>
<p>  I’m not sure what you mean by “write a file to the file”?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/52d24164f316a4beb3868371ef8b8a10?s=48&d=mm&r=g">Igor</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-494008">December 28, 2018 at 2:08 pm</a></p>
<p>  Sorry 🙂 Write frame to file.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-494513">January 2, 2019 at 9:39 am</a></p>
<p>  You can use the “cv2.imwrite” function to write a frame to disk.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/88a10ceb37f26fcdedba65b1257e6d84?s=48&d=mm&r=g">Yong Shen</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-493473">December 24, 2018 at 3:02 pm</a></p>
<p>hi Adrian，thanks for your tutorial!<br>I tried to run this project using opencv 3.3.0 instead of 3.4.2 to avoid lengthy reinstallation… can it work in opencv 3.3.0 ?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-493834">December 27, 2018 at 10:33 am</a></p>
<p>  I would highly recommend you use OpenCV 3.4.2. You can actually <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/19/pip-install-opencv/">install OpenCV via pip</a> and save yourself quite a bit of time.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/f31e456bdf30334963e44e6fd95cbb74?s=48&d=mm&r=g">Tejesh</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-493701">December 26, 2018 at 10:09 am</a></p>
<p>Happy Married life and thanks once again for such enriching article.</p>
<p>BTW, you had in one of your articles mentioned a link to the zip file containing the General Purpose Faces to be used with the code. Can you please share that link once again over here?</p>
<p>Thanks in Advamce</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-493807">December 27, 2018 at 10:10 am</a></p>
<p>  Thanks Tejesh, although I’m not sure what you mean by the “general purpose faces” — could you elaborate?</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/656b1bf9757cef1c1ae7b37d15585b36?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://www.excis3.be/">Excis3</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-494020">December 28, 2018 at 3:26 pm</a></p>
<p>Hi Adrian,<br>Thanks for the great tutorial and clear site.<br>Its a ton of information. I just started this afternoon after searching the web on how to start, and now i have my own small dataset, and the application is running great.</p>
<p>My next step is finetuning with Face Alignment, and put more data in my dataset.</p>
<p>Thanks.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-494511">January 2, 2019 at 9:38 am</a></p>
<p>  Congratulations on already being up and running with your face recognition system, nice job!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/2e423ec864459f6c0b5bac8083109861?s=48&d=mm&r=g">Muhammad Salman Ali Khan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-494116">December 29, 2018 at 3:20 pm</a></p>
<p>Hello Adrian,</p>
<p>I am facing this error when I run train model:<br>ValueError: The number of classes has to be greater than one; got 1 class</p>
<p>In line 34 =&gt; recognizer.fit(data[“embeddings”], labels)</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-494501">January 2, 2019 at 9:30 am</a></p>
<p>  Are you trying to train a face recognizer to recognize just a single person? Keep in mind that you need <em>at least</em> two classes to train a machine learning model. If you’re trying to train a face recognition system with more than 2 classes and you still received that error then you have an issue parsing your image paths and extracting the person name/ID.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/2eed04332db45fd687612df2cfdd8354?s=48&d=mm&r=g">Kyle Anderson</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-569478">November 3, 2019 at 3:19 pm</a></p>
<p>  What happens if you do want to just train one one person, at least for the time being? I’m trying to create a python script that takes pictures of a person, then trains itself to recognize that person. There may eventually be more than one person, after more people sign up, but for the first user there would only be one person.</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/167e179e2c162ab9edb818172cb0b9b3?s=48&d=mm&r=g">Bhanu Jamwal</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-495095">January 8, 2019 at 5:43 am</a></p>
<p>now the question is how to convert this folder opencv-face-recognition in a api which one can use in web application and websites to have this feature.i will be very thankful if you remove my doubts and lead me a way out of this<br>thank you<br>bhanu</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-495101">January 8, 2019 at 6:35 am</a></p>
<p>  I demonstrated how to create a REST API for computer vision and deep learning <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/">here.</a> You’ll need to modify the code to swap out the Keras code for the OpenCV face recognition code but the framework is there. Good luck!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/167e179e2c162ab9edb818172cb0b9b3?s=48&d=mm&r=g">Bhanu Jamwal</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-495233">January 9, 2019 at 5:50 am</a></p>
<p>thank you sir.<br>you are great<br>have a nice day.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-495936">January 11, 2019 at 9:50 am</a></p>
<p>  You are welcome!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/da74df27a053582cd102e8967c00c652?s=48&d=mm&r=g">Mike</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-495344">January 9, 2019 at 9:05 pm</a></p>
<p>Hi Adrian!</p>
<p>Thanks alot for these amazing tutorials, i’ve gained lot’s of interest about the computer vision subject and i’ve been enjoying your deep learning crash course.</p>
<p>I’m doing a class project for my University, which involves face recognition.</p>
<p>One of the requirements of the teacher is the installation of the scikit-learn package.. i’ve noticed that you have used it in this tutorial.</p>
<p>Now, my concern is, my teacher also expressed that people that use PyTorch or TensorFlow will get a better grade in their projects. I’m not familiar with PyTorch but i’ve noticed in this tutorial that you do indeed have a PyTorch implementation, am i right?</p>
<p>In that case, can scikit learning and PyTorch work together? Am i misunderstanding something about this? Also, what possibly could i add in terms of PyTorch usage that could improve this tutorial that you provided (besides the points that you mention in the end of the tutorial (face-aligment, more data, etc) ?</p>
<p>Thank you!<br>Happy 2019.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-495926">January 11, 2019 at 9:41 am</a></p>
<p>  This tutorial leverages a model trained with PyTorch but it’s not actually a PyTorch tutorial. I personally prefer Keras as my deep learning library of choice. If you’re interested in combining scikit-learn with Keras be sure to take a look at my book, <a target="_blank" rel="noopener" href="https://pyimagesearch.com/deep-learning-computer-vision-python-book/">Deep Learning for Computer Vision with Python</a>, which includes chapters using both Keras and scikit-learn.</p>
<p>  Best of luck with the project!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/da74df27a053582cd102e8967c00c652?s=48&d=mm&r=g">Mike</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-495961">January 11, 2019 at 11:59 am</a></p>
<p>  I see, so in this tutorial in particular we are indeed using PyTorch and scikit together, correct?</p>
<p>  Yes, i’ve asked my teacher about that and he also says Keras wouldve been better but it would also be harder (?), i am indeed very interested in this computer vision subject and your book in particular.. just would like to know if you’re planning to have some sort of discount for students or anything like that because paying 200+ € right now as a student isn’t as easy as it would be in the future 😀</p>
<p>  Thanks Andrian!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-495971">January 11, 2019 at 1:23 pm</a></p>
<p>  No, this tutorial is using OpenCV and scikit-learn. The model itself was trained with PyTorch there is no actual PyTorch code being utilized. Instead, we are using a model that has already been trained.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/ecf65dbdcd8555ac1e3b605b4357d581?s=48&d=mm&r=g">Gaurav</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-496396">January 14, 2019 at 11:26 pm</a></p>
<p>I found This technique is not gives output accurately ..<br>please can you have a more accurate technique to recognition???</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-496654">January 16, 2019 at 9:47 am</a></p>
<p>  How you tried my suggestions in the “Drawbacks, limitations, and how to obtain higher face recognition accuracy” section of the tutorial?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/ecf65dbdcd8555ac1e3b605b4357d581?s=48&d=mm&r=g">Gaurav</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497221">January 21, 2019 at 12:48 am</a></p>
<p>  Yes I followed your Suggestions.<br>  I take 70 samples per person.<br>  then also wrong match and not match scenarios happened more times.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497438">January 22, 2019 at 9:29 am</a></p>
<p>  How many unique people are in your database? 70 samples per person is a good number but I’m curious how many total people are in your dataset?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/ecf65dbdcd8555ac1e3b605b4357d581?s=48&d=mm&r=g">Gaurav</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497585">January 23, 2019 at 6:53 am</a></p>
<p>  Adrian<br>  i include 3 peoples in my dataset.<br>  it cannot display accurately names sometime it display right name but sometime it gives the name of another person.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497847">January 25, 2019 at 7:09 am</a></p>
<p>  For only 3 people the model should be performing better. Have you used the <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">dlib face recognizer</a> as well? Does that model perform any better?</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/ecf65dbdcd8555ac1e3b605b4357d581?s=48&d=mm&r=g">Gaurav</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498871">February 1, 2019 at 1:51 am</a></p>
<p>  yes i used it</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498912">February 1, 2019 at 6:36 am</a></p>
<p>  At that point if dlib and the FaceNet model are not achieving good accuracy you may need to consider fine-tuning an existing model. But for only 3 people either dlib or FaceNet should be performing much better. I think there may be a logic error in your code so I would go back and reinvestigate.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/da74df27a053582cd102e8967c00c652?s=48&d=mm&r=g">Mike</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497707">January 24, 2019 at 8:59 am</a></p>
<p>Hey Adrian!</p>
<p>How could one implement face alignment on this tutorial?</p>
<p>I can perform face alignment because of your other tutorial but don’t know what i’m supposed to do with the new aligned faces.. would you save them directly in your dataset? If so, how?</p>
<p>Thanks in advance.<br>Cumps!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497843">January 25, 2019 at 6:56 am</a></p>
<p>  Take a look at my <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/">face alignment tutorial</a> on how to properly align faces. You would want to align them before computing the 128-d face embeddings.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e3f3ecd952301d83f453eb8af7de29bc?s=48&d=mm&r=g">Abdull</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497855">January 25, 2019 at 7:17 am</a></p>
<p>Hi Adrian may i ask why do u resize the image in the first place?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497882">January 25, 2019 at 7:52 am</a></p>
<p>  High resolution images may look visually appealing to us but they do little to increase the accuracy of computer vision systems. We reduce image size to (1) reduce noise and thereby increase accuracy and (2) ensure our algorithms run faster. The smaller an image is, the less data there is to process and the faster the algorithm will run.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/e3f3ecd952301d83f453eb8af7de29bc?s=48&d=mm&r=g">Abdull</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497886">January 25, 2019 at 8:49 am</a></p>
<p>  So it’s basicly about the dimension reduction feature of scikit-learn?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498379">January 29, 2019 at 7:07 am</a></p>
<p>  We are reducing the dimensions of the image/frame but I’d be careful calling it “dimensionality reduction”. Dimensionality reduction typically refers to a set of algorithms that reduce the dimensionality of an input set of features based on some sort algorithm that maximizes feature importance (PCA is a good example). Here aren’t removing pixels based on “importance”, we’re simply preprocessing the input image by reducing its size.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/b880cd6ec34396048b6bcdf245ed07bd?s=48&d=mm&r=g">Philipe Huan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498131">January 27, 2019 at 3:22 pm</a></p>
<p>greetings , i have a question, in the file of labels, their content has only a name per person recognized or are there names for represent each image file?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498359">January 29, 2019 at 6:47 am</a></p>
<p>  Sorry, I don’t think I understand your question. Could you elaborate?</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e595fb67d3ba0ceabfdc15ca0368c982?s=48&d=mm&r=g">Dario</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498239">January 28, 2019 at 3:56 pm</a></p>
<p>Hello Adrian, how we could train a model to recognize rotated faces in different angles??? I want to make facial recognition through a eye fish camera</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498254">January 28, 2019 at 5:43 pm</a></p>
<p>  You normally wouldn’t do that. You would detect the face and then perform <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/">face alignment</a> before performing face recognition.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d7f52723c6e058db6de8d575812c1789?s=48&d=mm&r=g">benny</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498446">January 29, 2019 at 7:42 pm</a></p>
<p>Hi Adrian, if I previously have many images trained using the SVM, and now I have several additional images (correspond to new people), I need to retrain the SVM by scanning through all 128-d vectors. It would take a lot of time when the number of images is kept increasing.</p>
<p>Is there any tricks to improve this scalability issue? Thank you</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498954">February 1, 2019 at 7:21 am</a></p>
<p>  You are correct, you would need to re-train the SVM from scratch. If you expect that more and more faces will be added I suggest you look at “online learning algorithms”.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/d7f52723c6e058db6de8d575812c1789?s=48&d=mm&r=g">benny</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-501194">February 13, 2019 at 9:51 pm</a></p>
<p>  Thank you. Apart from the scalability issue, I would like to know the performance of SVM compared with other simple classifier. For example, L1, L2 distance, and cosine similarity. Any comments on this comparison? Thanks</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-501292">February 14, 2019 at 12:48 pm</a></p>
<p>  Are you asking me to run the comparison for you? While I’m happy to provide this code to you for free please keep in mind that I’m not going to run a bunch of additional experiments for you. This blog is here for you to learn from, to get value from, and better yourself as a deep learning and computer vision practitioner.</p>
<p>  I would highly encourage you to run the experiments and note the results. Let the empirical results guide you.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/0c20efc2c178862aa3ffaf6be7a3b464?s=48&d=mm&r=g">San Man</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-499181">February 2, 2019 at 12:06 pm</a></p>
<p>Hello Adrian,<br>Congratulations for your wedding!<br>Thanks so much for sharing your knowledge, it’s just incredible what you are doing.<br>I was going through your code. When I ran it, the faces which were there in the model were detected accurately. But the faces which were not there were detected wrongly as some one else.<br>I had about 10-12 images of each person.<br>Any idea on how I can reduce the false positives?</p>
<p>Thanks,<br>Sandeep</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-499743">February 5, 2019 at 9:33 am</a></p>
<p>  See <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/">this tutorial</a> for my suggestions on how to improve your face recognition model.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e67d8f890dadcfd32ad3fef8a7298419?s=48&d=mm&r=g">Hala</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-499325">February 3, 2019 at 10:35 am</a></p>
<p>hi Adrian,<br>many thanks for your efforts, i have a question please:<br>if i have many many images for many faces and i need to group it automatically by unique ids (grouping all faces to the same user in one unique id), how i can do it?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-499739">February 5, 2019 at 9:30 am</a></p>
<p>  See <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/07/09/face-clustering-with-python/">this tutorial on face clustering.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d289824d54baae6c7b900f9bd0a98fc4?s=48&d=mm&r=g">Roberto Marcoccio</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-499772">February 5, 2019 at 11:19 am</a></p>
<p>Hi, i tried to build my own face normalized dataset applying face alignment you described in the other topic, but that causes to have all 256×256 pixels “aligned” images ….Applying on that the extraction of embedding I noticed something was wrong because not all the images were processed. Debugging finally I got that the face detection of the extraction step of this module applied on the 256×256-sized images obtained cropping the ROI of the alignment step doesn’t work well. To confirm that I also just modified the routine cropping the ROI for each image from the face detection (without performing alignment) and saving it as new dataset and the extraction step just serialized 1 encoding !!!! Summarizing it seems that a further face detection applied on images already “detected” and saved with the dimension of the ROI doesn’t work. If that I don’t know how to apply alignment to normalize my face dataset. Could you pls help ???????????</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-500292">February 7, 2019 at 7:26 am</a></p>
<p>  I’m a bit confused regarding your pipeline. You performed face detection, aligned the faces, and saved the ROI of the face to disk, correct? From there all you need to do is train your model on the aligned ROIs (not the original images).</p>
<p>  If only 1 encoding is being computed then you likely have a bug in your code (such as the same filename is being used for each ROI and the files are overwriting each other). You may have a path-related issues as well. Double-check and triple-check your code as it’s likely a logic problem.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/ddc6793b79ee9fc718cf4f8c5bdfaabb?s=48&d=mm&r=g">Muhammad Hassam</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-499811">February 5, 2019 at 2:58 pm</a></p>
<p>hi Adrian thanks for this amazing post<br>i wondering if i could use this code on raspberry 3 pi b+ or not ?<br>i used this <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/25/raspberry-pi-face-recognition/">https://pyimagesearch.com/2018/06/25/raspberry-pi-face-recognition/</a> tutorial and this worked fine</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-500288">February 7, 2019 at 7:20 am</a></p>
<p>  Yes, but the face recognition will be very slow. You may also need to use a Haar cascade instead of a deep learning-based face detector.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5c0984f434f8857cd952f0fa1e34aad0?s=48&d=mm&r=g">Steve</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-569515">November 3, 2019 at 9:49 pm</a></p>
<p>  Muhammad, I have a raspberry pi and a camera located where I want to capture images and then the images are sent back to my main PC for processing. There are probably a lot of ways to do it, but this pyimagesearch topic got me on the right path: <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2019/04/15/live-video-streaming-over-network-with-opencv-and-imagezmq/">https://pyimagesearch.com/2019/04/15/live-video-streaming-over-network-with-opencv-and-imagezmq/</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/32d98e0b564017a3213e26a993691dec?s=48&d=mm&r=g">Vishal</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-501053">February 13, 2019 at 4:32 am</a></p>
<p>1)hey Adrian how can i self tuned this code can you guide?</p>
<p>2)and also in this code I not get Unknown Label to any unknown person?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-501305">February 14, 2019 at 12:59 pm</a></p>
<p>  Both of your questions can be address in <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/">this tutorial.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/32d98e0b564017a3213e26a993691dec?s=48&d=mm&r=g">Vishal</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-501206">February 13, 2019 at 11:56 pm</a></p>
<p>How the Self tuning will be done?<br>can you guide me about it.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a4bc224bb49119d319c20703453237c4?s=48&d=mm&r=g">Ucha Samadashvili</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-501430">February 14, 2019 at 6:55 pm</a></p>
<p>Hello Adrian,<br>firstly, I am grateful for your work. It has helped me for my Senior Design class project.<br>I want to ask you a question:<br>The way machine learning algorithms usually work (from what I understand) is, it gets trained on dataset allowing the algorithm to set weights. When training is done and we want to predict or classify we simply input the new data into a function which already has weights set. Effectively we do not have to compare the new data to all the previous data.<br>Now, the algorithm for face recognition you described has to look for a face at each frame and then encode it and then compare it to every single encoding in the database. While this is fine for my project since we are only 3 in the group and each has about 50 images in their face directories, it is relatively slow. Yes, I am running the program on a CPU and I understand it can be much faster. However, is there a way of training the machine in such a way that instead of going through each individual encodings (150 in my case) it can go through only 3 where each encoding is going to be some kind of average of one persons face. I know doing the avarage is kind of silly coz of angles and facial expressions etc. but there got to be a way for it work faster.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-501531">February 15, 2019 at 6:21 am</a></p>
<p>  There are a few questions here so let me answer them individually.</p>
<ol>
<li><p>Yes, many machine learning algorithms are trained on a set of data, any weights/parameters are set during the training, and the model is serialized to disk. Keep in mind we’re doing the same thing here though with just a few caveats. We have a pre-trained face recognizer that is capable of producing 128-d embeddings. We treat it as a “feature extractor” and then train a model on top of those 128-d embeddings. Think of the face embedder as a feature extractor and you’ll see how it’s just the same method.</p>
</li>
<li><p>You won’t obtain much of a speedup by training from scratch. The model will still need to perform a forward-pass to compute the 128-d embeddings. The only step you’re removing is the Linear SVM which will be pretty fast, regardless.</p>
</li>
<li><p>That said, if you want to train your own custom network refer to the documentation I have provided in the tutorial as well as the comments.</p>
</li>
</ol>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/fd5332518b2fc5cf8240c792b2a62fa5?s=48&d=mm&r=g">ucha</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-501581">February 15, 2019 at 1:14 pm</a></p>
<p>  I don’t think you understood my question. Perhaps, I did not phrase it correctly. Finding a face on each frame is very similar to what other machine learning algorithms do. What I was asking about is comparing the already embedded face to each and every face encoding in the database. To be precise, the efficiency of the voting system is under the question. I was wondering if it is possible to compare the encoded face from frame to some kind of average encoding of each person in the database.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-502538">February 20, 2019 at 12:52 pm</a></p>
<p>  It would be easier to instead <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/">perform face alignment</a>, average all faces in the database, and then compute the 128-d embedding for the face.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/077be0f39a6a7c60ca2ecb468723621e?s=48&d=mm&r=g">Neha Jain</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-502648">February 21, 2019 at 1:54 am</a></p>
<p>Hi Adrian.<br>Can this library be supported by Python 2.7 and Windows Operating system</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-502833">February 22, 2019 at 6:37 am</a></p>
<p> Technically yes, but you’ll need to install dlib on Windows. Please keep in mind that I don’t support Windows here on the PyImageSearch blog. I highly recommend you use a Unix-based OS such as Ubuntu or macOS for computer vision.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/6c4ae254327c088cd3c528919e0116d0?s=48&d=mm&r=g">Vasya</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-503059">February 23, 2019 at 1:53 pm</a></p>
<p>Hi Adrian!<br>Thank you so much for your work. Is there a way to add images of new people to an already trained system without running through all already existing images?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-503746">February 27, 2019 at 6:14 am</a></p>
<p> Yes, you can insert logic in the code to check and see if a face has already been quantified by the model (the file path would serve as a good image ID). If so, skip the image (but still keep the computed 128-d embedding for the face). The actual model will need to be retrained after extracting features.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/030d98bc0721f87cfd5f87a91cd0d1e8?s=48&d=mm&r=g">Huang-Yi Li</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-503462">February 26, 2019 at 1:34 am</a></p>
<p>If I want to update a new person into our model, whether this model can not be retrained.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-503719">February 27, 2019 at 5:45 am</a></p>
<p> Yes, the model will have to be re-trained if you add in a new person.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/06066a56b243fb4dbc24ff62731323a2?s=48&d=mm&r=g">Pankaj Kumar</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-504697">March 4, 2019 at 4:28 am</a></p>
<p>Hello Adrian,<br>can u please tell me why u passing unknown person images, this model itself should recognize unknown person if it not trained on that person….<br>i want to use SVC only..is their a way to achieve this</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-504982">March 5, 2019 at 8:47 am</a></p>
<p> The purpose of the unknown class is exactly that — label people as “unknown” if they are not in the training set. You can use an SVM with a linear kernel to obtain your goal.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/6164e359a9fb5a1e747ca4b6504367b0?s=48&d=mm&r=g">kharman</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-505787">March 9, 2019 at 10:35 am</a></p>
<p>will this post will help to create a project to recognize plants?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506570">March 13, 2019 at 3:54 pm</a></p>
<p> No, you should use a different type of machine learning or deep learning than that. If you’re new to the world of computer vision and image processing take a look at <a target="_blank" rel="noopener" href="https://pyimagesearch.com/practical-python-opencv/">Practical Python and OpenCV</a> which includes an introductory chapter on plant classification.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/9ada9d76d56192c7abee93a8cc8c6f46?s=48&d=mm&r=g">Sari</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-505794">March 9, 2019 at 11:21 am</a></p>
<p>Why linear svm classifier is better than knn classifier?<br>Which method is most effective when we have dataset and many faces?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506567">March 13, 2019 at 3:52 pm</a></p>
<p> Hey Sari — I cover machine learning concepts <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2019/01/14/machine-learning-in-python/">this tutorial.</a> That post will help address your question.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/7f3c62278bccbaac09bcd3f830058e07?s=48&d=mm&r=g">Mustapha Nakbi</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506080">March 11, 2019 at 10:09 am</a></p>
<p>Hi adrian,<br>I am not satisfied with the SVM trained model, can i define my own deepLearning network(using tensorflow) instead of svm to get better result?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506548">March 13, 2019 at 3:34 pm</a></p>
<p> Have you tried fine-tuning the existing face embedding model? that would be my primary suggestion.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/7f3c62278bccbaac09bcd3f830058e07?s=48&d=mm&r=g">Mustapha Nakbi</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506710">March 14, 2019 at 9:54 am</a></p>
<p>  I am using openface the same embedder model, how to make tuning ,please tell me.<br>  and, is’t possible after extracting the face region i will train the CNN with these regions?</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a0af53a27d84f929e1bd889ac1d85625?s=48&d=mm&r=g">Jaiganesh</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506264">March 12, 2019 at 5:12 am</a></p>
<p>Hi Adrain,<br>I am working for Face recognition feature implementation for Robot to recognize registered office members face. So, in order to recognize face, we can only capture “few pictures (max 5?)” from my office members and i will not able to collect more pictures of each and everyone. With these few samples, we will need to do the face recognition.</p>
<p>With this requirement in my hand, i found your previous post (<a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/</a>) on dlib with face_recogtion library and tested with few of my team member face pictures (“aligned” with <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/">https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/</a>), but it is not recognizing correctly as expected (identifying as wrong person). May be my team members are chinese and look similar?</p>
<p>So, here i need your advise and suggestion on which one to use?<br>Should you use this post application (OpenCV Face Recognition)? Or your previous post with dlib? for my above development scenario? Please suggest.</p>
<p>And regarding porting train_model.py script on dlib based face recognition application, i have copied recognizer.pickle and le.pickle from this post to other application with dlib on the same output directory. And also modified the train_model.py with “encodings” text to look for encodings.pickle file and ran the train_model.py script. But after this training model script, i see still the face recognition is not so accurate as expected for Robot. Please correct me if i did anything wrong here.</p>
<p>Please guide and help on this. Thank you.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506535">March 13, 2019 at 3:19 pm</a></p>
<p> I would try using dlib’s embedding model and then try training a Linear SVM or Logistic Regression model (from this post) on the extracted embeddings. I’ve found dlib’s model to be a bit more accurate.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/033912e2a7fb9fd1499af28b9fd283aa?s=48&d=mm&r=g">Johan</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-508301">March 21, 2019 at 7:55 am</a></p>
<p>  which post?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-508512">March 22, 2019 at 8:39 am</a></p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">This one.</a></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e82277641ca237ee59b9b4ce03465cdb?s=48&d=mm&r=g">tony</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506593">March 13, 2019 at 4:52 pm</a></p>
<p>Hi Adrian</p>
<p>Thanks a lot for such an informative post. I have followed the procedure to train my own set of images and recognize. I had put 6 images of a person in the folder dataset &gt; name of the person. My question is if the network cannot work effectively for the new set of images, how does it classify you or trisha for just 6 images ?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-507868">March 19, 2019 at 10:31 am</a></p>
<p> It’s not that the network will “never work” with a small image dataset — it’s that larger image datasets are always <em>preferred</em> for higher accuracy and reliability.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/80a2286a020e98bf7f46af5fec98f092?s=48&d=mm&r=g">Arbaz Pathan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506848">March 15, 2019 at 10:33 am</a></p>
<p>I have done this project, and done it using webcam. Now when the frame window is opening it is giving an fps of 0.34 to 0.40 and it is lagging very much. Due to this we are not getting accurate output. The RAM consumption during this process is 92%. So please do tell us how to resolve this issue. Is this the problem of webcam or raspberry pi?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-507857">March 19, 2019 at 10:21 am</a></p>
<p> If you’re trying to perform face face recognition on the Raspberry Pi you should be following <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/25/raspberry-pi-face-recognition/">this tutorial instead.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/39ce36b5470dcd086f356208e1af4a7b?s=48&d=mm&r=g">Danny</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-508477">March 22, 2019 at 7:03 am</a></p>
<p>I have to use Deep learning classifiers instead of linear support vector classifier …how it can be done?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/39ce36b5470dcd086f356208e1af4a7b?s=48&d=mm&r=g">Danny</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-508482">March 22, 2019 at 7:37 am</a></p>
<p> Adrian , SVM is not satisfactory … could pls refer me a deep learning model to train on the embeddings…for better accuracy… and if any new face is detected it is not recognizing as unknown…</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-508494">March 22, 2019 at 8:24 am</a></p>
<p>  Hi Danny — you’ll definitely want to read <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/">this tutorial</a> where I share my suggestions on obtaining higher face recognition accuracy.</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/fc5db8e72efe50223bb3046b422a7b14?s=48&d=mm&r=g">Tara Prasad Tripathy</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-509008">March 25, 2019 at 4:17 am</a></p>
<p>Hi Adrian,<br>I was wondering whether the dlib pipeline which you wrote in another post, takes care of face alignment or do we have to incorporate it?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-509428">March 27, 2019 at 8:55 am</a></p>
<p> No, you need to manually perform face alignment yourself. Refer to <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/">this tutorial on face alignment.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/91ed5366e48112b91a116efadecf6cee?s=48&d=mm&r=g">fajar yuda pratama</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-509907">March 29, 2019 at 9:10 am</a></p>
<p>can we train new face without delete last train? for add face data without train all of face again</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-510611">April 2, 2019 at 6:15 am</a></p>
<p> I have addressed that comment in the comments section a few times, please give the comments a read.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/77be938c31a0df5e77feedaac8024824?s=48&d=mm&r=g">mithil</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-510817">April 3, 2019 at 1:21 am</a></p>
<p>how to improve the accuracy ??? for me the result is in-accurate. wrong prediction for faces</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-511128">April 4, 2019 at 1:27 pm</a></p>
<p> Kindly take the time to read the tutorial. I cover your question in the “Drawbacks, limitations, and how to obtain higher face recognition accuracy” section.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/f210f71654aadd78fd768f04e0892327?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://university/">qusay</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-512007">April 9, 2019 at 11:26 am</a></p>
<p>Hi Adrian .. i have project on face recognition but i can not know how to dteremine the percentage of error of algorithms that used in face recognition .and what is the best algorithm for face recognition .Thanx</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/8b3bcc677b86c738f4b101c8119ee4e9?s=48&d=mm&r=g">Victoria</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-512044">April 9, 2019 at 3:30 pm</a></p>
<p>Hey Adrian,</p>
<p>Thank you so much for this guide! My question is, do I need to input the names of the folders into the code where it says “name” or will that assign itself automatically? For example, in lines 47-53, can I leave it as “name” or should I say “adrian = imagePath.split(os.path.sep)[-2]” instead? I’m also having difficulty setting everything up, I’m new to OpenCV but I believe I have version 3.4.7. is this okay or should I get 3.4.2 instead? Thanks in advance!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-512506">April 12, 2019 at 12:18 pm</a></p>
<p> You don’t have to change the code, just update the names of the directories and the images inside each of the directories.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/1d999c0924278eab9677b5818acc8679?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://gmail.com/">sivaparvathi</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-512130">April 10, 2019 at 3:56 am</a></p>
<p>Hi Ardian,</p>
<p>Is it possible when the unknown person is came ,it detects unknown and generating Id for him/her. If again same that unknown person will come,It have to show previous generated Id .</p>
<p>If possible please resolve my issue.Thanks in advance</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d1b65d1602b4578055b667f913699a72?s=48&d=mm&r=g">Zheng Li</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514139">April 22, 2019 at 9:13 am</a></p>
<p>Hi, Adrian,</p>
<p>Did you had test the LightenCNN face recognizition model(<a target="_blank" rel="noopener" href="https://github.com/AlfredXiangWu/face_verification_experiment">https://github.com/AlfredXiangWu/face_verification_experiment</a>)? How about it compared to OpenFace,and dlib’s embedding mode?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514706">April 25, 2019 at 8:57 am</a></p>
<p> I have not tested that model, I am not familiar with it.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/07e581b9b8196cc9dd8d9a52017a625a?s=48&d=mm&r=g">唐国梁</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514182">April 22, 2019 at 1:50 pm</a></p>
<p>Thanks a lot . it is really helpful. It worked well.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514702">April 25, 2019 at 8:55 am</a></p>
<p> You’re welcome, I’m glad you found it helpful!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5ec2d7c7a8523b7dc3281ad7232651ff?s=48&d=mm&r=g">Quek Yao Jing</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514635">April 25, 2019 at 3:14 am</a></p>
<p>Hi, Adrian I am a fan of your blog. Your blog had really helped me learn OpenCV a lot. From the previous tutorial, DLIB is used for face detection and k-nearest neighbor is used for a face recognition/classification. While in this tutorial OpenFace is used for face detection and SVM is used for face recognition and classification.</p>
<p>From what I had experience from your code, face detection is more accurate using DLIB while SVM is better in the classification of faces. So, now I am planning on using 128-d embeddings generated from DLIB and use SVM for classification.</p>
<p>My question is if I used this method, will the false positive still occurs if I will need to recognize the 1000-10,000 of people? Because I just wonder how the API like AWS, Microsoft Azure can get really good accuracy despite so many people is using their API.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514664">April 25, 2019 at 8:36 am</a></p>
<p> For 1,000-10,000 people you should really consider fine-tuning the model rather than using the pre-trained model for embeddings. You will likely obtain far better accuracy.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/5ec2d7c7a8523b7dc3281ad7232651ff?s=48&d=mm&r=g">Quek Yao Jing</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514728">April 25, 2019 at 9:16 am</a></p>
<p>  Thanks for your reply Dr Adrian, what does fine-tuning the model means? Does it mean we need to retrain the K-NN or SVM model for the classification process or we need to retrain a custom model for face detection? Because it seems like dlib doing a good job detect face inside the image.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514755">April 25, 2019 at 9:40 am</a></p>
<p>  No, fine-tuning the model means taking the existing model weights and re-training it on faces/classes it was not originally trained on. <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/05/14/a-gentle-guide-to-deep-learning-object-detection/">This post</a> covers fine-tuning in the context of object detection — the same applies to face recognition as well.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/5ec2d7c7a8523b7dc3281ad7232651ff?s=48&d=mm&r=g">Quek Yao Jing</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514758">April 25, 2019 at 9:49 am</a></p>
<p>  Thanks Dr Adrian. I will check on your post. I had googled a bit and found out that it seems to be related to Deep Transfer Learning (DTL).</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/54d5b8e9208c10e60499d2d4279eaf60?s=48&d=mm&r=g">Hamdi Abd</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-515180">April 28, 2019 at 8:31 pm</a></p>
<p>why i get this issue !!<br>serializing 0 encodings . . .<br>what i should do !!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-515612">May 1, 2019 at 11:49 am</a></p>
<p> It sounds like the path to your input directory of images is not correct. Double-check your file paths.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/795f1a46a158a7cd900581c80acd6da6?s=48&d=mm&r=g">Elijah</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-516648">May 8, 2019 at 3:25 pm</a></p>
<p>Hi, which of the recognition methods is more efficient? This tutorial or the previous?</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/862a7fa2a346399b41484f7252e83573?s=48&d=mm&r=g">Marcello Beneventi</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-517276">May 12, 2019 at 6:25 am</a></p>
<p>Hi Adrian,</p>
<p>Thanks for such awesome blogs and I really learnt many concepts from you. You are kind of my Guru in computer vision.</p>
<p>I needed a little help, I am trying to combine face recognition and object detection both in single unit to preform detection on single video stream. How I am suppose to load 2 different model to process video in single frame? Kindly help.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-517809">May 15, 2019 at 3:01 pm</a></p>
<p> I would suggest you take a look at <a target="_blank" rel="noopener" href="https://pyimagesearch.com/deep-learning-computer-vision-python-book/">Raspberry Pi for Computer Vision</a> where I cover object detection (including video streams) in detail. I’d be happy to help but make sure you read the book to understand the concepts first.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/36eab9d9ae30563e69b03af1df915332?s=48&d=mm&r=g">Taka</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-517327">May 12, 2019 at 2:01 pm</a></p>
<p>Thanks Adrian, you’re just a rare being….</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-517804">May 15, 2019 at 2:58 pm</a></p>
<p> I assume that’s a good thing? 😉</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/ca7c7c2256b6729332a0a2e06c881d8f?s=48&d=mm&r=g">Yusuf Yasin</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-517384">May 13, 2019 at 12:04 am</a></p>
<p>Hey all,</p>
<p>I downloaded the code and made sure all the dependencies and libraries were installed. Unfortunately, whenever i run the code it works for the first couple of seconds identifying faces perfectly, then after a few seconds it causes the PC to crash resulting in a hard reboot.</p>
<p>Has anyone else been facing this problem. If so any help is much appreciated, thanks!</p>
<p>I’ve already posted this once before but it doesn’t seem to appear to be posted.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-517800">May 15, 2019 at 2:56 pm</a></p>
<p> That’s definitely odd behavior. What are the specs of your PC? And what operating system?</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/950d1207c1f6be14d91eee39ac605149?s=48&d=mm&r=g">bagas</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-517883">May 15, 2019 at 8:52 pm</a></p>
<p>Hey Adrian, i use 9500 class data train and success, but i running script recognize_video.py very slowly. can help?</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e9f998df92b979c497155fe7325ccc34?s=48&d=mm&r=g">quynhnttt</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-517915">May 16, 2019 at 3:26 am</a></p>
<p>Hi adrian<br>Can I do this project with IP camera?<br>thank you</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5b72e542cc0edfa62b81434393063980?s=48&d=mm&r=g">david</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-518191">May 18, 2019 at 11:35 am</a></p>
<p>Hi Adrian<br>Can I do this project with camera IP or camera USB</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a0c010949f6ddd07b244c9b805a950aa?s=48&d=mm&r=g">Mohamad</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-519197">May 25, 2019 at 6:31 am</a></p>
<p>Hi Adrian</p>
<p>when i run the code i get this error</p>
<p>AttributeError: ‘NoneType’ object has no attribute ‘shape’</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/a0c010949f6ddd07b244c9b805a950aa?s=48&d=mm&r=g">Mohamad</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-519199">May 25, 2019 at 6:33 am</a></p>
<p> the error located in line 50 image = imutils.resize(image, width=600)</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-519968">May 30, 2019 at 9:31 am</a></p>
<p>  Your path to the input image is correct and the returned image/frame is None. Double-check the path to your input file.</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/591547bb8a684e01255d7f703b7c3b6f?s=48&d=mm&r=g">jon</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-520474">June 2, 2019 at 2:23 am</a></p>
<p>thanks for this awesome tutorial!<br>would you point me to the right direction on how i can track the learning of this model?</p>
<p>thanks again</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-521039">June 6, 2019 at 8:30 am</a></p>
<p> What do you mean by “track the learning”?</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/7a5b63029d238d750844971e7b8099b4?s=48&d=mm&r=g">Michael Maher</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-520555">June 2, 2019 at 5:23 pm</a></p>
<p>Hey Mr.Adrian</p>
<p>Thank you so much for all your hard work, sharing with us all this knowledge.</p>
<p>You published many face recognition methods, which one would you consider the most accurate? I am building a project where I want to depend on “face unlock” to unlock/open,</p>
<p>What is the best method suitable for this in your opinion?</p>
<p>Thanks.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-521033">June 6, 2019 at 8:07 am</a></p>
<p> It depends on the project but I like using the <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">dlib face recognition embeddings</a> and then training a SVM or Logistic Regression model on top of the embeddings.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a8d13d40a4e9bc09904310fc74a68313?s=48&d=mm&r=g">kayle</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-520747">June 4, 2019 at 9:30 am</a></p>
<p>great work !! this program can recognise only human faces no ? cause i am wondering if i can use it to recognise my plants on real streaming time is it possible to do that just by giving pictures of my plants ? if its not the case can you give the path of a program that can do that please</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-521003">June 6, 2019 at 6:53 am</a></p>
<p> No, this method is used only for face recognition.</p>
<p> For plant recognition I would recommend either <a target="_blank" rel="noopener" href="https://pyimagesearch.com/deep-learning-computer-vision-python-book/">Deep Learning for Computer Vision with Python</a> or <a target="_blank" rel="noopener" href="https://pyimagesearch.com/pyimagesearch-gurus/">the PyImageSearch Gurus course</a> (both of which cover plant recognition).</p>
<p> I hope that helps!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e142ea9ee3b7df2a20c48dfc14919858?s=48&d=mm&r=g">Abay</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-521285">June 8, 2019 at 10:23 am</a></p>
<p>Adrian, thank you for such a great article!</p>
<p>I tried to use my own trained model on PyTorch and exported into ONNX. However when I try to read it with OpenCV I get errors. I found that overall people have problems with importing deep learning models into cv.dnn (Keras, PyTorch(Onnx), etc).</p>
<p>So my question is: isn’t it better to have a separate microservice (e,g. flask + PyTorch) which will serve requests coming from an app which probably uses OpenCV to send image/images(in case of video stream). How such architecture will differ in terms of speed compared to the case when open cv uses a pretrained model as you showed above.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-521691">June 12, 2019 at 1:49 pm</a></p>
<p> The OpenCV, PyTorch, Keras, TensorFlow, ONNX, etc. ecosystem has a long way to go. It’s a good initiative but it can be a real pain to convert your models.</p>
<p> You can technically use a microservice but that increases overhead due to HTTP requests, latency, etc.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/6ed83c97290c420eedddea58f9ffd36e?s=48&d=mm&r=g">Pyro</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-521736">June 12, 2019 at 9:06 pm</a></p>
<p>Hello Adrian, when i download and use your trains and code without changing anything with adrian.jpg. There are a lot of squares in your face and all like %50 adrian. There are like 7-8 squares (adrians). I gave it a try with my photos, added like 40 photos, removed outputs.</p>
<p>First i used extract_embeddings.py, then i trained it with the script and i tried to recognize and result is same. There are like 7-8 squares in my face and all are %40-50.</p>
<p>Help me please :/</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-521787">June 13, 2019 at 9:36 am</a></p>
<p> The fact that there are multiple face detections which is the root of the issue. What version of OpenCV are you using?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/6ed83c97290c420eedddea58f9ffd36e?s=48&d=mm&r=g">Pyro</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-521797">June 13, 2019 at 11:12 am</a></p>
<p>  Hello Adrian, i use OpenCV 4.0. i took a look at your <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/">https://pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/</a> but i couldn’t figure how to use both together.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-522364">June 19, 2019 at 2:28 pm</a></p>
<p>  I would suggest taking a step back. Start with a fresh project and apply <em>just</em> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/">face detection</a> and see if you are able to replicate the error.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/688eac7077211067ae7a840e1f0b04b3?s=48&d=mm&r=g">Khoa</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-522714">June 23, 2019 at 3:04 pm</a></p>
<p>Hello,</p>
<p>I ran your code successfully. However, in some cases, I want to filter the images with lower confidence. For example, the code recognizes two people as me with the confidence 98.05% and 92.90%. How can I filter the ones below 95% ?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-523165">June 26, 2019 at 1:30 pm</a></p>
<p> All you need is an “if” statement. Check the confidence and throw out the ones that are &lt; 95%.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5d08cc9c16d1be4db348db058df1e2f7?s=48&d=mm&r=g">kumar sahir</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-522842">June 24, 2019 at 11:33 am</a></p>
<p>Dear adrian,<br>first thank you for your excellent tutorial it is very helpful, I am PhD student in computer science, I saw your tutorial about facial recognition, I was very interested in your solution, and i want to know if it is possible to make the search on web application (From web Navigator) instead of using shell commande, thnak you very much</p>
<p>Cordially</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-523150">June 26, 2019 at 1:18 pm</a></p>
<p> Yes, absolutely. I would recommend you wrap the project in a REST API. <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/">This tutorial</a> would likely be a good start for you.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d35c797a51d50787d326144245205610?s=48&d=mm&r=g">sizbro</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-522849">June 24, 2019 at 12:30 pm</a></p>
<p>Hey Adrian, I know its been a while since you answered a question on this post, but I have one lingering curiosity. I have been trying to add members of my own family to the dataset so it can recognize them. Right now I have been having an issue with the labels as it still shows either ‘Adrian’ or ‘Trisha.’ Do you know how I can edit the labels so that the names of my family are there instead?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-523147">June 26, 2019 at 1:17 pm</a></p>
<p> I’m not sure what you mean by it’s been “awhile”. I regularly comment and help readers out on this post on a weekly basis. You should take a look at the recent comments before making such a statement 😉</p>
<p> As for adding family members you need to:</p>
<ol>
<li><p>Delete the “Adrian” and “Trisha” directories and add in your respective family members  </p>
</li>
<li><p>Extract the facial embeddings from your dataset  </p>
</li>
<li><p>Train the model</p>
<p>From there your family members names will show up.</p>
</li>
</ol>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/3120253ce41a8a43a06ccb06f34e65ee?s=48&d=mm&r=g">Mudassir Ahmed Khan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-525142">July 10, 2019 at 4:01 am</a></p>
<p>What are command line arguments and their parsing?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-525171">July 10, 2019 at 9:32 am</a></p>
<p> You can read about command line arguments <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/03/12/python-argparse-command-line-arguments/">in this tutorial.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d7234df5deec5567e23c575aaf0f7dce?s=48&d=mm&r=g">Khang</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-525653">July 14, 2019 at 5:40 am</a></p>
<p>How do I apply facial landmarks on this tutorial to increase accuracy of recognition?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-527287">July 25, 2019 at 10:06 am</a></p>
<p> You can use them to perform <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/">face alignment.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/06f302054df1716601d88ef472798e08?s=48&d=mm&r=g">Rajath Bharadwaj</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-526297">July 19, 2019 at 3:57 am</a></p>
<p>I was wondering how to recognize multiple faces. Could you give me some leads on that? That’d be great and very helpful.</p>
<p>And thank-you for all your great tutorials and codes. Really, it’s helped me a lot!<br>Thanks once again.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-527259">July 25, 2019 at 9:42 am</a></p>
<p> This method does work with multiple faces so perhaps I’m not understanding your question?</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/4e33e2197422c731da797543d3108590?s=48&d=mm&r=g">Jose Fierro</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-526545">July 20, 2019 at 8:08 pm</a></p>
<p>Congrats!!, Great tutorial Master.</p>
<p>I just have a question, each time you add a new person, do need to train again the SVM or exists another way?</p>
<p>Thanks</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-527250">July 25, 2019 at 9:35 am</a></p>
<p> If you add a new person you will need to retrain the SVM.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5348c75918f8f0928d9af5cc4c9c8674?s=48&d=mm&r=g">Amos Cheung</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-527328">July 25, 2019 at 12:26 pm</a></p>
<p>Hi! First of all thanks for the tutorial. I just have one question. Is there anyway to construct the code so that all new faces will be recognized as “unknown” rather than having to add data in there. Thanks</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/5c0984f434f8857cd952f0fa1e34aad0?s=48&d=mm&r=g">Steve</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-569516">November 3, 2019 at 9:56 pm</a></p>
<p> It will already do this. Each image gets converted into an embedding (a bunch of numbers). Each person will have a pattern to their embeddings. If you have enough images, the SVM will pick up on those patterns. Since the “unknown” folder has a ton of random images in it, those embeddings will be all over the place — it won’t have an easy to model pattern. So it should learn, on its own, that if a face is “weird” it should be labelled unknown.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d01c231f9284829b458d6b305bcd4f62?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://www.practicallifereflection.com/">Pawan</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-527408">July 26, 2019 at 4:16 am</a></p>
<p>Hi adrian!! I am a big fan of your work and although it is too late i wish you a happy married life.I was wondering , can we combine your open cv with face recognition tutorial(this tutorial) with the pan-tilt motor based face recognition tutorial and enhance the fps with movidius ncs2 tutorial(on raspberry pi) to make a really fast people identification raspberry pi system which can then be utilized for further projects.I just wanted to know whether it can be done or not and if it can be done, how should i go ahead with it ?I have already applied and made these projects separately in different virtual environments, now i need to somehow integrate it.Thanks for your help in advance.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/8a83f2ab071ca1fd9c5d05ab55345c79?s=48&d=mm&r=g">Bruce Young</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-527609">July 27, 2019 at 3:32 pm</a></p>
<p>Thanks for your great tutorials. 🙂</p>
<p>I had an error while training that several others have had.</p>
<p>“AttributeError: ‘NoneType’ object has no attribute ‘shape’”</p>
<p>My path to the training images folder(s) was fine.</p>
<p>For my case at least, the issue was that I am doing the tutorials on a Linux machine but I collected the images using my Mac and then copied the folders across the network to the Linux machine. That process copies both the resource and data forks of the image files on the Mac as well as the Mac .ds_store file. Many of these files are hidden. Once I made fresh dataset image folders and copied the training images into them using the Linux machine, all was good.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-532776">August 7, 2019 at 1:04 pm</a></p>
<p> Thanks for sharing, Bruce!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/8d91404151760bd70274a67e8b5f64eb?s=48&d=mm&r=g">Fouk</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-528089">July 29, 2019 at 4:30 am</a></p>
<p>how to use face-recognition with gpio in raspi</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-532763">August 7, 2019 at 12:59 pm</a></p>
<p> That exact question is covered inside <a target="_blank" rel="noopener" href="https://pyimagesearch.com/raspberry-pi-for-computer-vision/">Raspberry Pi for Computer Vision.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/38cd39c8e11c0a3f7024dd2a6d775d3e?s=48&d=mm&r=g">Laxmi Kant</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-538205">August 15, 2019 at 2:46 am</a></p>
<p>Hi Adrian,<br>Thank you so much for developing it quick and easy stuff with OpenCV.<br>I am using it on Windows-10 machine, it worked great.<br>Thank you once again for creating it.<br>lax</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-539189">August 16, 2019 at 5:32 am</a></p>
<p> Thanks Laxmi!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5709ea0dc18a6509e9d31806eed7433c?s=48&d=mm&r=g">john</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-540550">August 20, 2019 at 3:22 am</a></p>
<p>Hi, Andrian!</p>
<p>U can help me to assign the picamera to on Jetson Nano for videostream face recognition?<br>The real issue is that I can install “picamera[array]” on my Jetson board.</p>
<p>Thanks!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a4f588f318c8fab8fa1daec1bf4e9338?s=48&d=mm&r=g">Ajmal</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-545037">August 25, 2019 at 4:04 am</a></p>
<p>Hello Adrian! Thanks a lot for these tutorials. Your tutorials have been my first intro to Computer Vision and I have fallen in love with the subject! I’m on my way to do my M.Sc. with a focus in computer vision now, and it all started from your tutorials! 😀</p>
<p>I’m aware this may not be the right place for this question, but wanted to know your take on it regardless:</p>
<p>How well does SVM scale? I tried to do a test with dummy vectors, and the training time seems to scale exponentially. Have you had any experiences in scaling this for large datasets (in the order of tens of thousands of classes perhaps)?</p>
<p>Also, what is your opinion on using Neural Networks for the classification of the embeddings as opposed to k-nn (perhaps with LSH) or SVM for scalability?</p>
<p>Any tips/information or links to resources would help me a lot! Thank you once again for these wonderful tutorials!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/42cc35b843d37962c7e8a7916099ee75?s=48&d=mm&r=g">Pankaj</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-546127">August 31, 2019 at 1:21 pm</a></p>
<p>Hey Adrian. Thank you for this amazing tutorial. Loved it. I’m working on a project where I’m supposed to recognize the faces of the moving people. Like people approaching my front door or maybe people in a locality , given I have the dataset of that locality. I’m supposed to identify unknown person accurately for safety purposes. Can you please help me on this. How can I use this tutorial in doing that .</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-547160">September 5, 2019 at 11:09 am</a></p>
<p> That exact project is covered inside <a target="_blank" rel="noopener" href="https://pyimagesearch.com/raspberry-pi-for-computer-vision/">Raspberry Pi for Computer Vision.</a> I suggest you start there.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/ed1f7d13bcdc64c1ff00f62174d5e769?s=48&d=mm&r=g">Nick Kim</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-548945">September 8, 2019 at 10:40 pm</a></p>
<p>Hi Adrian,</p>
<p>Great post as usual but wondering why SVM is used for classifying rather than a fully connected neural network with softmax activation?</p>
<p>Thanks,</p>
<p>Nick</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-550348">September 12, 2019 at 11:42 am</a></p>
<p> You could create your own separate FC network but it would require more work and additional parameter tuning. It’s easier and more efficient to just use a SVM in this case.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5c3fe2975aadc5f22ed393d31e62c4a8?s=48&d=mm&r=g">Shashi Kiran</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-549844">September 11, 2019 at 4:15 am</a></p>
<p>Hi Adrian, Thanks for sharing such wonderful blogs on OpenCV, DL. Very useful, informative, educational and well presented in layman terms. I have learnt a few things so far thru your articles.</p>
<p>My question : Let’s say I have trained my engine with 20 or more images and some images were not well interpreted by the engine. How would I know that ?</p>
<p>Is there someway the engine will try to digest it and throw it out with some result that says image is 90% good, or 10% good/bad.( It could be image is blurred, image where the eyes are closed, streaks in images, rotation was not aligned and engine could not process, and other things that you mentioned in the “Drawbacks” section of your article – things like that )<br>These lead to bad probability which I can avoid by creating a second set of well crafted images.</p>
<p>Was hoping to hear your opinion on it.<br>I have about 40 images of a single person and yet I get 20 or 30% probability and sometimes wrong names during recognition !! so I am thinking my set is no good – but which image is bad in my set ? I need to be able to identify that so that I can train my engine with a better set of photos.</p>
<p>Again thanks for wonderful articles.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a3111b30d53917cb4ad0d15a38936614?s=48&d=mm&r=g">John</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-551635">September 15, 2019 at 7:35 pm</a></p>
<p>Hi Adrian, thanks for the tutorial. I have a question about processing speed. Is there any way that the forward() function speed can be improved or why does this take the most time? When running this on a Raspberry Pi, it seems to be the bottleneck of the recognition. Makes things especially harder when trying to recognize faces in frames from a live video stream.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-552810">September 19, 2019 at 10:06 am</a></p>
<p> Hey John — take a look at <a target="_blank" rel="noopener" href="https://pyimagesearch.com/raspberry-pi-for-computer-vision/">Raspberry Pi for Computer Vision</a> where I show you how to perform face recognition in real-time on the RPi.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c1bc36fed66d402357a9aa23a9aa9c49?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="https://www.wahanajayagroup.com/">Kontraktor Kolam Renang</a></p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-587381">December 1, 2019 at 9:29 am</a></p>
<p> I also have a question similar to this. And thanks Adrian for the answer.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d2cf8e758c1f9bf09a3e68bea07ff384?s=48&d=mm&r=g">Abhishek Gupte</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-553505">September 22, 2019 at 4:44 pm</a></p>
<p>Hey Adrian,<br>I’m facing a rather weird problem. When I train one of my friend’s face BUT NOT MINE, the program still recognizes my face as my friend’s with an above 50% probability but when I train BOTH OUR FACES, it recognizes my face correctly as an almost equal probability as in the former case. What seems to be the problem?<br>P.S I also tried experimenting with different values of C but to no avail.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/17d4f287e7d456b2493ebeb721d3faf8?s=48&d=mm&r=g">Joe</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-557089">October 2, 2019 at 11:26 pm</a></p>
<p>Hi Adrian,</p>
<p>Can this work with greyscale images? I.e. can cv2.dnn.blobFromImage still be used with non RGB images. Asking this because I want the recognition to not be dependent on lighting (if lighting actually even affects this)</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-557424">October 3, 2019 at 12:16 pm</a></p>
<p> Just stack your grayscale image to form an RGB image representation:</p>
<p> <code>image = np.dstack([gray] * 3)</code></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d2cf8e758c1f9bf09a3e68bea07ff384?s=48&d=mm&r=g">Abhishek Gupte</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-563563">October 9, 2019 at 5:27 pm</a></p>
<p>Hey Adrian,<br>First of all great post, now since you reply to so many people, I won’t take much of your time.<br>I have just one question. Can an image size(resolution, size on disk) disparity between dataset and camera feed or between images in the dataset make a difference to the probability? I get varying like 20% difference whenever I run the webcam-recognizer script at different times, with the same trained face under same light conditions. With stranger faces it varies just as much and so i can’t set an exact threshold of probability. Some of the sizes on disk for the images is 5 Kb whereas some 200 Kb. Also the images coming from the feed each equal 70kb. Please comment as it’s causing the greatest hindrance. So close I am to building a face recognition system yet this gnawing problem.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-563643">October 10, 2019 at 10:10 am</a></p>
<p> The image size on disk doesn’t matter but the image resolution <em>does</em> matter. If your image is too small there won’t be enough information to accurately recognize the face. If the image is too large then there will be too many fine-grained details and it could potentially “confuse” your model. Typically input face recognition resolutions are in the 64×64 to 224×224 pixel range.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/3980fc782797298c6a79526211562850?s=48&d=mm&r=g">Abhishek Gupte</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-563669">October 10, 2019 at 11:42 am</a></p>
<p>  What is basically the difference between the resolutions of your camera feed and dataset(the one containing pics of you and your wife and the unknowns)?</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/2e8c2b2072f002ac31ed3094b837a61b?s=48&d=mm&r=g">Harshit</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-563742">October 11, 2019 at 3:59 am</a></p>
<p>Just One Question——</p>
<p>Why did we make a blob of the face first, could’nt we directly pass the image to the embedder after resizing it to fit the input_size of the first layer to the embedder.</p>
<p>Is it necessary to make the blob??</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-564823">October 17, 2019 at 8:07 am</a></p>
<p> OpenCV requires that we create the blobs when using models loaded using OpenCV’s “dnn” model. We need to blobs in this example:</p>
<ol>
<li>One blob when performing face detection  </li>
<li>We then create separate blobs for each detected face</li>
</ol>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/8acf0136329615c5cc834b792c1b9a80?s=48&d=mm&r=g">manideep</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-564062">October 14, 2019 at 1:53 am</a></p>
<p>Thank you so much bro</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-564807">October 17, 2019 at 7:57 am</a></p>
<p> You are welcome!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/17d4f287e7d456b2493ebeb721d3faf8?s=48&d=mm&r=g">Joe</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-564506">October 15, 2019 at 10:34 pm</a></p>
<p>Hi Adrian,</p>
<p>I am looking to improve the method and am starting with preprocessing of images, specifically face alignment. If face alignment is used to preprocess the images, is there an effect on classifying test images if the face in the image is not completely horizontal (i.e. the eyes lie in the same y-coordinates)?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-564798">October 17, 2019 at 7:50 am</a></p>
<p> It <em>can</em> effect the accuracy of the faces are not aligned. They don’t need to be perfectly aligned, but the more aligned they are, the better.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/17d4f287e7d456b2493ebeb721d3faf8?s=48&d=mm&r=g">Joe</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-564887">October 17, 2019 at 1:17 pm</a></p>
<p>  My question may have been unclear. If the training data is aligned, but the face in the test image is not aligned, is that an issue?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-567126">October 25, 2019 at 10:30 am</a></p>
<p>  The embedding models tend to be pretty robust so it’s not the “end of the world” but if you’re aligning the training data you should also try to align the testing data.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a3111b30d53917cb4ad0d15a38936614?s=48&d=mm&r=g">John</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-564882">October 17, 2019 at 12:51 pm</a></p>
<p>Hello,</p>
<p>For the unknown dataset, is it better to have many pictures of a few people (say 6 different people with 10 pictures each) or as many random people as possible (say 60 different people rather than 6 sets of 10 pictures per person)?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-567127">October 25, 2019 at 10:31 am</a></p>
<p> That really depends on your application. I prefer to have examples of many different people but if you know for a fact there are people you are not interested in recognizing (perhaps coworkers in a work place) then you should consider gathering examples of <em>just</em> those people.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/7893dfdb872cc8d4bd3a0fab8a982764?s=48&d=mm&r=g">Tien</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-565231">October 18, 2019 at 10:47 pm</a></p>
<p>Hi Adrian,<br>Thanks for your tutorial, it helps me so much to start learning deep learning and face recognition. From this tutorial, i try to improve accuracy by using dlib’s embedding model but have a low accuracy. So i just want to ask one question that if i extract embeddings by using dlib and face_recognition with my dataset, i will use dlib and face_recognition again in step #3 to extract embeddings instead of the model used in this tutorial. It’s right?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-567122">October 25, 2019 at 10:28 am</a></p>
<p> Yes, you must use the <em>same</em> face embedding model that was used to extract embeddings from your training data.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d0ec7c58e225d7de85eaf121b25ea4fa?s=48&d=mm&r=g">Safi</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-567654">October 27, 2019 at 4:00 pm</a></p>
<p>Hi <em>Adrian</em>,</p>
<p>I’ve a questions if you have ever had times to wrote a tutorial about detecting faces using IP camera.<br>I’m working on something to detect multiple faces &amp; gender using IP camera (rtsp).</p>
<p>if you do have this tutorial please share the link with down below. thanks I really appreciate your effort.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5c0984f434f8857cd952f0fa1e34aad0?s=48&d=mm&r=g">Steve</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-569518">November 3, 2019 at 10:24 pm</a></p>
<p>If I put a ton of unknown images in the unknown folder, it starts predicting that everyone is unknown. I can fix this by either remove unknowns or by just copy/pasting all the images in my other folders so they’re roughly equal (or having it copy the embeddings multiple times for the same effect). Any thoughts on which is better?</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/3506b4c89abda39cf0956e12883973e1?s=48&d=mm&r=g">Yusra Shaikh</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-573746">November 15, 2019 at 3:54 am</a></p>
<p>Hi Adrian</p>
<p>This tutorial worked perfectly! i was amazed at how easy it was. All thanks to your detailed explanation. I wanted to extend this project to detect intruders, and raise an alert via SMS. Can you help me just a general overview of how this can be done?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-576863">November 21, 2019 at 9:24 am</a></p>
<p> Take a look at <a target="_blank" rel="noopener" href="https://pyimagesearch.com/raspberry-pi-for-computer-vision/">Raspberry Pi for Computer Vision</a> which covers that exact project — detecting intruders and sending an SMS alert.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/64a436a1f4a53de421d20cda470e064e?s=48&d=mm&r=g">Will</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-585843">November 28, 2019 at 5:31 am</a></p>
<p>Hi Adrian, thank you for the amazing tutorial.</p>
<p>I have a question and I would like to hear your oppion.</p>
<p>Do you think the reason of the “unknown” prediction when you was wearing a sunglasses is because OpenFace use eye-aligned for preprocessing the input image, so when we wear sunglasses OpenFace cannot align our face correctly that result in low accuracy?</p>
<p>What I mean hear is that although we add more data of people wearing sunglasses in the dataset, maybe the accuracy would not be improved because the OpenFace algorithm cannot perform eye-aligned.</p>
<p>If yes, how would you overcome this problem?</p>
<p>Again, thank you for the nice post. Have a nice day</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/172d4f00689d259a709b23a4c071028c?s=48&d=mm&r=g">Meg</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-587063">November 30, 2019 at 11:31 am</a></p>
<p>Hiii. Does this project detect faces in real time?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-589881">December 5, 2019 at 10:35 am</a></p>
<p> Yes, it certainly does.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/ae1288eb05c74189dd8c8f9236ddb6a3?s=48&d=mm&r=g">AKBAR HIDAYATULOH</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-591259">December 11, 2019 at 2:55 am</a></p>
<p>hi, thanks for the great tutorials. I want to ask, how can i capture face recognition only one time for detected faces as long as the faces are inside the frame, so the captured faces are not every frame, can you give some advice. thanks</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-591596">December 12, 2019 at 10:06 am</a></p>
<p> Try using <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/">basic centroid tracking.</a> Each bounding box will have a unique ID that you can use to keep track of each face.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/bc4a2e2d6f818701459f0b3d4a43ffcb?s=48&d=mm&r=g">Aonty</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-646991">January 13, 2020 at 12:50 pm</a></p>
<p>How can I use it for attendance system</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-656562">January 16, 2020 at 10:33 am</a></p>
<p> I would suggest you read <a target="_blank" rel="noopener" href="https://pyimagesearch.com/raspberry-pi-for-computer-vision/">Raspberry Pi for Computer Vision</a> which covers how to build a custom attendance system.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/7f17aa344c89591b5e7131de0d959d50?s=48&d=mm&r=g">Aadit</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-659456">January 17, 2020 at 2:38 pm</a></p>
<p>Hello Adrian,<br>Can we use this code in rasberry pi to implement face recognition?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-671610">January 23, 2020 at 9:32 am</a></p>
<p> If you want to perform face recognition on the RPi, <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/25/raspberry-pi-face-recognition/">read this tutorial</a> or my book, <a target="_blank" rel="noopener" href="https://pyimagesearch.com/raspberry-pi-for-computer-vision/">Raspberry Pi for Computer Vision.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/6702095f5a6461768ed0aabd6a05943c?s=48&d=mm&r=g">Vinit Shetye</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-686075">January 29, 2020 at 3:12 am</a></p>
<p>Hey Adrian. Thank you for this amazing tutorial. Loved it. I’m working on a project where I’m supposed to recognize the helmet when riders ride the bike. if helmet not recognize then kill switch off bike not working</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-688992">January 30, 2020 at 8:41 am</a></p>
<p> I would recommend using a deep learning-based object detector for that, such as Faster R-CNN, SSD, or RetinaNet. You can use those models to detect the helmet. I cover them inside <a target="_blank" rel="noopener" href="https://pyimagesearch.com/deep-learning-computer-vision-python-book/">Deep Learning for Computer Vision with Python.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/6e77761fad03a1e454df6815264f8442?s=48&d=mm&r=g">Martin</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-696742">February 3, 2020 at 12:21 pm</a></p>
<p>Hi Adrian<br>is it possible to use a Siamese network to recognize the face?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-703791">February 5, 2020 at 2:02 pm</a></p>
<p> Yes, absolutely. I would suggest you read up on siamese networks, triplet loss, and one-shot learning.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/578843c9c21df64ad2125b6e2f0a20c6?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://.../">faiz____</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-722611">February 12, 2020 at 5:27 am</a></p>
<p>thankyou so much, you are like hero to me.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-725624">February 13, 2020 at 10:58 am</a></p>
<p> Thank you for the kind words.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/f4511fd4e33e8f21cdfa8ca676cf35c2?s=48&d=mm&r=g">Sanju</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-722901">February 12, 2020 at 7:48 am</a></p>
<p>Hi Adrian,</p>
<p>What if we want to include the images which belong to some other person apart from the faces present in the dataset ? Do we have to train the model again to recognize that newly added face??</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/10a8bbb79635e9e760d01b81ba7e321b?s=48&d=mm&r=g">someone</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-728400">February 14, 2020 at 12:14 pm</a></p>
<p>This might be too broad of a question, but: how do I improve the rejection rate of unknown faces?<br>I currently have two faces trained, but, running some video data, other persons come very close to my comfort limit.<br>I have about 30-80 pictures trained for each face, but not aligned, in various lighting environments. They are pretty low res, trained from the same camera that does the recognition (~640-720p).</p>
<p>Perhaps over-training raises the risk of false positives? Should different lighting be trained with a different label? (e.g. IR vs daylight)</p>
<p>Should unknown persons be put into a different folder? Into several different folders? Currently I have no such folder in my training set, just the faces I want to detect.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-746995">February 20, 2020 at 9:42 am</a></p>
<p> If you have enough training data you may want to consider training a siamese network with triplet loss — doing so would likely improve the face recognition accuracy.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d15ec2cd8fac468e2fb54d5b77c95cec?s=48&d=mm&r=g">Satyam</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-728550">February 14, 2020 at 1:06 pm</a></p>
<p>Sir where and how to change the hyperparameters i.e; C value as mentioned in ur post to improve the system.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/288d90d121caeb77304b1ed2ff7adb84?s=48&d=mm&r=g">sinem</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-765982">March 11, 2020 at 8:38 am</a></p>
<p>Hi Adrian 🙂<br>How can I use Python and OpenCV to find facial similarity?</p>
<p>I’ve successfully used OpenCV and Python to extract faces from multiple photographs using Haar Cascades.</p>
<p>I now have a directory of images, all of which are faces of different people.</p>
<p>What I’d like to do is take a sample image, and then see which face it most looks like.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766034">March 11, 2020 at 4:43 pm</a></p>
<p> Yes, but I would recommend you follow <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">this guide on face recognition.</a> Extract the 128-d feature vector for each face and then compute the Euclidean distance between the faces. Faces with smaller distances are considered “more similar”.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/288d90d121caeb77304b1ed2ff7adb84?s=48&d=mm&r=g">sinem</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766253">March 12, 2020 at 6:33 pm</a></p>
<p>  Thank you very much Adrian. Can I do it using the LBPH algorithm, which is the facial recognition algorithm included in OpenCv?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766939">March 19, 2020 at 10:02 am</a></p>
<p>  Yes, but your accuracy won’t be as good as using the deep learning-based face recognition methods covered here.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/24876b2979b22a6e65c5532910d6e9f2?s=48&d=mm&r=g">sepideh</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766350">March 14, 2020 at 5:42 am</a></p>
<p>Hi Adrian<br>Thank you very much for your complete code and description.<br>I wondering to khow , how many face can recognize by this code?<br>and store how many person identity?<br>is this good for a university recognition system?<br>I would be very happy if you could introduce a code or article that could recognition many faces (for university or big company)</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/1e23615ff3bb76e04e57182ce176f7a7?s=48&d=mm&r=g">Phil</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766833">March 18, 2020 at 8:27 pm</a></p>
<p>For those who are using sklearn v.0.22, there was a change in the library recently that yields the error: AttributeError: ‘SVC’ object has no attribute ‘_n_support’</p>
<p>An explanation is found here:<br><a target="_blank" rel="noopener" href="https://github.com/scikit-learn/scikit-learn/issues/15902">https://github.com/scikit-learn/scikit-learn/issues/15902</a></p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/1e23615ff3bb76e04e57182ce176f7a7?s=48&d=mm&r=g">Phil</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766834">March 18, 2020 at 8:28 pm</a></p>
<p> To fix the error, you can either revert to &lt;0.22 or retrain the model and then run the recognize[_video].py</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766918">March 19, 2020 at 9:43 am</a></p>
<p> Thanks for sharing, Phil!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/57d78783fffdd7596755a2ef730f7171?s=48&d=mm&r=g">Venkat Mukthineni</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-767465">March 24, 2020 at 9:14 am</a></p>
<p>Hey Adrian! Great job</p>
<p>I would like to know how to extract the bounding boxes of recognized or unrecognized face from video stream/ image. I want the face in bounding box to get saved in a folder. I would like to extend the project with google reverse image search (on unrecognized faces)</p>
<p>Thank you</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-767646">March 25, 2020 at 1:34 pm</a></p>
<p> That’s absolutely possible, but I get the impression that you may be trying to “run before you walk”. Take the time to learn the basics of OpenCV first, including the “cv2.imwrite” function and basic NumPy array slicing/cropping. <a target="_blank" rel="noopener" href="https://pyimagesearch.com/practical-python-opencv/">Practical Python and OpenCV</a> will help you with just that.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/1bb95587f5169c3b2a69b3e092309778?s=48&d=mm&r=g">Ayoub</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-768266">March 29, 2020 at 5:28 am</a></p>
<p>Hello Adrian , please how can i create an embeddings.pickle and le.pickle for my dataset , thanks !</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-769546">April 1, 2020 at 9:35 am</a></p>
<p> Follow the steps in this tutorial as I show you how to run the Python scripts used to generate those files.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e7723b7db5de3fe3440336123c921ed5?s=48&d=mm&r=g">Yasser</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-768284">March 29, 2020 at 1:25 pm</a></p>
<p>HI…</p>
<p>Are the openface and face_recognition different models? or they work together?<br>I am confused between them</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/23090779df0bfe147c3a96d6ab304cb5?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="https://www.motovegan.co.id/">Kaldu Jamur</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-769416">March 31, 2020 at 10:14 pm</a></p>
<p>Thank you Adrian, this really helped me. Always success ..</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-769525">April 1, 2020 at 9:22 am</a></p>
<p> Thanks Kaldu!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/b08290e8d027c109c801846e91820980?s=48&d=mm&r=g">Barry McQuain</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-769656">April 1, 2020 at 5:04 pm</a></p>
<p>Hi,</p>
<p>Great Post!! Everything works as planned, but I have a few questions:</p>
<ol>
<li><p>I get slightly different percentages compared to your demo (ie, 51.03% v 47.09%). It still works, but my results are not exactly identical using the zipped data and code with no changes. Any idea why that might be? It’s not a big deal, just curious.</p>
</li>
<li><p>I added my seven family members, 12 pictures of each, plus another 10 or so random unknowns to dataset to train and a few of each as the images to test. My results are about 50/50 in terms of identifying the correct person.</p>
</li>
<li><p>When it correctly identifies a person, the highest percentage I see is about 35%, and whether correct or not, I see all of my results in the 15% to 35% range. Is this to be expected? Without my new data, the original tutorial of just you and your wife seemed to give results more in the 40% to 60% range.</p>
</li>
<li><p>After adding the new dataset pics above, I do step #1 to extract the 128D data. Then step #2 to retrain. The retraining appears to happen almost instantly, takes less than 1 second. Is it really re-training? I would have expected the retraining to take longer?</p>
</li>
<li><p>You refer to a process whereby I can “retrain” or “fine-tune” – can you give a little more detail about how I can do that? I would like to create a sample of 30 people in my dataset and retrain on just those 30 (with of course a few random ones too). Is this possible?</p>
</li>
<li><p>I left the original pics of you and your wife in my dataset when I added my seven family members. But now, it doesn’t recognize you (adrian.jpg now only gives me a 17.29% and tells me you are now unknown).</p>
</li>
<li><p>I am a high school teacher, and I would like to show my class these results (and I believe a few are ready to learn this themselves.) I have 30 students. Approx how many training pictures of each do you think I will need? Per the above at the present time, I am only 50% correct in identifying the correct person given a dataset of 10 unique people with 12 pics of each.<br>That isn’t high enough for me to show them. Do you think it is possible, using this tutorial example, that I could get something closer to 95% correct identification of those 30 students (or in my test above, closer to 95% correct identification of my seven family members?</p>
</li>
</ol>
<p>Thanks v much!!</p>
<p>B</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-772538">April 9, 2020 at 9:32 am</a></p>
<p> Hey Barry, I’m happy to help, but as I’ve <a target="_blank" rel="noopener" href="https://pyimagesearch.com/faqs/">mentioned on my FAQs page</a>, kindly keep your comments to one question at a time. I receive 250+ emails per day and 100s of blog post comments. I try my best to get to them all, but multipart questions, especially seven of them, isn’t something I can do. If you want more detailed help <a target="_blank" rel="noopener" href="https://pyimagesearch.com/books-and-courses/">kindly become a customer first</a> and then I can help with these longer questions. I hope you understand that I’m trying to “do the most good” and get to as many people as possible, but I can’t possibly get to everyone with these types of comments.</p>
<p> Feel free to pick one of the above questions you want me to answer though.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d429d37d2c16d200b0158406ddcfe287?s=48&d=mm&r=g">Thabang</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-770766">April 6, 2020 at 10:09 am</a></p>
<p>Hi Adrian. Great tutorial. “deeply” enjoyed it. Thanks.</p>
<p>Is it possible to adapt the code to say; If the person in the frame is recognised then they have access to a room? How can i go about to do that?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-772483">April 9, 2020 at 9:10 am</a></p>
<p> I would suggest looking into “smart locks”. Some have APIs that you can integrate with and could then “unlock” when a face is recognized.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/7101c780da72f6f509d2cfc7171b2501?s=48&d=mm&r=g">erol</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-774664">April 10, 2020 at 1:19 pm</a></p>
<p>hi adrian<br>I am doing a face recognition project with raspberry pi for my school project. The door will be unlocked as the face recognition status. I need to transfer the image taken from the camera to the computer and open and lock the door upon request. How can I make this connection</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-785498">April 16, 2020 at 8:07 am</a></p>
<p> I suggest using <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2019/04/15/live-video-streaming-over-network-with-opencv-and-imagezmq/">ImageZMQ.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d4a406b3b129e421976211e1373f3f4c?s=48&d=mm&r=g">Jestin</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-786277">April 16, 2020 at 3:54 pm</a></p>
<p>Hi Adrian,</p>
<p>Firstly, thanks for all the amazing content!<br>I’d like to understand if there is any specific reason for using OpenCV’s face detector instead of dlib’s?<br>I’m curious since you have used dlib’s face detector in your other blog on training a custom dlib shape predictor.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/b584aa14257c001aaefc221984ab71fa?s=48&d=mm&r=g">Nel</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-786405">April 16, 2020 at 5:04 pm</a></p>
<p>Thanks for the article Adrian,</p>
<p>I am working on a project about face recognition in an uncontrolled environment. So, I need a model to detect unknown person, who I don’t have any pictures of in the dataset. I mean, their picture isn’t even in the unknow folder and my model should label them “unknown”. How should I do that?</p>
<p>Thanks</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5141893eba3d2baa314e61d812e8fae0?s=48&d=mm&r=g">Talpe Loyange</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-787370">April 17, 2020 at 6:46 am</a></p>
<p>Have you used both machine language and deep learning for this project and for what ?Can you explain about that</p>
</li>
</ol>
<h4 id="Comment-section"><a href="#Comment-section" class="headerlink" title="Comment section"></a>Comment section</h4><p>Hey, Adrian Rosebrock here, author and creator of PyImageSearch. While I love hearing from readers, a couple years ago I made the tough decision to no longer offer 1:1 help over blog post comments.</p>
<p>At the time I was receiving 200+ emails per day and another 100+ blog post comments. I simply did not have the time to moderate and respond to them all, and the sheer volume of requests was taking a toll on me.</p>
<p>Instead, my goal is to <em>do the most good</em> for the computer vision, deep learning, and OpenCV community at large by focusing my time on authoring high-quality blog posts, tutorials, and books/courses.</p>
<p><strong>If you need help learning computer vision and deep learning, <a target="_blank" rel="noopener" href="https://pyimagesearch.com/books-and-courses/">I suggest you refer to my full catalog of books and courses</a></strong> — they have helped tens of thousands of developers, students, and researchers <em>just like yourself</em> learn Computer Vision, Deep Learning, and OpenCV.</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/books-and-courses/">Click here to browse my full catalog.</a></p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/Log//" class="article-tag-list-link color4">Log</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2022/02/27/20220227/">Expand all items >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-20220226" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/02/26/20220226/">20220226</a>
    </h1>
  

        
        <a href="/2022/02/26/20220226/" class="archive-article-date">
  	<time datetime="2022-02-25T16:00:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>20220226</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
    

      
<div id="toc" class="toc-article">
  <strong class="toc-title">Contents</strong>
  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Working-directories"><span class="toc-number">1.</span> <span class="toc-text">Working directories</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#URL"><span class="toc-number">2.</span> <span class="toc-text">URL</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Primer-BLAST"><span class="toc-number">3.</span> <span class="toc-text">Primer-BLAST</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Primer-document-organization"><span class="toc-number">4.</span> <span class="toc-text">Primer document organization</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Fontconfig-warning"><span class="toc-number">5.</span> <span class="toc-text">Fontconfig warning</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#How-to-control-window-properties-on-VLC-by-command-line-The-VideoLAN-Forums"><span class="toc-number">6.</span> <span class="toc-text">How to control window properties on VLC by command-line? - The VideoLAN Forums</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Researchers-simulate-behavior-of-living-%E2%80%98minimal-cell%E2%80%99-in-three-dimensions"><span class="toc-number">7.</span> <span class="toc-text">Researchers simulate behavior of living ‘minimal cell’ in three dimensions</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Simultaneous-cross-evaluation-of-heterogeneous-E-coli-datasets-via-mechanistic-simulation"><span class="toc-number">8.</span> <span class="toc-text">Simultaneous cross-evaluation of heterogeneous E. coli datasets via mechanistic simulation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Testing-biochemical-data-by-simulation"><span class="toc-number">8.1.</span> <span class="toc-text">Testing biochemical data by simulation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Structured-Abstract"><span class="toc-number">8.2.</span> <span class="toc-text">Structured Abstract</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#INTRODUCTION"><span class="toc-number">8.2.1.</span> <span class="toc-text">INTRODUCTION</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RATIONALE"><span class="toc-number">8.2.2.</span> <span class="toc-text">RATIONALE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RESULTS"><span class="toc-number">8.2.3.</span> <span class="toc-text">RESULTS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CONCLUSION"><span class="toc-number">8.2.4.</span> <span class="toc-text">CONCLUSION</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">8.3.</span> <span class="toc-text">Abstract</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Counting-protein-molecules-for-single-cell-proteomics"><span class="toc-number">9.</span> <span class="toc-text">Counting protein molecules for single-cell proteomics</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract-1"><span class="toc-number">9.1.</span> <span class="toc-text">Abstract</span></a></li></ol></li></ol>
</div>

        <h1 id="Working-directories"><a href="#Working-directories" class="headerlink" title="Working directories"></a>Working directories</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Work&#x2F;References&#x2F;Tech&#x2F;DNA_Amplification&#x2F;RPA&#x2F;Testing&#x2F;RPA_HSV-1; \</span><br><span class="line">cd $&#123;i&#125;; nemo -n $&#123;i&#125;</span><br><span class="line"># </span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Work&#x2F;References&#x2F;Tech&#x2F;DNA_Amplification&#x2F;RPA&#x2F;Testing&#x2F;RPA_HSV-1&#x2F;Regular_PCR.list # Regular PCR for CD and EF fragments</span><br><span class="line">cat Regular_PCR.list | sed &#39;s&#x2F;^&#x2F;&gt;&#x2F;&#39; | sed &#39;s&#x2F;\t\t&#x2F;\n&#x2F;&#39;</span><br><span class="line">#########################################################################</span><br><span class="line"># Primer check with primerBLAST</span><br><span class="line">#</span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Work&#x2F;References&#x2F;Tech&#x2F;DNA_Amplification&#x2F;RPA&#x2F;Testing&#x2F;RPA_HSV-1; \</span><br><span class="line">cd $&#123;i&#125;; nemo -n $&#123;i&#125;</span><br><span class="line">#</span><br><span class="line">rm *.txt *.csv *.asn</span><br><span class="line">#</span><br><span class="line">Query&#x3D;Primer_Sum.fas</span><br><span class="line">Subject&#x3D;NC_001806_HSV-1.fas</span><br><span class="line">primerBLAST $&#123;Query&#125; $&#123;Subject&#125;</span><br><span class="line">Query&#x3D;Primer_Sum.fas</span><br><span class="line">Subject&#x3D;GU734771_HSV-1_strain_F_CG.fas</span><br><span class="line">primerBLAST $&#123;Query&#125; $&#123;Subject&#125;</span><br><span class="line">#########################################################################</span><br><span class="line"># Health from Samsung</span><br><span class="line"># </span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T_9&#x2F;Samsung_Health; cd $&#123;i&#125;; nemo -n $&#123;i&#125;; i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T_9&#x2F;Samsung_Health&#x2F;Samsung_Health.md; subl $&#123;i&#125;</span><br><span class="line">#########################################################################</span><br><span class="line"># 诗歌网站下载</span><br><span class="line"># </span><br><span class="line">wget -rnp http:&#x2F;&#x2F;www.gushicimingju.com&#x2F;gushi&#x2F;</span><br></pre></td></tr></table></figure>
<h1 id="URL"><a href="#URL" class="headerlink" title="URL"></a>URL</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.google.com/search?q=fundamental+behaviors+emerge+from+simulations+of+a+living+minimal+cell&oq=Fundamental+behaviors+emerge+from+simulations+of+a+living+minimal+cell&aqs=chrome.0.0i512l2.2066j0j7&client=ubuntu&sourceid=chrome&ie=UTF-8">fundamental behaviors emerge from simulations of a living minimal cell - Google Search</a></li>
<li><a target="_blank" rel="noopener" href="https://www.eurekalert.org/news-releases/940215">Researchers simulate behavior of living ‘mini | EurekAlert!</a></li>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0092867421014884">Fundamental behaviors emerge from simulations of a living minimal cell - ScienceDirect</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cell.com/cell/fulltext/S0092-8674(21)01488-4?_returnURL=https://linkinghub.elsevier.com/retrieve/pii/S0092867421014884?showall=true">Fundamental behaviors emerge from simulations of a living minimal cell: Cell</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cell.com/action/showPdf?pii=S0092-8674(21)01488-4">Fundamental behaviors emerge from simulations of a living minimal cell</a></li>
<li><a target="_blank" rel="noopener" href="https://reader.elsevier.com/reader/sd/pii/S0092867421014884?token=864798B0DDCFBBDE34FAB0810F5C0D5920DB7C4D150BF63E2CABF9520E5A380D53F38455386530464BD08AC8864C528A&originRegion=eu-west-1&originCreation=20220226115614">Fundamental behaviors emerge from simulations of a living minimal cell | Elsevier Enhanced Reader</a></li>
<li><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aav3751">Simultaneous cross-evaluation of heterogeneous E. coli datasets via mechanistic simulation</a></li>
<li><a target="_blank" rel="noopener" href="https://www.google.com/search?q=Counting+protein+molecules+for+single-cell+proteomics&oq=Counting+protein+molecules+for+single-cell+proteomics&aqs=chrome..69i57.444j0j7&client=ubuntu&sourceid=chrome&ie=UTF-8">Counting protein molecules for single-cell proteomics - Google Search</a></li>
<li><a target="_blank" rel="noopener" href="https://pubmed.ncbi.nlm.nih.gov/35063071/">Counting protein molecules for single-cell proteomics - PubMed</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/labs/pmc/articles/PMC8855622/#__ffn_sectitle">Counting protein molecules for single-cell proteomics</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cell.com/cell/pdf/S0092-8674(21)01451-3.pdf">Counting protein molecules for single-cell proteomics: Cell</a></li>
<li><a target="_blank" rel="noopener" href="https://makersplace.com/marketplace/">Marketplace</a></li>
<li><a target="_blank" rel="noopener" href="https://makersplace.com/mrlongshanks/gravitational-thinking-field-1-of-1-302997/">Gravitational thinking-field | Rare Digital Artwork | MakersPlace</a></li>
<li><a target="_blank" rel="noopener" href="https://makersplace.com/mrlongshanks/gravitational-thinking-field-1-of-1-302997/">Gravitational thinking-field | Rare Digital Artwork | MakersPlace</a></li>
<li><a target="_blank" rel="noopener" href="http://www.gushicimingju.com/novel/laocanyoujixuji/1076.html">《老残游记续集》第一回　元机旅店传龙语　素壁丹青绘马鸣_老残游记续集全文阅读_国学荟</a></li>
<li><a target="_blank" rel="noopener" href="http://www.gushicimingju.com/search/alls/%E6%9C%B1%E6%B7%91%E7%9C%9F">【朱淑真】的查询结果_国学荟</a></li>
<li><a target="_blank" rel="noopener" href="http://www.gushicimingju.com/shiren/zhushuzhen/shiju/">朱淑真的诗句名言(100句全)_国学荟</a></li>
<li><a target="_blank" rel="noopener" href="http://www.gushicimingju.com/search/gushi/%E6%9C%B1%E6%B7%91%E7%9C%9F/">关于、描写【朱淑真】的古诗(共4首)_国学荟</a></li>
<li><a target="_blank" rel="noopener" href="http://www.gushicimingju.com/search/mingyan/%E6%9C%B1%E6%B7%91%E7%9C%9F/">关于、描写【朱淑真】的诗句和名言(共0句)_国学荟</a></li>
<li><a href="file:///home/ht/Downloads/www.gushicimingju.com/gushi/zhuzi/index.html">关于竹子的古诗(492首)_国学荟</a></li>
</ul>
<h1 id="Primer-BLAST"><a href="#Primer-BLAST" class="headerlink" title="Primer-BLAST"></a>Primer-BLAST</h1><ul>
<li><a target="_blank" rel="noopener" href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-134#additional-information">Primer-BLAST: A tool to design target-specific primers for polymerase chain reaction | BMC Bioinformatics | Full Text</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ncbi.nlm.nih.gov/tools/primer-blast/">Primer designing tool</a></li>
<li><a target="_blank" rel="noopener" href="https://omicstutorials.com/primer-blast-new-primer-design-tool-tutorial/">Primer-BLAST-New Primer Design Tool-Tutorial – Omics tutorials</a></li>
<li><a target="_blank" rel="noopener" href="https://www.biostars.org/p/107922/">Automated Primer-Blast like funcionnality</a></li>
</ul>
<h1 id="Primer-document-organization"><a href="#Primer-document-organization" class="headerlink" title="Primer document organization"></a>Primer document organization</h1><ul>
<li>To be newly located, GU734771_HSV-1_strain_F_CG.gb</li>
<li><a target="_blank" rel="noopener" href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-134">Primer-BLAST: A tool to design target-specific primers for polymerase chain reaction | BMC Bioinformatics | Full Text</a></li>
</ul>
<h1 id="Fontconfig-warning"><a href="#Fontconfig-warning" class="headerlink" title="Fontconfig warning"></a>Fontconfig warning</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.cxybb.com/article/weixin_33810302/92026495">ubuntu技巧_weixin_33810302的博客-程序员宝宝 - 程序员宝宝</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Fontconfig warning: &quot;&#x2F;etc&#x2F;fonts&#x2F;conf.d&#x2F;51-local.conf&quot;, line 90: Having multiple &lt;family&gt; in &lt;alias&gt; isn&#39;t supported and may not work as expected</span><br></pre></td></tr></table></figure>

<h1 id="How-to-control-window-properties-on-VLC-by-command-line-The-VideoLAN-Forums"><a href="#How-to-control-window-properties-on-VLC-by-command-line-The-VideoLAN-Forums" class="headerlink" title="How to control window properties on VLC by command-line? - The VideoLAN Forums"></a><a target="_blank" rel="noopener" href="https://forum.videolan.org/viewtopic.php?t=124021">How to control window properties on VLC by command-line? - The VideoLAN Forums</a></h1><h1 id="Researchers-simulate-behavior-of-living-‘minimal-cell’-in-three-dimensions"><a href="#Researchers-simulate-behavior-of-living-‘minimal-cell’-in-three-dimensions" class="headerlink" title="Researchers simulate behavior of living ‘minimal cell’ in three dimensions"></a>Researchers simulate behavior of living ‘minimal cell’ in three dimensions</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.eurekalert.org/news-releases/940215">Researchers simulate behavior of living ‘mini | EurekAlert!</a></li>
</ul>
<p>Simulations offer insight into fundamental principles of life</p>
<p><a target="_blank" rel="noopener" href="https://www.eurekalert.org/releaseguidelines">Peer-Reviewed Publication</a></p>
<p>UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN, NEWS BUREAU</p>
<p>PrintEmail App</p>
<p> <a target="_blank" rel="noopener" href="https://www.eurekalert.org/multimedia/814360"><img src="https://earimediaprodweb.azurewebsites.net/Api/v1/Multimedia/7c5bab6c-f219-4114-b11b-6263fe9a52f6/Rendition/low-res/Content/Public" alt="Research Team"></a> </p>
<p><strong>IMAGE: WITH THEIR COLLEAGUES, RESEARCHERS, FROM LEFT, GRADUATE STUDENT ZANE THORNBURG, CHEMISTRY PROFESSOR ZAIDA (ZAN) LUTHEY-SCHULTEN AND GRADUATE STUDENTS BENJAMIN GILBERT AND TROY BRIER SUCCESSFULLY SIMULATED A LIVING “MINIMAL CELL.” THE ADVANCE WILL AID IN CREATING COMPUTER MODELS THAT ACCURATELY PREDICT HOW LIVING CELLS WILL BEHAVE WHEN CHANGES ARE MADE TO THEIR GENOMES OR OTHER CHARACTERISTICS.</strong> <a target="_blank" rel="noopener" href="https://www.eurekalert.org/multimedia/814360">view more</a> </p>
<p>CREDIT: PHOTO BY L. BRIAN STAUFFER</p>
<p>CHAMPAIGN, Ill. — Scientists report that they have built a living “minimal cell” with a genome stripped down to its barest essentials – and a computer model of the cell that mirrors its behavior. By refining and testing their model, the scientists say they are developing a system for predicting how changes to the genomes, living conditions or physical characteristics of live cells will alter how they function.</p>
<p>They report their findings in the journal <em>Cell</em>.</p>
<p>Minimal cells have pared-down genomes that carry the genes necessary to replicate their DNA, grow, divide and perform most of the other functions that define life, said <a target="_blank" rel="noopener" href="http://links.illinois.edu/f/a/RXaC6rveFi2wQUtX10ZzjQ~~/AAMFlAA~/RgRjxBW8P0QiaHR0cHM6Ly9jaGVtaXN0cnkuaWxsaW5vaXMuZWR1L3phblcDc3BjQgph4LyQ4WFil-vtUhFkaXlhQGlsbGlub2lzLmVkdVgEAAAABA~~">Zaida (Zan) Luthey-Schulten</a>, a <a target="_blank" rel="noopener" href="http://links.illinois.edu/f/a/RYSYusUpnrRTBkQz5qGMYA~~/AAMFlAA~/RgRjxBW8P0QfaHR0cHM6Ly9jaGVtaXN0cnkuaWxsaW5vaXMuZWR1L1cDc3BjQgph4LyQ4WFil-vtUhFkaXlhQGlsbGlub2lzLmVkdVgEAAAABA~~">chemistry</a> professor at the University of Illinois Urbana-Champaign who led the work with graduate student Zane Thornburg. “What’s new here is that we developed a three-dimensional, fully dynamic kinetic model of a living minimal cell that mimics what goes on in the actual cell,” Luthey-Schulten said.</p>
<p>The simulation maps out the precise location and chemical characteristics of thousands of cellular components in 3D space at an atomic scale. It tracks how long it takes for these molecules to diffuse through the cell and encounter one another, what kinds of chemical reactions occur when they do, and how much energy is required for each step.</p>
<p>To build the minimal cell, scientists at the J. Craig Venter Institute in La Jolla, California, turned to the simplest living cells – the mycoplasmas, a genus of bacteria that parasitize other organisms. In previous studies, the JCVI team built a synthetic genome missing as many nonessential genes as possible and grew the cell in an environment enriched with all the nutrients and factors needed to sustain it. For the new study, the team added back a few genes to improve the cell’s viability. This cell is simpler than any naturally occurring cell, making it easier to model on a computer.</p>
<p>Simulating something as enormous and complex as a living cell relies on data from decades of research, Luthey-Schulten said. To build the computer model, she and her colleagues at Illinois had to account for the physical and chemical characteristics of the cell’s DNA; lipids; amino acids; and gene-transcription, translation and protein-building machinery. They also had to model how each component diffused through the cell, keeping track of the energy required for each step in the cell’s life cycle. NVIDIA graphic processing units were used to perform the simulations.  </p>
<p>“We built a computer model based on what we knew about the minimal cell, and then we ran simulations,” Thornburg said. “And we checked to see if our simulated cell was behaving like the real thing.”</p>
<p>The simulations gave the researchers insight into how the actual cell “balances the demands of its metabolism, genetic processes and growth,” Luthey-Schulten said. For example, the model revealed that the cell used the bulk of its energy to import essential ions and molecules across its cell membrane. This makes sense, Luthey-Schulten said, because mycoplasmas get most of what they need to survive from other organisms.</p>
<p>The simulations also allowed Thornburg to calculate the natural lifespan of messenger RNAs, the genetic blueprints for building proteins. They also revealed a relationship between the rate at which lipids and membrane proteins were synthesized and changes in membrane surface area and cell volume.</p>
<p>“We simulated all of the chemical reactions inside a minimal cell – from its birth until the time it divides two hours later,” Thornburg said. “From this, we get a model that tells us about how the cell behaves and how we can complexify it to change its behavior.”</p>
<p>“We developed a three-dimensional, fully dynamic kinetic model of a living minimal cell,” Luthey-Schulten said. “Our model opens a window on the inner workings of the cell, showing us how all of the components interact and change in response to internal and external cues. This model – and other, more sophisticated models to come – will help us better understand the fundamental principles of life.”</p>
<p>Luthey-Schulten holds the Murchison-Mallory Endowed Chair in Chemistry. She is a professor of <a target="_blank" rel="noopener" href="http://links.illinois.edu/f/a/kgH1Eb3aoGY98L9hAnbzlw~~/AAMFlAA~/RgRjxBW8P0QdaHR0cHM6Ly9waHlzaWNzLmlsbGlub2lzLmVkdS9XA3NwY0IKYeC8kOFhYpfr7VIRZGl5YUBpbGxpbm9pcy5lZHVYBAAAAAQ~">physics</a> and an affiliate of the <a target="_blank" rel="noopener" href="http://links.illinois.edu/f/a/cd_LwkgqHXIhazja8JXi7w~~/AAMFlAA~/RgRjxBW8P0QdaHR0cHM6Ly9iZWNrbWFuLmlsbGlub2lzLmVkdS9XA3NwY0IKYeC8kOFhYpfr7VIRZGl5YUBpbGxpbm9pcy5lZHVYBAAAAAQ~">Beckman Institute for Advanced Science and Technology</a>, the <a target="_blank" rel="noopener" href="http://links.illinois.edu/f/a/9p6XYAk7qI1vUOAKHL4-UA~~/AAMFlAA~/RgRjxBW8P0QdaHR0cHM6Ly93d3cuaWdiLmlsbGlub2lzLmVkdS9XA3NwY0IKYeC8kOFhYpfr7VIRZGl5YUBpbGxpbm9pcy5lZHVYBAAAAAQ~">Carl. R. Woese Institute for Genomic Biology</a>, the <a target="_blank" rel="noopener" href="http://links.illinois.edu/f/a/puo0iOrXk0cdK94oQYfEUQ~~/AAMFlAA~/RgRjxBW8P0QgaHR0cHM6Ly9iaW9waHlzaWNzLmlsbGlub2lzLmVkdS9XA3NwY0IKYeC8kOFhYpfr7VIRZGl5YUBpbGxpbm9pcy5lZHVYBAAAAAQ~">Center for Biophysics and Quantitative Biology</a> and the <a target="_blank" rel="noopener" href="http://links.illinois.edu/f/a/gJpzD4ok_kSul94ljBaBgw~~/AAMFlAA~/RgRjxBW8P0R_aHR0cHM6Ly9iZWNrbWFuLmlsbGlub2lzLmVkdS9yZXNlYXJjaC9tb2xlY3VsYXItc2NpZW5jZS1hbmQtZW5naW5lZXJpbmctcmVzZWFyY2gtdGhlbWUvdGhlb3JldGljYWwtYW5kLWNvbXB1dGF0aW9uYWwtYmlvcGh5c2ljc1cDc3BjQgph4LyQ4WFil-vtUhFkaXlhQGlsbGlub2lzLmVkdVgEAAAABA~~">Theoretical and Computational Biophysics</a> group at the U. of I. She also is the co-director of the National Science Foundation’s <a target="_blank" rel="noopener" href="http://links.illinois.edu/f/a/bJf5udTCO1YBo5IQkvPRBg~~/AAMFlAA~/RgRjxBW8P0QfaHR0cHM6Ly9jcGxjLmlsbGlub2lzLmVkdS9hYm91dFcDc3BjQgph4LyQ4WFil-vtUhFkaXlhQGlsbGlub2lzLmVkdVgEAAAABA~~">Center for the Physics of Living Cells</a> at Illinois.</p>
<p>The National Science Foundation and National Institutes of Health support this research.</p>
<p><strong>Editor’s notes</strong>: </p>
<p>To reach Zaida (Zan) Luthey-Schulten, email <a href="mailto:zan@illinois.edu">zan@illinois.edu</a>.</p>
<p>To reach Zane Thornburg, email <a href="mailto:zanert@illinois.edu">zanert@illinois.edu</a>.</p>
<p>The paper “Fundamental behaviors emerge from simulations of a living minimal cell” is available to members of the media from the <a href="mailto:news@illinois.edu">U. of I. News Bureau.</a></p>
<h1 id="Simultaneous-cross-evaluation-of-heterogeneous-E-coli-datasets-via-mechanistic-simulation"><a href="#Simultaneous-cross-evaluation-of-heterogeneous-E-coli-datasets-via-mechanistic-simulation" class="headerlink" title="Simultaneous cross-evaluation of heterogeneous E. coli datasets via mechanistic simulation"></a>Simultaneous cross-evaluation of heterogeneous <em>E. coli</em> datasets via mechanistic simulation</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aav3751">Simultaneous cross-evaluation of heterogeneous E. coli datasets via mechanistic simulation</a></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aav3751#con1">DEREK N. MACKLIN</a> <a target="_blank" rel="noopener" href="https://orcid.org/0000-0002-7257-0315">HTTPS://ORCID.ORG/0000-0002-7257-0315</a><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aav3751#con2">TRAVIS A. AHN-HORST</a> <a target="_blank" rel="noopener" href="https://orcid.org/0000-0002-3049-3366">HTTPS://ORCID.ORG/0000-0002-3049-3366</a><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aav3751#con3">HEEJO CHOI</a> <a target="_blank" rel="noopener" href="https://orcid.org/0000-0002-2848-0267">HTTPS://ORCID.ORG/0000-0002-2848-0267</a><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aav3751#con4">NICHOLAS A. RUGGERO</a> <a target="_blank" rel="noopener" href="https://orcid.org/0000-0002-5034-9716">HTTPS://ORCID.ORG/0000-0002-5034-9716</a><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aav3751#con5">JAVIER CARRERA</a><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aav3751#con6">JOHN C. MASON</a> <a target="_blank" rel="noopener" href="https://orcid.org/0000-0002-4289-7464">HTTPS://ORCID.ORG/0000-0002-4289-7464</a><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aav3751#con7">GWANGGYU SUN</a> <a target="_blank" rel="noopener" href="https://orcid.org/0000-0002-2649-7807">HTTPS://ORCID.ORG/0000-0002-2649-7807</a><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aav3751#con8">ERAN AGMON</a> <a target="_blank" rel="noopener" href="https://orcid.org/0000-0003-1279-2474">HTTPS://ORCID.ORG/0000-0003-1279-2474</a><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aav3751#con9">MIALY M. DEFELICE</a> <a target="_blank" rel="noopener" href="https://orcid.org/0000-0002-7197-6292">HTTPS://ORCID.ORG/0000-0002-7197-6292</a><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aav3751#">[…]</a><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aav3751#con21">MARKUS W. COVERT</a> <a target="_blank" rel="noopener" href="https://orcid.org/0000-0002-5993-8912">HTTPS://ORCID.ORG/0000-0002-5993-8912</a> +12 authors <a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aav3751#tab-contributors">Authors Info &amp; Affiliations</a></p>
<p>SCIENCE • 24 Jul 2020 • Vol 369, Issue 6502 • <a target="_blank" rel="noopener" href="https://doi.org/10.1126/science.aav3751">DOI: 10.1126/science.aav3751</a></p>
<h2 id="Testing-biochemical-data-by-simulation"><a href="#Testing-biochemical-data-by-simulation" class="headerlink" title="Testing biochemical data by simulation"></a>Testing biochemical data by simulation</h2><p>Can a bacterial cell model vet large datasets from disparate sources? Macklin <em>et al.</em> explored whether a comprehensive mathematical model can be used to verify or find conflicts in massive amounts of data that have been reported for the bacterium <em>Escherichia coli</em>, produced in thousands of papers from hundreds of labs. Although most data were consistent, there were data that could not accommodate known biological results, such as insufficient output of RNA polymerases and ribosomes to produce measured cell-doubling times. Other analyses showed that for some essential proteins, no RNA may be transcribed or translated in a cell’s lifetime, but viability can be maintained without certain enzymes through a pool of stable metabolites produced earlier.</p>
<p><em>Science</em>, this issue p. <a target="_blank" rel="noopener" href="https://doi.org/10.1126/science.aav3751">eaav3751</a></p>
<h2 id="Structured-Abstract"><a href="#Structured-Abstract" class="headerlink" title="Structured Abstract"></a>Structured Abstract</h2><h3 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h3><p>The generation of biological data is presenting us with one of the most demanding analysis challenges the world has ever faced, not only in terms of storage and accessibility, but more critically in terms of its extensive heterogeneity and variability. Although issues associated with heterogeneity and variability each represent major analysis problems on their own, the challenges posed by both in combination are even more difficult but also present greater opportunities. The problems arise because assessing the data’s veracity means not only determining whether the data are reproducible but also, and perhaps more deeply, whether they are cross-consistent, meaning that the interpretations of multiple heterogeneous datasets all point to the same conclusion. The opportunities emerge because seemingly discrepant results across multiple studies and measurement modalities may not be due simply to the errors associated with particular techniques, but also to the complex, nonlinear, and highly interconnected nature of biology. Therefore, what is required are analysis methods that can integrate and evaluate multiple data types simultaneously and in the context of biological mechanisms.</p>
<h3 id="RATIONALE"><a href="#RATIONALE" class="headerlink" title="RATIONALE"></a>RATIONALE</h3><p>Here, we present a large-scale, integrated modeling approach to simultaneously cross-evaluate millions of heterogeneous data against themselves, based on an extensive computer model of <em>Escherichia coli</em> that accounts for the function of 1214 genes (or 43% of the well-annotated genes). The model incorporates an extensive set of diverse measurements compiled from thousands of reports and accounting for many decades of research performed in laboratories around the world. Curation of these data led to the identification of &gt;19,000 parameter values, which we integrated by creating a computational model that brings molecular signaling and regulation of RNA and protein expression together with carbon and energy metabolism in the context of balanced growth. A major advantage of this modeling approach is that heterogeneous data are linked mechanistically through the simulated interaction of cellular processes, providing the most natural, intuitive interpretation of an integrated dataset. Thus, this model enabled us to assess the cross-consistency of all of these datasets as an integrated whole.</p>
<h3 id="RESULTS"><a href="#RESULTS" class="headerlink" title="RESULTS"></a>RESULTS</h3><p>We assessed the cross-consistency of the parameter set and identified areas of inconsistency by populating our model with the literature-derived parameters and by running detailed simulations of cellular life cycles. Although analysis of these simulations showed that most of the data were in fact cross-consistent, we also identified critical areas in which the data incorporated in our model were not. These inconsistencies led to readily observable consequences, including that the total output of the ribosomes and RNA polymerases described by the data are not sufficient for a cell to reproduce measured doubling times, that measured metabolic parameters are neither fully compatible with each other nor with overall growth, and that essential proteins are absent during the cell cycle—and the cell is robust to this absence. After correcting for these inconsistencies, the model is capable of validatable predictions compared with previously withheld data. Finally, considering these data as a whole led to successful predictions in vitro, in this case protein half-lives.</p>
<h3 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h3><p>Construction of a highly integrative and mechanistic mathematical model provided us with an opportunity to integrate and cross-validate a vast, heterogeneous dataset in <em>E. coli</em>, a process we now call “deep curation” to reflect the multiple layers of curation that we perform (analogous to “deep learning” and “deep sequencing”). By highlighting areas in which studies in <em>E. coli</em> contradict each other, our work suggests lines of fruitful experimental inquiry that may help to resolve discrepancies, leading to both new biological insights and a more coherent understanding of this critical model organism. We hope that this work, by demonstrating the value of a large-scale integrative approach to understanding, interpreting, and cross-validating large datasets, will inspire further efforts to comprehensively characterize other organisms of interest.</p>
<p><img src="https://www.science.org/cms/10.1126/science.aav3751/asset/1a183dec-db82-43b2-a155-26a20707a6ef/assets/graphic/369_aav3751_fa.jpeg"></p>
<p><strong>Integrating experimental and computational components, scientists constructed a model of <em>E. coli.</em></strong> Although the model described here resides as software (freely available on GitHub), the model depicted in the photo above is composed of Corning plasticware and filter tips, network cables, and Mac accessories.</p>
<p>Art: Erik Jacobsen; Photo: Bernard Andre</p>
<p><a target="_blank" rel="noopener" href="https://www.science.org/doi/10.1126/science.aav3751#Fa">OPEN IN VIEWER</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>The extensive heterogeneity of biological data poses challenges to analysis and interpretation. Construction of a large-scale mechanistic model of <em>Escherichia coli</em> enabled us to integrate and cross-evaluate a massive, heterogeneous dataset based on measurements reported by various groups over decades. We identified inconsistencies with functional consequences across the data, including that the total output of the ribosomes and RNA polymerases described by data are not sufficient for a cell to reproduce measured doubling times, that measured metabolic parameters are neither fully compatible with each other nor with overall growth, and that essential proteins are absent during the cell cycle—and the cell is robust to this absence. Finally, considering these data as a whole leads to successful predictions of new experimental outcomes, in this case protein half-lives.</p>
<h1 id="Counting-protein-molecules-for-single-cell-proteomics"><a href="#Counting-protein-molecules-for-single-cell-proteomics" class="headerlink" title="Counting protein molecules for single-cell proteomics"></a>Counting protein molecules for single-cell proteomics</h1><ul>
<li><a target="_blank" rel="noopener" href="https://pubmed.ncbi.nlm.nih.gov/35063071/">Counting protein molecules for single-cell proteomics - PubMed</a></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://pubmed.ncbi.nlm.nih.gov/?term=Slavov+N&cauthor_id=35063071">Nikolai Slavov</a> <a target="_blank" rel="noopener" href="https://pubmed.ncbi.nlm.nih.gov/35063071/#affiliation-1" title="Department of Bioengineering, Northeastern University, Boston, Massachusetts 02115, United States; Barnett Institute, Northeastern University, Boston, Massachusetts 02115, United States. Electronic address: nslavov@northeastern.edu.">1</a></p>
<p>Affiliations expand</p>
<ul>
<li>  PMID: <strong>35063071</strong></li>
<li>  PMCID: <a target="_blank" rel="noopener" href="http://www.ncbi.nlm.nih.gov/pmc/articles/pmc8855622/">PMC8855622</a></li>
<li>  DOI: <a target="_blank" rel="noopener" href="https://doi.org/10.1016/j.cell.2021.12.013">10.1016/j.cell.2021.12.013</a></li>
</ul>
<h2 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h2><p>Technologies for counting protein molecules are enabling single-cell proteomics at increasing depth and scale. New advances in single-molecule methods by Brinkerhoff and colleagues promise to further increase the sensitivity of protein analysis and motivate questions about scaling up the counting of the human proteome.</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/Log//" class="article-tag-list-link color4">Log</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2022/02/26/20220226/">Expand all items >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-20220223" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/02/23/20220223/">20220223</a>
    </h1>
  

        
        <a href="/2022/02/23/20220223/" class="archive-article-date">
  	<time datetime="2022-02-22T16:00:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>20220223</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
    

      
<div id="toc" class="toc-article">
  <strong class="toc-title">Contents</strong>
  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Working-directories"><span class="toc-number">1.</span> <span class="toc-text">Working directories</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Remove-silent-part-in-a-audio-document"><span class="toc-number">2.</span> <span class="toc-text">Remove silent part in a audio document</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#How-to-Set-Nemo-as-Default-File-Manager-in-Ubuntu"><span class="toc-number">3.</span> <span class="toc-text">How to Set Nemo as Default File Manager in Ubuntu</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#How-to-Set-Nemo-as-Default-File-Manager-in-Ubuntu-1"><span class="toc-number">4.</span> <span class="toc-text">How to Set Nemo as Default File Manager in Ubuntu</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Install-Nemo-File-Manager-in-Ubuntu-Unity-Desktop"><span class="toc-number">4.1.</span> <span class="toc-text">Install Nemo File Manager in Ubuntu (Unity Desktop)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Install-dconf-tools"><span class="toc-number">4.2.</span> <span class="toc-text">Install dconf-tools</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Set-Nemo-as-Default-File-Manager"><span class="toc-number">4.3.</span> <span class="toc-text">Set Nemo as Default File Manager</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Set-Nautilus-Back-as-the-Default-File-Manager"><span class="toc-number">4.4.</span> <span class="toc-text">Set Nautilus Back as the Default File Manager</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Un-install-Nemo"><span class="toc-number">4.5.</span> <span class="toc-text">Un-install Nemo</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Keyboard-shortcut-in-Nemo-to-open-terminal-in-active-folder-Unix-amp-Linux-Stack-Exchange"><span class="toc-number">4.6.</span> <span class="toc-text">Keyboard shortcut in Nemo to open terminal in active folder - Unix &amp; Linux Stack Exchange</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A2%81%E6%99%93%E5%A3%B0"><span class="toc-number">5.</span> <span class="toc-text">梁晓声</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%AE%E5%9F%8E"><span class="toc-number">5.0.1.</span> <span class="toc-text">浮城</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9B%AA%E5%9F%8E"><span class="toc-number">5.0.2.</span> <span class="toc-text">雪城</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B4%E8%BD%AE"><span class="toc-number">5.0.3.</span> <span class="toc-text">年轮</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E7%BA%A2%E5%8D%AB%E5%85%B5%E7%9A%84%E8%87%AA%E7%99%BD"><span class="toc-number">5.0.4.</span> <span class="toc-text">一个红卫兵的自白</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%AD%E5%9B%BD%E7%94%9F%E5%AD%98%E5%90%AF%E7%A4%BA%E5%BD%95"><span class="toc-number">5.0.5.</span> <span class="toc-text">中国生存启示录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9C%9F%E5%8E%86%E5%8F%B2%E5%9C%A8%E6%B0%91%E9%97%B4"><span class="toc-number">5.0.6.</span> <span class="toc-text">真历史在民间</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AC%B2%E8%AF%B4"><span class="toc-number">5.0.7.</span> <span class="toc-text">欲说</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%83%81%E9%97%B7%E7%9A%84%E4%B8%AD%E5%9B%BD%E4%BA%BA"><span class="toc-number">5.0.8.</span> <span class="toc-text">郁闷的中国人</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%AF%E7%81%AD"><span class="toc-number">5.0.9.</span> <span class="toc-text">泯灭</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%A2%E7%A3%A8%E5%9D%8A"><span class="toc-number">5.0.10.</span> <span class="toc-text">红磨坊</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%AD%E5%9B%BD%E4%BA%BA%EF%BC%8C%E4%BD%A0%E7%BC%BA%E4%BA%86%E4%BB%80%E4%B9%88"><span class="toc-number">5.0.11.</span> <span class="toc-text">中国人，你缺了什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%81%E6%99%93%E5%A3%B0-%E6%88%91%E7%9A%84%E5%A4%A7%E5%AD%A6"><span class="toc-number">5.0.12.</span> <span class="toc-text">梁晓声-我的大学</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%AC%E5%8D%8E%E9%97%BB%E8%A7%81%E5%BD%95"><span class="toc-number">5.0.13.</span> <span class="toc-text">京华闻见录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%8A%E4%BA%BA%E4%BC%8A%E4%BA%BA"><span class="toc-number">5.0.14.</span> <span class="toc-text">伊人伊人</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A8%E5%BC%9F"><span class="toc-number">5.0.15.</span> <span class="toc-text">表弟</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%96%B2%E6%83%AB%E7%9A%84%E4%BA%BA"><span class="toc-number">5.0.16.</span> <span class="toc-text">疲惫的人</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E8%A7%89%E6%97%A5%E6%9C%AC"><span class="toc-number">5.0.17.</span> <span class="toc-text">感觉日本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%BE%E5%B7%B4"><span class="toc-number">5.0.18.</span> <span class="toc-text">尾巴</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A7%E4%B8%8A%E7%9A%84%E8%88%9E%E8%80%85"><span class="toc-number">5.0.19.</span> <span class="toc-text">弧上的舞者</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%8C%E8%80%85%E5%9C%A8%E6%A1%A5%E5%A4%B4"><span class="toc-number">5.0.20.</span> <span class="toc-text">歌者在桥头</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8B%A1%E7%8C%BE%E6%98%AF%E4%B8%80%E7%A7%8D%E5%86%92%E9%99%A9%E7%9A%84%E6%B8%B8%E6%88%8F"><span class="toc-number">5.0.21.</span> <span class="toc-text">狡猾是一种冒险的游戏</span></a></li></ol></li></ol></li></ol>
</div>

        <h1 id="Working-directories"><a href="#Working-directories" class="headerlink" title="Working directories"></a>Working directories</h1><h1 id="Remove-silent-part-in-a-audio-document"><a href="#Remove-silent-part-in-a-audio-document" class="headerlink" title="Remove silent part in a audio document"></a>Remove silent part in a audio document</h1><p><strong>Not confirmed</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://unix.stackexchange.com/questions/124734/how-to-remove-silence-part-from-mp3-that-is-extracted-from-tv-drama">ffmpeg - How to remove silence part from mp3 that is extracted from tv drama - Unix &amp; Linux Stack Exchange</a></li>
<li><a target="_blank" rel="noopener" href="https://myconverters.com/posts/remove-background-noise-from-audio/">5 Ways to Remove Background Noise from Audio 2021</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/25697596/using-ffmpeg-with-silencedetect-to-remove-audio-silence#:~:text=Use%20the%20silenceremove%20filter.%20This%20removes%20silence%20from,-i%20input.mp3%20-af%20silenceremove%3D1%3A0%3A-50dB%20output.mp3%20This%20removes%20silence">using FFMPEG with silencedetect to remove audio silence - Stack Overflow</a></li>
</ul>
<p>Use the <code>silenceremove</code> filter. <strong>This removes silence from the audio track only - it will leave the video unedited, i.e., things will go out of sync</strong></p>
<p>Its arguments are a little cryptic.</p>
<p>An example</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -i input.mp3 -af silenceremove&#x3D;1:0:-50dB output.mp3</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>This removes silence</p>
<ul>
<li>  at the beginning (indicated by the first argument <code>1</code>)</li>
<li>  with minimum length zero (indicated by the second argument <code>0</code>)</li>
<li>  silence is classified as anything under -50 decibels (indicated by <code>50dB</code>).</li>
</ul>
<p>Documentation: <a target="_blank" rel="noopener" href="https://ffmpeg.org/ffmpeg-filters.html#silenceremove">FFMPEG silence remove filter</a></p>
<p>Also anyone looking to find the right value to classify silence as may wish to look into normalising their input audio volume to <code>0dB</code> first, to do this in ffmpeg see <a target="_blank" rel="noopener" href="https://superuser.com/questions/323119/how-can-i-normalize-audio-using-ffmpeg">this answer</a>.</p>
<p><strong>Edit</strong></p>
<p>As pointed out by @mems, to detect whether your version of ffmpeg has the filter run</p>
<p><code>ffmpeg -hide_banner -filters | grep silenceremove</code></p>
<p>if you have the filter it’ll output something like</p>
<p><code>silenceremove A-&gt;A Remove silence</code></p>
<p>After reading the FFmpeg silenceremove <a target="_blank" rel="noopener" href="https://ffmpeg.org/ffmpeg-filters.html#silenceremove">documentation</a>, this is how you remove silence at the beginning and end of an audio file (keeps silence in the middle).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -i &quot;INPUT.mp3&quot; -af silenceremove&#x3D;start_periods&#x3D;1:stop_periods&#x3D;1:detection&#x3D;peak &quot;OUTPUT.mp3&quot;</span><br></pre></td></tr></table></figure>

<h1 id="How-to-Set-Nemo-as-Default-File-Manager-in-Ubuntu"><a href="#How-to-Set-Nemo-as-Default-File-Manager-in-Ubuntu" class="headerlink" title="How to Set Nemo as Default File Manager in Ubuntu"></a>How to Set Nemo as Default File Manager in Ubuntu</h1><p><strong>Done!</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://sourcedigit.com/13826-set-nemo-default-file-manager-ubuntu/">How to Set Nemo as Default File Manager in Ubuntu</a></li>
</ul>
<p>By Sourabh / June 26, 2016Category: <a target="_blank" rel="noopener" href="https://sourcedigit.com/how-to/">How To</a>, <a target="_blank" rel="noopener" href="https://sourcedigit.com/ubuntu/">Ubuntu</a></p>
<p>How to set Nemo as default file manager in Ubuntu. Change default filemanager to Nemo and make Nemo the default file browser in Ubuntu. I assume you have installed the Nemo File Manager. If not, you can follow the link given below:</p>
<h1 id="How-to-Set-Nemo-as-Default-File-Manager-in-Ubuntu-1"><a href="#How-to-Set-Nemo-as-Default-File-Manager-in-Ubuntu-1" class="headerlink" title="How to Set Nemo as Default File Manager in Ubuntu"></a>How to Set Nemo as Default File Manager in Ubuntu</h1><ul>
<li><a target="_blank" rel="noopener" href="https://sourcedigit.com/13826-set-nemo-default-file-manager-ubuntu/">How to Set Nemo as Default File Manager in Ubuntu</a></li>
</ul>
<p>By Sourabh / June 26, 2016Category: <a target="_blank" rel="noopener" href="https://sourcedigit.com/how-to/">How To</a>, <a target="_blank" rel="noopener" href="https://sourcedigit.com/ubuntu/">Ubuntu</a></p>
<p>How to set Nemo as default file manager in Ubuntu. Change default filemanager to Nemo and make Nemo the default file browser in Ubuntu. I assume you have installed the Nemo File Manager. If not, you can follow the link given below:</p>
<h2 id="Install-Nemo-File-Manager-in-Ubuntu-Unity-Desktop"><a href="#Install-Nemo-File-Manager-in-Ubuntu-Unity-Desktop" class="headerlink" title="Install Nemo File Manager in Ubuntu (Unity Desktop)"></a><a target="_blank" rel="noopener" href="https://sourcedigit.com/19218-install-the-latest-nemo-file-manager-on-ubuntu-via-ppa/">Install Nemo File Manager in Ubuntu (Unity Desktop)</a></h2><p>You must stop Nautilus from handling the desktop icons and use Nemo file manager. To do so, use the following commands:</p>
<h2 id="Install-dconf-tools"><a href="#Install-dconf-tools" class="headerlink" title="Install dconf-tools"></a>Install dconf-tools</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install dconf-tools  </span><br><span class="line">gsettings set org.gnome.desktop.background show-desktop-icons false  </span><br></pre></td></tr></table></figure>
<p>Nemo is enabled by default to draw the desktop icons. Simply, start Nemo or restart the session (log out and log in).</p>
<h2 id="Set-Nemo-as-Default-File-Manager"><a href="#Set-Nemo-as-Default-File-Manager" class="headerlink" title="Set Nemo as Default File Manager"></a>Set Nemo as Default File Manager</h2><p>To set Nemo as the default file manager and replacing Nautilus, run the following command:  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xdg-mime default nemo.desktop inode&#x2F;directory application&#x2F;x-gnome-saved-search  </span><br></pre></td></tr></table></figure>
<h2 id="Set-Nautilus-Back-as-the-Default-File-Manager"><a href="#Set-Nautilus-Back-as-the-Default-File-Manager" class="headerlink" title="Set Nautilus Back as the Default File Manager"></a>Set Nautilus Back as the Default File Manager</h2><p>If you want to set Nautilus back as the default file manager and replace Nemo, run the following commands:  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gsettings set org.gnome.desktop.background show-desktop-icons true  </span><br><span class="line">xdg-mime default nautilus.desktop inode&#x2F;directory application&#x2F;x-gnome-saved-search  </span><br></pre></td></tr></table></figure>

<p>The above set of commands will set Nautilus as the default file manager.</p>
<h2 id="Un-install-Nemo"><a href="#Un-install-Nemo" class="headerlink" title="Un-install Nemo"></a>Un-install Nemo</h2><p>If you wantto un-install and remove Nemo, run the following command:  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove nemo nemo-*  </span><br><span class="line">sudo rm &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;webupd8team-nemo-*.list  </span><br></pre></td></tr></table></figure>
<p>Nemo is the official file manager for the Linux Mint OS running the Cinnamon desktop.</p>
<h2 id="Keyboard-shortcut-in-Nemo-to-open-terminal-in-active-folder-Unix-amp-Linux-Stack-Exchange"><a href="#Keyboard-shortcut-in-Nemo-to-open-terminal-in-active-folder-Unix-amp-Linux-Stack-Exchange" class="headerlink" title="Keyboard shortcut in Nemo to open terminal in active folder - Unix &amp; Linux Stack Exchange"></a><a target="_blank" rel="noopener" href="https://unix.stackexchange.com/questions/253211/keyboard-shortcut-in-nemo-to-open-terminal-in-active-folder">Keyboard shortcut in Nemo to open terminal in active folder - Unix &amp; Linux Stack Exchange</a></h2><ul>
<li><p>Create the file ~/.gnome2/accels/nemo:</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir -p ~&#x2F;.gnome2&#x2F;accels</span><br><span class="line">$ touch ~&#x2F;.gnome2&#x2F;accels&#x2F;nemo</span><br></pre></td></tr></table></figure>
</li>
<li><p>  Then add the following line in that file (replace “F4” with whatever shortcut you want to use):</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(gtk_accel_path &quot;&lt;Actions&gt;&#x2F;DirViewActions&#x2F;OpenInTerminal&quot; &quot;F4&quot;)</span><br></pre></td></tr></table></figure>
<p>In Nemo, a terminal is just part of the package! No funny shortcuts needed! <strong>:-)</strong></p>
<p><a target="_blank" rel="noopener" href="https://i.stack.imgur.com/Xhyup.png"><img src="https://i.stack.imgur.com/Xhyup.png" alt="Nemo with terminal active"></a></p>
<p>Just ensure the plug-in is installed:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install nemo-terminal</span><br></pre></td></tr></table></figure>
<p>Which will ensure you have a terminal ready at all times.</p>
<p><strong>If you want additional terminals on top of the standard one,</strong> just right-click any directory and choose the menu-item Open in Terminal and it’ll open an additional terminal. Continue doing this for more terminal windows. <strong>;-)</strong></p>
<h1 id="梁晓声"><a href="#梁晓声" class="headerlink" title="梁晓声"></a>梁晓声</h1><p><img src="https://www.shutxt.com/d/file/p/b87df04ecc5b4ddc430a8a22943ab6ec.jpg" alt="f9198618367adab4e51474c88ed4b31c8601e471.jpg">梁晓声，原名梁绍生。当代著名作家。1949年9月22日出生于哈尔滨市，祖籍山东荣成市泊于镇温泉寨。中国作家协会会员。曾创作出版过大量有影响的小说、散文、随笔及影视作品。中国现当代以知青文学成名的代表作家之一。现居北京，任教于北京语言大学人文学院汉语言文学专业。</p>
<p>1968年到1975年曾在黑龙江生产建设兵团第一师劳动。1977年任北京电影制片厂编辑、编剧，1988年调至中国儿童电影制厂任艺术委员会副主任，中国电影审查委员会委员及中国电影进口审查委员会委员。2002年开始任北京语言大学中文系教授。</p>
<p>2009年8月参加了人民日报总编室和人民网联合举办的文化讲坛。就“透视当代文化生态”进行了主题演讲：“我们有8亿多农民，这三分之二中国人的生活是与‘发达国家’四个字形成巨大反差的。我觉得我们还应该低调一些，我们还是发展中国家，甚至相当长的时期内，我们可能都依然是发展中国家。” </p>
<p>2011年10月出席了历史小说《天下知音——欧阳修》在杭州举行研讨会。 </p>
<p>2013年1月出席了由搜狐网主办的“向教育提问”搜狐教育年度盛典。 </p>
<p>2014年10月15日参加了由习近平总书记主持召开的文艺工作座谈会。 </p>
<p>2015年初出版了《我们的时代与社会》、《我相信中国的未来》两本书。 </p>
<p>他的创作风格归纳为：现实主义的英雄化风格，现实主义的平民化风格，现实主义的寓言化风格。</p>
<p>梁晓声的知青小说《知青》表现了悲壮的英雄主义和理想主义的特点，充满激情是这个阶段的创作风格。他的的小说《知青》展示了特殊年代的年轻人特有的痛苦中的困惑、困惑中的思索、思索后对自身及过去经历的肯定，梁晓声因此也是“青春无悔”型知青文学的典型代表。</p>
<p>当中国走上现代化建设之路，社会开始转型的时候，精英文学逐渐走向边缘，世俗化的文学日益兴起，梁晓声的作品在关注知青这一群体的生存状态的同时，开始将目光投向了社会最底层的平民生活。与其他作家寻找平民身上的劣根性不同，梁晓声更多的是从平民立场出发，去描写平民的日常生活，表现他们在艰苦的社会环境中所具有的正直和善良本性，将人们心灵光辉的一面展现出来。在《人间烟火》、《父亲》中，表现出的是梁晓声平和的平民化创作风格。</p>
<p>伴随着市场经济体制的引入，社会经济迅速发展，商品大潮冲击着固有的价值观。而梁晓声则以他的社会责任感，对人民灼热的情感，始终保持着冷静的头脑，依然勤奋地在文学园地里笔耕不辍，接连创作出大量紧扣时代、引起社会广泛关注与深刻反响的小说、散文等艺术形式丰富多样的作品。《浮城》这部作品犹可感受到梁晓声本人的内在精神品格的提炼与追求、脉搏的热烈跳动，这时的梁晓声运用寓言式的描写对社会及人性的丑恶进行无情的剖析，表现出了冷峻的创作风格。 </p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/19509/"><img src="https://www.shutxt.com/d/file/xiandai/2b94ce09e529fbd889fbf78d436250f4.jpg"></a></p>
<h3 id="浮城"><a href="#浮城" class="headerlink" title="浮城"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/19509/">浮城</a></h3><p>简介： 一座沿海城市骤然脱离了陆地，在海上越漂越远，成为一座浮城。面对突如其来的世界末日，人们手足无措。为了抵抗无数的海鸥，占据浮城，大家倾巢出动，尽管死伤无数，最终却保住了自己的根据地。事后，众人变得近乎疯狂，彼此之间相互猜疑，为了延续自己的生命，不惜残害同胞、杀人抢劫以夺取救生圈、水源和食物。当眼前的希望一次又一次破灭，绝望的人们选择跳海结束自己的生命。只有那些依然沉浸在美好的幻想中的人们，继续随浮城漫无目的地漂浮。最终，浮城载着这些执着的人们，消失在大海深处。作者通过描述浮城中发生的各种事件，揭露了现世人生的悲剧，批判了人性的</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/9243/"><img src="https://www.shutxt.com/d/file/pinglun/998a4f1c1787462e0b915943194c76f8.jpg"></a></p>
<h3 id="雪城"><a href="#雪城" class="headerlink" title="雪城"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/9243/">雪城</a></h3><p>简介：《雪城》是梁晓声著的一部经典长篇小说。《雪城》曾被改编成电视连续剧热映。 她，独身穿入另外的时空，却像凤凰浴火重生，给了她一种无比充实的生活，善良，帮助她一路走来，带给别人希望，于是，便在这个异世界，留下了一个见证的地方――雪城。 《雪城》以电视连续剧的形式通过大众媒体，走向了千家万户，尤其是刘欢激情演唱的主题歌更是传遍大江南北，使梁晓声的声誉达到了顶峰。 本书是中国当代名家长篇小说代表作之一，也是梁晓声的长篇代表作之一。梁晓声的小说作品以知青题材为主，有些人称之为北大荒小说，代表作包括《那是一片神奇的土地》、《今夜有暴风雪》</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1952/"><img src="https://www.shutxt.com/d/file/waiwen/c18a11d6411d59961d5acbe28476abef.jpg"></a></p>
<h3 id="年轮"><a href="#年轮" class="headerlink" title="年轮"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1952/">年轮</a></h3><p>简介：本书以共和国同龄人与祖国共同走过的历程为背景，以曾在北大荒上山下乡的六位男女知青的人生道路为主线，展示了中国社会近几十年所经历的坎坷和风风雨雨，勾划出共和国的年轮。 书中的主人公经历了二十世纪六十年代的自然灾害，过早地尝到了生活的艰辛；轰轰烈烈的文化大革命，使他们激扬过也失落过；神奇的北大荒曾使他们热血沸腾，也令他们迷茫无奈。他们有中国传统的家庭亲情，又有比亲情更高、为朋友义不容辞的友情，还有阴差阳错、充满了悲剧色彩的爱情。当改革大潮席卷大地的时候，他们人生最美好的时光已经逝去。但是，他们凭着坚忍不拔的意志，顽强地与命运</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/rw/1103/"><img src="https://www.shutxt.com/d/file/dangdai/78fb8c3062cd7ab3e0af9104565a2566.jpg"></a></p>
<h3 id="一个红卫兵的自白"><a href="#一个红卫兵的自白" class="headerlink" title="一个红卫兵的自白"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/rw/1103/">一个红卫兵的自白</a></h3><p>简介：本书是一部纪实性的长篇小说，真实再现了“文章”那个令人伤心、痛苦而又荒唐的特殊年代，表现了作者在特定情境中对人的灵魂的自我叩问。七户大杂院，四月十六日的《北京日报》，批判“三家村”，站毛主席一边，人民群众，人变“鬼”，红五类、红袖标、红卫兵，大黄楼批斗，批斗父亲，三十元钱，美丽的囚徒，卧轨，进京火车，到了北京，见到毛主席，离开四川，张珊和姚舞，我回家了，家非家，“炮轰派”灭亡。</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/rw/9242/"><img src="https://www.shutxt.com/d/file/pinglun/e18780e813238f5db5271e37fd9c7a88.jpg"></a></p>
<h3 id="中国生存启示录"><a href="#中国生存启示录" class="headerlink" title="中国生存启示录"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/rw/9242/">中国生存启示录</a></h3><p>简介：这是一本面对现实如何生存的启示录，一本发现人生智慧、追寻幸福生活的指南书。是中国第一部以培育健康价值观为专题的心灵励志书。作家、教授梁晓声把他用心灵发展出的生存智慧反哺到精神境界中，再用他从社会生活中发展出的知识浸透到心灵里去，为那些生活在社会各阶层的人们，寻找一份心灵寄托与精神生活的充实，以及能够解决一些现实问题的方法。他以平常心去思考人生和社会中的常态及潜在的问题，用从容的心态做一些新的尝试和突破，力图为大众的现实生活寻找借鉴，在现实与精神的总作用下，告诉我们他自己的生活感受，启迪人们面对各种各样的现实应该拥有</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/ls/9240/"><img src="https://www.shutxt.com/d/file/pinglun/331a1d1987d94b130f349489adcde755.jpg"></a></p>
<h3 id="真历史在民间"><a href="#真历史在民间" class="headerlink" title="真历史在民间"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/ls/9240/">真历史在民间</a></h3><p>简介：《真历史在民间》是作家梁晓声第一本以民间视角解读国家历史的著作！讲述历史的同时针砭当下 这是一部讲述历史同时针砭当下的时政作品。上半部分以时间为序，梁晓声回忆这个半个多世纪自己亲身参与或见证的三反五反、大跃进、文革、80年代后期等鲜为人知的敏感历史，以散文的笔法真实记录半个世纪名人高干、大小官员、各色商人、各等物的命运，用一个个在那个极端年代里的人物经历拼凑出来这个年代的大环境，他们是这个时代的政治牺牲品。融叙述与议论为一体，以个人命运反应大时代特征。以故事的方式剖析当今的中国现实，解读大国成长道路上的坎坷，从中</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1107/"><img src="https://www.shutxt.com/d/file/dangdai/d8ffc312f87fecdd8b7ff70141a533ae.jpg"></a></p>
<h3 id="欲说"><a href="#欲说" class="headerlink" title="欲说"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1107/">欲说</a></h3><p>简介：欲望时代的理想主义写作：一部中国当代的《红与黑》。一个发生在24小时内的惊心动魄的反腐故事。除夕之夜，调任北方某省省委书记的刘思毅欣然飞回南方过年；几个小时后，该省的顺安市爆发了史无前例的大骚乱；民营企业家王启兆和省委副书记赵慧芝神秘失踪。其后的一天，刘思毅发现诸多事件的背后居然是一张权力与金钱交织的错综复杂的网……官与商的灰色博弈，情与法的两难抉择，都在生活流的实时叙事中饱满呈现，小说深刻地展示了一幅社会转型时期政治原生态的浮世绘。 一个发生在24小时内的惊心动魄的反腐故事。除夕之夜，调任北方某省省委书记的刘思毅欣然</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/rw/9241/"><img src="https://www.shutxt.com/d/file/pinglun/32358f62cdc464f6f251b48804d21541.jpg"></a></p>
<h3 id="郁闷的中国人"><a href="#郁闷的中国人" class="headerlink" title="郁闷的中国人"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/rw/9241/">郁闷的中国人</a></h3><p>简介：中国人的郁闷由来已久。 1949年之前，中国人生活在水深火热之中，郁闷自不必说；1949年后，人们迎来了新的时代。然而，好景不长，文革那个险象环生、危机四伏的10年让人们着实郁闷；轰轰烈烈的社会主义建设也没少折腾，社会转型时期的阵痛折磨着中国人坚强的神经。 现如今人们不愁吃喝了，但不知何时起，苏丹红、牛肉膏、瘦肉精、染色馒头、硫磺姜出现了；学校不包分配了，找工作也要权钱交易；入学托关系，住院托关系；豆腐渣工程频繁出现了，矿难接二连三，瞒报也接二连三；物价飞涨了，买房买不起，租房也只能蜗居了解决了温饱的中国人，又开始郁闷了。 梁晓声用坦率的笔调、睿智</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1101/"><img src="https://www.shutxt.com/d/file/dangdai/aca9fdf5cbc8c160822b77b71aca8323.jpg"></a></p>
<h3 id="泯灭"><a href="#泯灭" class="headerlink" title="泯灭"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1101/">泯灭</a></h3><p>简介：这两个共和国的同龄人，从不曾是共和国的任何方面的宠儿，甚至从不曾被关注过，他们只不过是中国草根阶层的两个儿子。他们的成长过程，倒确乎是新中国一味强化信仰的历史阶段。那一段历史可日之为“红色政治”。对于宠儿，自然或会成为信仰。一切言行，对错唯以是否符合“红色政治”的准绳为原则。这样的人在从前的中国比比皆是，但子卿和晓声却都不是那样的人，“红色政治”只不过染红了他们的发肤而已。事实上，他们对于那“红色”，还每每地心生怀疑和不安。</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1104/"><img src="https://www.shutxt.com/d/file/dangdai/00f3c6f5ac2bb6fe4f1f855aae744588.jpg"></a></p>
<h3 id="红磨坊"><a href="#红磨坊" class="headerlink" title="红磨坊"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1104/">红磨坊</a></h3><p>简介：九月的夜风已经使人感到有些凉了，像刚饮过满满一瓶冰镇矿泉水的嘴，闹着玩儿似的，迎面朝素徐徐地吹气。这是秋天偎向北京的最初的迹象，一年四季二十四个节气间的交替，差不多总在夜里进行，而在白天呈现端倪。素是最后一批离开图书馆的人之一。校园完全的岑寂下来了。两幢六层的学生宿舍楼的窗子几乎全黑了，还亮着的是走廊灯和厕所灯。在那两幢楼里并没有素的一张床位。因为她去年已从这所大学毕业了。当时谋不到职业。人类早已度过了思想成熟期，因而哲学仿佛变得毫无意义了。偏偏，素读的正是哲学。这是她人生抉择的第一次失误，一次重大失误。</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/rw/9246/"><img src="https://www.shutxt.com/d/file/pinglun/b7476e7fb228c4274497fd6c0a1dfeff.jpg"></a></p>
<h3 id="中国人，你缺了什么"><a href="#中国人，你缺了什么" class="headerlink" title="中国人，你缺了什么"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/rw/9246/">中国人，你缺了什么</a></h3><p>简介：《中国人，你缺了什么》收录了梁晓声1993到2013年二十年间的时评杂文，谈及涉猎时评写作的原因，他表示传统作家应该是针对社会问题写作的，这本文集正体现出他关心社会民生的一面。梁晓声形容自己手握两支笔，关注现实的虚构类文字给喜欢读小说的人看，而非虚构类时评可以直接发表在报纸、网络等多个媒介上，能够影响更多人。Www.XIAbOOk．com</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/zj/1097/"><img src="https://www.shutxt.com/d/file/dangdai/f03977c21112a0eb8f66369dc5ac8431.jpg"></a></p>
<h3 id="梁晓声-我的大学"><a href="#梁晓声-我的大学" class="headerlink" title="梁晓声-我的大学"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/zj/1097/">梁晓声-我的大学</a></h3><p>简介：一九七四年九月二十七日，下午二三点钟，哈尔滨至上海的一趟火车进站。一个其貌不扬的年轻人被人流裹着，步子虚浮地出了上海站。上海很热，三十四五度左右。这年轻人穿件卡叽布的、旧的、在洗染店染过的、黑色而又变灰了的学生制服。一条崭新的、裤线笔直的“的卡”裤子，蓝色的，太长，折起一寸有余。一双半新的网球鞋。头戴一顶崭新单帽。</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/zj/1098/"><img src="https://www.shutxt.com/d/file/dangdai/a710af802f2ff7dfcfccee05bc94fe19.jpg"></a></p>
<h3 id="京华闻见录"><a href="#京华闻见录" class="headerlink" title="京华闻见录"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/zj/1098/">京华闻见录</a></h3><p>简介：一九七七年九月我从复旦大学毕业，分配到北京。报到前有半个月假。三年没探家，很想家，想母亲。但我打算分配单位确定了，工作几个月后再探家。我非常希望尽早知道我的工作单位将是何处，非常希望尽早对这个单位产生感情。</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1106/"><img src="https://www.shutxt.com/d/file/dangdai/43e2c9ac5e2d49ba1ec41f843ee1faf2.jpg"></a></p>
<h3 id="伊人伊人"><a href="#伊人伊人" class="headerlink" title="伊人伊人"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1106/">伊人伊人</a></h3><p>简介：小说以现代城市一处有名的“伊人酒吧”为开场背景，讲述了一个从上世纪五十年代至今三代人的情爱故事。富有音乐才华的乔祺和风姿绰约的酒吧“老板娘”秦岑之间的情爱纠葛；他们俩人共同经营着三家连锁酒吧，生意一度红火；他们的爱情与婚姻却游移不定……可就在2004年除夕的雪夜，当他们正要谈婚论嫁的时候，乔祺从小抚养并深爱的“小妹妹”乔乔突然出现，从而演泽着一幕幕感人至深的爱的故事……乔祺和秦岑共同经营有名的“伊人酒吧”，享受着金钱、爱情、性带来的多重愉悦。2004年除夕的雪夜，乔祺正准备向秦岑求婚时，酒吧里来了个乖张邪性的“小妖精”，当着</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/9244/"><img src="https://www.shutxt.com/d/file/pinglun/21b85cd4aafd59e5c5dc512b8081e4c1.jpg"></a></p>
<h3 id="表弟"><a href="#表弟" class="headerlink" title="表弟"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/9244/">表弟</a></h3><p>简介：《表弟》获中篇小说选刊奖。性格和命运定律的诠释，是了解当代大学生生活与学习的一个窗口。Www.ＸIAboＯk.com</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1100/"><img src="https://www.shutxt.com/d/file/dangdai/05844c404b979d5f9d2ce0d9507ac616.jpg"></a></p>
<h3 id="疲惫的人"><a href="#疲惫的人" class="headerlink" title="疲惫的人"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1100/">疲惫的人</a></h3><p>简介：王君生和妻子的关系谈不上恩爱，但是他和她也都不愿承认不恩爱。那是一种似是无情似有情的夫妻关系。大部分时间里，也就是每星期从星期一到星期五“似无情”；星期六深夜，儿子睡实了，他蹑悄地转移到妻子那张床上以后，有那么一个来小时夫妻之间“似有情”，如果某星期这一个来小时内没实质性的“活动内容”，那么第二天连同其后的六天，妻子必将对他更加显得“似无情”。不但“似无情”，还仿佛内心里忍受着特大的委屈。所以他一向很重视星期六深夜那一个来小时的同床机会，并且尽量向妻子奉献比上一次多点儿的温柔。不消说：妻子的回报一般总要比他的奉献质量高</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/23183/"><img src="https://www.shutxt.com/d/file/xiandai/5bce6d4bd797e8515fa55e9a6b28686c.jpg"></a></p>
<h3 id="感觉日本"><a href="#感觉日本" class="headerlink" title="感觉日本"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/23183/">感觉日本</a></h3><p>简介：我先请他站到走廊里，替他前前后后上上下下一通扫。扫尽他身上的灰土，又兑了盆热水，带着毛巾香皂，请他到筒子楼的公共洗脸池那儿洗把热水脸。他脸上灰土太多。几把脸洗过，水已浑了。他的目光便望向我拎在手中的暖水瓶。心中有请求又不便开口。我看出了他的意思，又替他兑了一大盆热水。他这才得以将他的脸洗得干乾净净，一边从内衣兜里掏出柄小梳子梳他那被风刮得乱蓬蓬的花白的头发，一边环视着公共洗脸池四周。不消说，那是我们那幢筒子楼最有碍观瞻的地方。垃圾触目皆是。水池子里沉淀了一层油腻腻粘乎乎的污浊。ｗｗｗ．ｘｉａｂｏｏｋ．ｃｏｍ</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1102/"><img src="https://www.shutxt.com/d/file/dangdai/0edac133322edef020a70f508e4225e5.jpg"></a></p>
<h3 id="尾巴"><a href="#尾巴" class="headerlink" title="尾巴"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1102/">尾巴</a></h3><p>简介：外星人光顾“我”家，告诉“我”将对说谎的地球人进行惩罚。于是“我”赶紧向上级部门反映，却没人相信“我”的话，还把“我”送进精神病院。事后，说过谎话的人无一例外，身后长出各种各样的动物尾巴，整个城市陷入一片恐慌。即便如此，还是没能改变人们说谎的现状，相反，大家在“尾巴城市”里又开始了另一番热火朝天的生活，依然存在尊卑等级、追名逐利、勾心斗角和虚伪狡诈。最终，所有人都被神奇的药水变成了药丸或者裸体小人儿。当“我”意识到要为说谎而忏悔，却为时已晚。小说极具现实讽刺意味，充分揭露了人性的弱点和社会的悲剧列位看家！不不，尊敬的可尊可敬</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1105/"><img src="https://www.shutxt.com/d/file/dangdai/e1b6278d5b2918476f65fbef38ccb716.jpg"></a></p>
<h3 id="弧上的舞者"><a href="#弧上的舞者" class="headerlink" title="弧上的舞者"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1105/">弧上的舞者</a></h3><p>简介：梁晓声是一位怎样的作家，早已有了种种较为普遍的公认。《这是一片神奇的土地》、《今夜有暴风雪》、《雪城》、《年轮》，使他成为“知青文学”的奠基人；《父亲》、《母亲》、《表弟》、《又是中秋》、《黑纽扣》、《白发卡》，使他成为“亲情小说”的代表性作家；而《民选》、《沉默权》、《档案》、《发言》、《学者之死》等等，是他直面现实又特别深刻的中篇力作，证明他始终恪守着关注现实的文学主张…… 所以1984年曾被人言为“梁晓声年”。 所以某报公开评选“感动中国的十位作家”，梁晓声榜上有名。 所以他被称为“平民作家”。 所以他被誉为“中</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/sw/9245/"><img src="https://www.shutxt.com/d/file/pinglun/f7e43ffd0a9bb8d44ed94d03514acb6a.jpg"></a></p>
<h3 id="歌者在桥头"><a href="#歌者在桥头" class="headerlink" title="歌者在桥头"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/sw/9245/">歌者在桥头</a></h3><p>简介：《歌者在桥头》收录了著名作家梁晓声近两三年创作的散文随笔60余篇。被文坛定格为正义维护者和民众代言人的梁晓声，其散文随笔里无不充盈着一种理想的精神，道德的力量和批判的锋芒。这部新作的许多篇章，都从良知与美善出发，无情抨击和辛辣讽刺了社会转型期凭着背景关系、职务、权力等等，谋私暴富或为富不仁的”新贵们”，既斥责了他们聚敛钱财的不择手段，也揭露了他们精神生活的苍白低下：相反，对那些终日为生计而奔波、而操劳，但活的有尊严、有追求的普通劳动者，那些身处底层但终不失仁义与质朴的沉默的大多数，却给予了深切的关爱与热情的礼赞。所表现出来</p>
<p><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1099/"><img src="https://www.shutxt.com/d/file/dangdai/33982beac6fbb6aca628c1a2dadf61fd.jpg"></a></p>
<h3 id="狡猾是一种冒险的游戏"><a href="#狡猾是一种冒险的游戏" class="headerlink" title="狡猾是一种冒险的游戏"></a><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1099/">狡猾是一种冒险的游戏</a></h3><p>简介：全因有了家，活着才是有些情趣的事。当然，这一点于小青年们也许恰恰相反。但于已届中年和中年以后的男人女人们，却是一个无可争议的事实。动物受了伤，还要回到自己的洞穴卧下舔舔伤口呢。中年人是人生最常受伤也最需要一声不哼的毅忍精神的年龄阶段。倘没家，则连个足可以卧下舔伤口的所在都没有了。同样是一声不哼地舔伤口，比较起来，有个家和没有个家那情形是大不一样的……但这里说的其实是潘美辰歌中唱的那种家，一个完全属于自己的“不大不小的地方”。</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://www.ssylu.com/archives/19120">《人世间》梁晓声.epub – 书山有路-一个分享书籍的小憩</a></li>
<li><a target="_blank" rel="noopener" href="https://545c.com/file/1184987-403782982">人世间.epub - 免费下载</a></li>
<li><a target="_blank" rel="noopener" href="https://www.shutxt.com/writer/436/">梁晓声作品集txt下载-梁晓声新书全部小说全集在线阅读</a></li>
<li><a target="_blank" rel="noopener" href="https://www.shutxt.com/rw/9242/">中国生存启示录txt下载_全文免费在线阅读_梁晓声_下书网</a></li>
<li><a target="_blank" rel="noopener" href="https://www.shutxt.com/ls/9240/">真历史在民间txt下载_全文免费在线阅读_梁晓声_下书网</a></li>
<li><a target="_blank" rel="noopener" href="https://www.shutxt.com/rw/9241/">郁闷的中国人txt下载_全文免费在线阅读_梁晓声_下书网</a></li>
<li><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1104/">红磨坊txt下载_全文免费在线阅读_梁晓声_下书网</a></li>
<li><a target="_blank" rel="noopener" href="https://www.shutxt.com/rw/9246/">中国人，你缺了什么txt下载_全文免费在线阅读_梁晓声_下书网</a></li>
<li><a target="_blank" rel="noopener" href="https://www.shutxt.com/zj/1097/">梁晓声-我的大学txt下载_全文免费在线阅读_梁晓声_下书网</a></li>
<li><a target="_blank" rel="noopener" href="https://www.shutxt.com/zj/1098/">京华闻见录txt下载_全文免费在线阅读_梁晓声_下书网</a></li>
<li><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/9244/">表弟txt下载_全文免费在线阅读_梁晓声_下书网</a></li>
<li><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1100/">疲惫的人txt下载_全文免费在线阅读_梁晓声_下书网</a></li>
<li><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/23183/">感觉日本txt下载_全文免费在线阅读_梁晓声_下书网</a></li>
<li><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1105/">弧上的舞者txt下载_全文免费在线阅读_梁晓声_下书网</a></li>
<li><a target="_blank" rel="noopener" href="https://www.shutxt.com/sw/9245/">歌者在桥头txt下载_全文免费在线阅读_梁晓声_下书网</a></li>
<li><a target="_blank" rel="noopener" href="https://www.shutxt.com/xiandai/1099/">狡猾是一种冒险的游戏txt下载_全文免费在线阅读_梁晓声_下书网</a></li>
</ul>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/Log//" class="article-tag-list-link color4">Log</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2022/02/23/20220223/">Expand all items >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-20220222" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/02/22/20220222/">20220222</a>
    </h1>
  

        
        <a href="/2022/02/22/20220222/" class="archive-article-date">
  	<time datetime="2022-02-21T16:00:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>20220222</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
    

      
<div id="toc" class="toc-article">
  <strong class="toc-title">Contents</strong>
  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Working-directories"><span class="toc-number">1.</span> <span class="toc-text">Working directories</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Default-file-manager-and-terminal"><span class="toc-number">2.</span> <span class="toc-text">Default file manager and terminal</span></a></li></ol>
</div>

        <h1 id="Working-directories"><a href="#Working-directories" class="headerlink" title="Working directories"></a>Working directories</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Work&#x2F;Experiments&#x2F;Primers&#x2F;Oligo_Order_上海生工北京合成部&#x2F;Reports # Primer reports, to be organized</span><br><span class="line"># </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="Default-file-manager-and-terminal"><a href="#Default-file-manager-and-terminal" class="headerlink" title="Default file manager and terminal"></a>Default file manager and terminal</h1><ul>
<li><a target="_blank" rel="noopener" href="https://askubuntu.com/questions/155897/how-to-make-nautilus-the-default-file-manager-in-lxde">How to make nautilus the default file manager in LXDE - Ask Ubuntu</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;home&#x2F;ht&#x2F;.config&#x2F;lxsession&#x2F;Lubuntu</span><br><span class="line"># autostart  autostart.0  desktop.conf  desktop.conf.0  desktop.conf.1</span><br><span class="line">cd ~&#x2F;.config&#x2F;lxsession&#x2F;LXDE</span><br><span class="line"># autostart  autostart.0  desktop.conf  desktop.conf.0</span><br><span class="line">cd ~&#x2F;.config&#x2F;openbox</span><br><span class="line"></span><br></pre></td></tr></table></figure>
      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">国家自然科学基金</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">NFS</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/Log//" class="article-tag-list-link color4">Log</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2022/02/22/20220222/">Expand all items >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-20220219" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/02/19/20220219/">20220219</a>
    </h1>
  

        
        <a href="/2022/02/19/20220219/" class="archive-article-date">
  	<time datetime="2022-02-18T16:00:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>20220219</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
    

      
<div id="toc" class="toc-article">
  <strong class="toc-title">Contents</strong>
  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Working-directories"><span class="toc-number">1.</span> <span class="toc-text">Working directories</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#dupeGuru"><span class="toc-number">2.</span> <span class="toc-text">dupeGuru</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#How-to-convert-a-m4a-sound-file-to-mp3-Ask-Ubuntu"><span class="toc-number">3.</span> <span class="toc-text">How to convert a m4a sound file to mp3? - Ask Ubuntu</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#aac2mp3"><span class="toc-number">3.1.</span> <span class="toc-text">aac2mp3</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#avconv"><span class="toc-number">3.2.</span> <span class="toc-text">avconv</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#How-to-hide-close-window-message-when-exit-Mate-Terminal-Support-amp-Help-Requests-Ubuntu-MATE-Community"><span class="toc-number">4.</span> <span class="toc-text">How to hide close window message when exit Mate-Terminal? - Support &amp; Help Requests - Ubuntu MATE Community</span></a></li></ol>
</div>

        <h1 id="Working-directories"><a href="#Working-directories" class="headerlink" title="Working directories"></a>Working directories</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;media&#x2F;ht&#x2F;ht_5T_9&#x2F;YouTubes&#x2F;Music&#x2F;Beauty_Music&#x2F;1&#x2F;Dua # music conversion and organized</span><br><span class="line"># </span><br><span class="line">cd &#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Work&#x2F;DL&#x2F;20220217 # to be organized</span><br><span class="line">cd &#x2F;media&#x2F;ht&#x2F;ht_5T_9&#x2F;YouTubes&#x2F;Economy&#x2F;1 # to be organized</span><br><span class="line"># </span><br><span class="line">OK &#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Work&#x2F;References&#x2F;Manufacturer_Manual&#x2F;Roche&#x2F;DIG_system&#x2F;DIG_Application_Manual&#x2F;DIG_detection&#x2F;Roche_DIG_system_Manual_ed_2008.pdf</span><br><span class="line">cd &#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Work&#x2F;References&#x2F;Important_Ref&#x2F;Journal_Checks&#x2F;Storage&#x2F;ISH</span><br><span class="line"># </span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;4T_1</span><br><span class="line">Drive&#x3D;$&#123;i&#125;</span><br><span class="line">find $&#123;Drive&#125; -type f &gt; &#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Management&#x2F;HD&#x2F;1&#x2F;&#96;basename $&#123;Drive&#125;&#96;.list</span><br><span class="line"># </span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;4T_1</span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T_3</span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T_5</span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T_6</span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T</span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T_9</span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T_18</span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T_7</span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T_8</span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T_10</span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T_12</span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T_15</span><br><span class="line">i&#x3D;&#x2F;media&#x2F;ht&#x2F;ht_5T_16</span><br><span class="line">Drive&#x3D;$&#123;i&#125;; find $&#123;Drive&#125; -type f &gt; &#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Management&#x2F;HD&#x2F;1&#x2F;&#96;basename $&#123;Drive&#125;&#96;_&#96;date +%Y%m%d_%H%M&#96;.list</span><br><span class="line"># </span><br><span class="line">du $&#123;Drive&#125; -sh</span><br></pre></td></tr></table></figure>

<h1 id="dupeGuru"><a href="#dupeGuru" class="headerlink" title="dupeGuru"></a>dupeGuru</h1><ul>
<li><a target="_blank" rel="noopener" href="https://launchpad.net/~dupeguru/+archive/ubuntu/ppa/+packages">Packages in “Dupeguru” : Dupeguru : “Dupeguru” team</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">axel https:&#x2F;&#x2F;launchpad.net&#x2F;~dupeguru&#x2F;+archive&#x2F;ubuntu&#x2F;ppa&#x2F;+files&#x2F;dupeguru_4.1.1-0~ppa~focal1_amd64.deb</span><br><span class="line">sudo gdebi dupeguru_4.1.1-0_ppa_focal1_amd64.deb</span><br></pre></td></tr></table></figure>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/arsenetar/dupeguru">arsenetar/dupeguru: Find duplicate files</a></li>
<li><a target="_blank" rel="noopener" href="https://launchpad.net/~dupeguru/+archive/ubuntu/ppa/+packages">Packages in “Dupeguru” : Dupeguru : “Dupeguru” team</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Stored_Softs&#x2F;Windows_Soft&#x2F;Small_Tools&#x2F;Duplicated_Doc_Deletion&#x2F;DupeGuru&#x2F;DupeGuru_Installer&#x2F;dupeguru_me_win64_6.8.1.msi</span><br><span class="line">&#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Stored_Softs&#x2F;Windows_Soft&#x2F;Small_Tools&#x2F;Duplicated_Doc_Deletion&#x2F;DupeGuru&#x2F;DupeGuru_Installer&#x2F;dupeguru_osx_3_8_0.dmg</span><br><span class="line">&#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Stored_Softs&#x2F;Windows_Soft&#x2F;Small_Tools&#x2F;Duplicated_Doc_Deletion&#x2F;DupeGuru&#x2F;DupeGuru_Installer&#x2F;dupeguru_pe_win64_2.10.1.msi</span><br><span class="line">&#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Stored_Softs&#x2F;Windows_Soft&#x2F;Small_Tools&#x2F;Duplicated_Doc_Deletion&#x2F;DupeGuru&#x2F;DupeGuru_Installer&#x2F;dupeguru_win64_3.8.0.msi</span><br><span class="line">&#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Stored_Softs&#x2F;Windows_Soft&#x2F;Small_Tools&#x2F;Duplicated_Doc_Deletion&#x2F;DupeGuru&#x2F;DupeGuru_Installer&#x2F;dupeguru_win64_3.9.1.msi</span><br><span class="line">&#x2F;media&#x2F;ht&#x2F;ht_5T&#x2F;Stored_Softs&#x2F;Windows_Soft&#x2F;Small_Tools&#x2F;Duplicated_Doc_Deletion&#x2F;DupeGuru&#x2F;DupeGuru_Installer&#x2F;dupeguru_win_3.8.0.msi</span><br></pre></td></tr></table></figure>

<h1 id="How-to-convert-a-m4a-sound-file-to-mp3-Ask-Ubuntu"><a href="#How-to-convert-a-m4a-sound-file-to-mp3-Ask-Ubuntu" class="headerlink" title="How to convert a m4a sound file to mp3? - Ask Ubuntu"></a><a target="_blank" rel="noopener" href="https://askubuntu.com/questions/65331/how-to-convert-a-m4a-sound-file-to-mp3">How to convert a m4a sound file to mp3? - Ask Ubuntu</a></h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -v 5 -y -i input.m4a -acodec libmp3lame -ac 2 -ab 192k output.mp3</span><br><span class="line"># </span><br><span class="line">i&#x3D;1.m4a</span><br><span class="line">ffmpeg -v 5 -y -i $&#123;i&#125; -acodec libmp3lame -ac 2 -ab 16k $&#123;i%.*&#125;.mp3</span><br></pre></td></tr></table></figure>
<h2 id="aac2mp3"><a href="#aac2mp3" class="headerlink" title="aac2mp3"></a><a target="_blank" rel="noopener" href="http://gimpel.gi.funpic.de/wiki/index.php?title=Howto:convert_aac/mp4_to_wav/mp3/ogg_on_Linux#aac2mp3_script">aac2mp3</a></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">#</span><br><span class="line"># $Id: aac2mp3,v 1.2  03&#x2F;30&#x2F;2008 10:00 Daniel Tavares (dantavares@gmail.com) - </span><br><span class="line"># Based on Script -  rali Exp $</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line"># Convert one or more AAC&#x2F;M4A files to MP3.  Based on a script example</span><br><span class="line"># I found at: http:&#x2F;&#x2F;gimpel.gi.funpic.de&#x2F;Howtos&#x2F;convert_aac&#x2F;index.html</span><br><span class="line">#</span><br><span class="line">ME&#x3D;&#96;basename $&#123;0&#125;&#96;</span><br><span class="line">FFMPEG&#x3D;&quot;&#x2F;usr&#x2F;bin&#x2F;ffmpeg&quot;</span><br><span class="line">EXT&#x3D;&quot;mp4&quot;</span><br><span class="line">BITRATE&#x3D;&quot;128&quot;</span><br><span class="line">do_usage() &#123;            # explanatory text</span><br><span class="line"> echo &quot;usage: $&#123;ME&#125; [-b nnn] [-e ext] [-f] [-c] [-r] [-v] [-h] [file list]&quot;</span><br><span class="line"> echo &quot;       Convert music from AAC format to MP3&quot;</span><br><span class="line"> echo &quot;  -m &#x2F;path&#x2F;app  Specify the location of ffmpeg(1)&quot;</span><br><span class="line"> echo &quot;  -b nnn        bitrate for mp3 encoder to use&quot;</span><br><span class="line"> echo &quot;  -e ext        Use .ext rather than .m4a extension&quot;</span><br><span class="line"> echo &quot;  -f            Force overwrite of existing file&quot;</span><br><span class="line"> echo &quot;  -c            Delete original AAC|M4A file(s)&quot;</span><br><span class="line"> echo &quot;  -v            Verbose output&quot;</span><br><span class="line"> echo &quot;  -h            This information&quot;</span><br><span class="line"> echo &quot;&quot;</span><br><span class="line"> echo &quot;For recursive directory, use: find -name &#39;*.$&#123;EXT&#125;&#39; -exec $&#123;ME&#125; &quot;&#123;&#125;&quot; [args]     \;&quot;</span><br><span class="line"> exit 0</span><br><span class="line"> &#125;</span><br><span class="line">do_error() &#123;</span><br><span class="line"> echo &quot;$*&quot;</span><br><span class="line"> exit 1</span><br><span class="line"> &#125;</span><br><span class="line">file_overwrite_check() &#123;</span><br><span class="line"> if [ &quot;$FORCE&quot; !&#x3D; &quot;yes&quot; ]</span><br><span class="line"> then</span><br><span class="line">   test -f &quot;$&#123;1&#125;&quot; &amp;&amp; do_error &quot;$&#123;1&#125; already exists.&quot;</span><br><span class="line"> else</span><br><span class="line">   test -f &quot;$&#123;1&#125;&quot; &amp;&amp; echo &quot;  $&#123;1&#125; is being overwritten.&quot;</span><br><span class="line"> fi</span><br><span class="line"> &#125;</span><br><span class="line">create_mp3() &#123;  # use ffmpeg(1) to convert from AAC to MP3</span><br><span class="line"> file_overwrite_check &quot;$&#123;2&#125;&quot;</span><br><span class="line"> test $VERBOSE &amp;&amp; echo -n &quot;Converting file: $&#123;1&#125;&quot;</span><br><span class="line"> $&#123;FFMPEG&#125; -v 5 -y -i &quot;$&#123;1&#125;&quot; -acodec libmp3lame -ac 2 -ab $&#123;BITRATE&#125;k &quot;$&#123;2&#125;&quot;;</span><br><span class="line"> if [ $? -ne 0 ]</span><br><span class="line"> then</span><br><span class="line">   echo &quot;&quot;</span><br><span class="line">   echo &quot;Error!&quot;</span><br><span class="line">   do_cleanup</span><br><span class="line">   do_error &quot;Exiting&quot;</span><br><span class="line"> fi</span><br><span class="line"> test $VERBOSE &amp;&amp; echo &quot;.  OK&quot;</span><br><span class="line"> &#125;</span><br><span class="line">do_cleanup() &#123;  # Delete intermediate and (optionally) original file(s)</span><br><span class="line"> test $&#123;RMM4A&#125; &amp;&amp; rm -f &quot;$&#123;1&#125;&quot;</span><br><span class="line"> test $VERBOSE &amp;&amp; echo &quot;.  OK&quot;</span><br><span class="line"> &#125;</span><br><span class="line">do_set_bitrate() &#123;</span><br><span class="line"> test $VERBOSE &amp;&amp; echo -n &quot;Setting bitrate to: $1 kbps&quot;</span><br><span class="line"> BITRATE&#x3D;$1</span><br><span class="line"> test $VERBOSE &amp;&amp; echo &quot;.  OK&quot;</span><br><span class="line"> &#125;</span><br><span class="line">GETOPT&#x3D;&#96;getopt -o l:m:b:e:cfhrv -n $&#123;ME&#125; -- &quot;$@&quot;&#96;</span><br><span class="line">if [ $? -ne 0 ]</span><br><span class="line">then</span><br><span class="line"> do_usage</span><br><span class="line">fi</span><br><span class="line">eval set -- &quot;$GETOPT&quot;</span><br><span class="line">while true</span><br><span class="line">do</span><br><span class="line"> case &quot;$1&quot; in</span><br><span class="line">   -m) FFMPEG&#x3D;$2 ; shift ; shift ;;</span><br><span class="line">   -b) do_set_bitrate $2 ; shift ; shift ;;</span><br><span class="line">   -e) EXT&#x3D;$2 ; shift ; shift ;;</span><br><span class="line">   -f) FORCE&#x3D;&quot;yes&quot; ; shift ;;</span><br><span class="line">   -c) RMM4A&#x3D;&quot;yes&quot; ; shift ;;</span><br><span class="line">   -v) VERBOSE&#x3D;&quot;yes&quot; ; shift ;;</span><br><span class="line">   -h) do_usage ;;</span><br><span class="line">   --) shift ; break ;;</span><br><span class="line">    *)  do_usage ;;</span><br><span class="line"> esac</span><br><span class="line">done</span><br><span class="line">test -f $FFMPEG || do_error &quot;$FFMPEG not found. Use \&quot;-m\&quot; switch.&quot;</span><br><span class="line">if [ $# -eq 0 ]</span><br><span class="line">then                    # Convert all files in current directory</span><br><span class="line"> for IFILE in *.$&#123;EXT&#125;</span><br><span class="line"> do</span><br><span class="line">   if [ &quot;$&#123;IFILE&#125;&quot; &#x3D;&#x3D; &quot;*.$&#123;EXT&#125;&quot; ]</span><br><span class="line">   then</span><br><span class="line">     do_error &quot;Not found $&#123;EXT&#125; in this folder.&quot;</span><br><span class="line">   fi</span><br><span class="line">   OUT&#x3D;&#96;echo &quot;$&#123;IFILE&#125;&quot; | sed -e &quot;s&#x2F;\.$&#123;EXT&#125;&#x2F;&#x2F;g&quot;&#96;</span><br><span class="line">   create_mp3 &quot;$&#123;IFILE&#125;&quot; &quot;$&#123;OUT&#125;.mp3&quot;</span><br><span class="line">   do_cleanup &quot;$&#123;IFILE&#125;&quot; </span><br><span class="line"> done</span><br><span class="line">else                    # Convert listed files</span><br><span class="line"> for IFILE in &quot;$*&quot;</span><br><span class="line"> do</span><br><span class="line">   test -f &quot;$&#123;IFILE&#125;&quot; || do_error &quot;$&#123;IFILE&#125; not found.&quot;  </span><br><span class="line">   OUT&#x3D;&#96;echo &quot;$&#123;IFILE&#125;&quot; | sed -e &quot;s&#x2F;\.$&#123;EXT&#125;&#x2F;&#x2F;g&quot;&#96;    </span><br><span class="line">   create_mp3 &quot;$&#123;IFILE&#125;&quot; &quot;$&#123;OUT&#125;.mp3&quot;</span><br><span class="line">   do_cleanup &quot;$&#123;IFILE&#125;&quot;    </span><br><span class="line"> done    </span><br><span class="line">fi   </span><br><span class="line">exit 0</span><br></pre></td></tr></table></figure>
<h2 id="avconv"><a href="#avconv" class="headerlink" title="avconv"></a>avconv</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">avconv -i input.m4a ouptut.mp3</span><br></pre></td></tr></table></figure>
<h1 id="How-to-hide-close-window-message-when-exit-Mate-Terminal-Support-amp-Help-Requests-Ubuntu-MATE-Community"><a href="#How-to-hide-close-window-message-when-exit-Mate-Terminal-Support-amp-Help-Requests-Ubuntu-MATE-Community" class="headerlink" title="How to hide close window message when exit Mate-Terminal? - Support &amp; Help Requests - Ubuntu MATE Community"></a><a target="_blank" rel="noopener" href="https://ubuntu-mate.community/t/how-to-hide-close-window-message-when-exit-mate-terminal/22886">How to hide close window message when exit Mate-Terminal? - Support &amp; Help Requests - Ubuntu MATE Community</a></h1><blockquote>
<p>Found the solution with DCONF-Editor. Run command: dconf-editor or install it first: sudo apt-get install dconf-editor then searched for: confirm-window-close SET to: false in: (org.mate.terminal.global) Now the exit dialogue on multiple Mate-Terminal tabs are gone.</p>
</blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://ubuntu-mate.community/t/how-to-hide-close-window-message-when-exit-mate-terminal/22886/5">How to hide close window message when exit Mate-Terminal? - Support &amp; Help Requests - Ubuntu MATE Community</a></li>
</ul>
<p>Found the solution with DCONF-Editor.</p>
<p>Run command: <strong>dconf-editor</strong><br>or install it first: <strong>sudo apt-get install dconf-editor</strong></p>
<p>then searched for: <strong>confirm-window-close</strong></p>
<blockquote>
<p>SET to: false</p>
</blockquote>
<p>in:<br><strong>(org.mate.terminal.global)</strong></p>
<p>Now the exit dialogue on multiple Mate-Terminal tabs are gone.</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">国家自然科学基金</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">NFS</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/Log//" class="article-tag-list-link color4">Log</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2022/02/19/20220219/">Expand all items >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
    </nav>
  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2022 Hongtu Liu
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>



<script src='https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js'></script>

<script>
    if (window.mermaid) {
        mermaid.initialize({"startOnload":true});
    }
</script>


    </div>
    <script>
	var yiliaConfig = {
		mathjax: false,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&laquo; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &raquo;</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>



{% if theme.mermaid.enable %}
  <script src='https://unpkg.com/mermaid@{{ theme.mermaid.version }}/dist/mermaid.min.js'></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({{ JSON.stringify(theme.mermaid.options) }});
    }
  </script>
{% endif %}

    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">All Items</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">Links</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">About Me</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">p62 Draft</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">QR Code</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">周易大师</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Hexo Note</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">PHMG</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Smallpdf</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">离线语音助手</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">FTP connection</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Tags in Hexo</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Special symbols</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Root Explorer</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">RE</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">ES File Explorer</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">FTP</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">English Correction online</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Image attached</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">HPV30</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Nrf2</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">HPV</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Exiftool</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Legado</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Document Clean-up</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Document Organization</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">SRA</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">integrity</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">vdb-validate</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Macro in Linux WPS</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Hantavirus evolution</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">LibreOffice</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">OpenOffice</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Apache OpenOffice</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">zcat</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">ICTV</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">virus</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">International Committee on Taxonomy of Viruses (ICTV)</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">hexo index</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">hexo content</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Comments in Rmd</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">KinhDown</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">pan.baidu.com</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">SRA tumor cell lines</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Zhang Li</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Guo Yifan</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Ji Tianjiao</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Size reduction of PDF</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">tBHQ</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Tert-Butylhydroquinone</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Authentication Required</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Gene Report</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">BioGPS</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">BaiduNetDisk</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">SRA analysis</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">HLA-G analysis</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">HEV Sequence Download</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Yin Wenjiao</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">E-MTAB-1733</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">AnyDesk update</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Usage of my mobile device</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Windows Docker Machine</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">container</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Tips of Hexo GitHub Pages</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Build Github Pages With Jekyll</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">SDS-PAGE</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Protein Marker</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">txid</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">ETE/ETE3</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Minoxidil</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">rsync</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">SRA-Tools</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">p62</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">E6</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">E7</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">wget</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Endometrial Cancer</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Wang Jiao</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Sex Determination</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">endometrial cancer</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">HLA-G</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">miRBase</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">RNA-Seq</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">ACNUC</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Microsoft Office</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">pdftk</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">PDF password</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">MUSCLE</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">MSA</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">RGB color</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">VPN</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">AppImage</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Outline</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">HEV</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Watch charging</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">代码的魅力</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Flatpak</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Shotcut</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">NSFC</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">E-MTAB-173</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Essential</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Kaiju</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Phage RnD</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Mobile phone</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">NCBI Datasets</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Hexo categories</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">COVID-19 Data Download</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Glossary</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Bacteriophage</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">ContraFect</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Intralytix</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">phagelux</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">菲吉乐科</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Hexo Categories</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">QQ浏览器</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Word</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Flu Biosafety</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">EBV Detection</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">COVID-19  Vaccine</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Nrf2-HSV</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Backup</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">HSV Exosome cn draft</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">B95-8_Nrf2 project</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">B95-8-Nrf2 project</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Offline map</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Google Patents</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">File manager</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Reactive Oxygen Species</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Herpesviridae sequence download</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">HFMD</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">STGCN Model</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Offline Wikipedia</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">flatpak</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Installation Log in X1</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Virus proliferation cell free system</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">CEB</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">COVID-19 Vaccine</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">PDF_Learning</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">R update</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">CRAN update</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Cairo-Dock</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Scheme of README documents</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Bacteriophage database</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Thesis Nrf2</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">凉山项目</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">凉山</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">HSV</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">CRAN</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Monkey B virus</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Hexo</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">恢复真实容量</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Pen drive Encrypting</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">中文语音识别</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">speech recognition</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">scrcpy</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">ADB</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">山西疾控</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">审计</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Antibody Purification</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Water</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">七千人大会</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Long COVID-19</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">算命</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">COVID-19</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Vaccine</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">疾病负担</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">BOD</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Burden of Disease</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Collison</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Nebulizer</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">气溶胶发生器</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">pH meter</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Potassium Chloride</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">KCl</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Android File Transfer</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">openvpn</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">openvpn3</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">WINE</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Figma</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Chinese conversion</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">OpenCC</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">中文简繁转换</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">R</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Gist</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">miR</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">miR prediction</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Primer organization</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">国家自然科学基金</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">NFS</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Kiwix</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Wikipedia</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">HSV-1</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Exosome</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">HGNC</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">file extension</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">RNA-Seq pipeline</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">HEV sequence download</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">HEVnet</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">CSV</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Excel</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Sheet</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Tree structure</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Markdown syntax</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">西部世界</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">IME in Ubuntu</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Markdown Mermaid examples</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Markdown mermaid</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">flowchart</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">MarkText</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Small RNA</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">PDF Note</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Markdown</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Text Editor</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Obsidian</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Gingko</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Manuskript</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">FocusWriter</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Typora</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">TreeSheets</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Zettlr</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Margarine</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Protein-Protein Docking</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">PPI</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Protein Assay</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Proteomics</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">GanttProject</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Tasker</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Shortcuts</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Agent</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">氢原子</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">氢自由基</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">My Linux</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Qv2ray</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">HIV</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Scheme</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">HD Management</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">README</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">HD Structure</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="https://www.ncbi.nlm.nih.gov/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>Old entry NCBI</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.nejm.org/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>The New England J of Medicine</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.thelancet.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>Lancet</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.nature.com/nature/current-issue" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>Nature</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.sciencemag.org/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>Science</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.cell.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>Cell</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.pnas.org/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>PNAS</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.jbc.org/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>JBC</a>
            </li>
          
            <li class="search-li">
              <a href="https://rupress.org/jcb" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>JCB</a>
            </li>
          
            <li class="search-li">
              <a href="https://clinicaltrials.gov/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>ClinicalTrials.gov</a>
            </li>
          
            <li class="search-li">
              <a href="http://192.168.1.10:8000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>MarginNote 3</a>
            </li>
          
            <li class="search-li">
              <a href="https://my.pcloud.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>pCloud</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">Step by step.</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>