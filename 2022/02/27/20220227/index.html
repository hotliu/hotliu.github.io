<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://example.com">
  <title>20220227 | Hongtu Liu</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Enable two-finger scroll via Settings in Windows 10 How To Enable Two Finger Scrolling In Windows 10   OpenCV Ref OpenCV: Install OpenCV-Python in Ubuntu EXTRACTING FACES AND FACIAL FEATURES FROM COLO">
<meta property="og:type" content="article">
<meta property="og:title" content="20220227">
<meta property="og:url" content="http://example.com/2022/02/27/20220227/index.html">
<meta property="og:site_name" content="Hongtu Liu">
<meta property="og:description" content="Enable two-finger scroll via Settings in Windows 10 How To Enable Two Finger Scrolling In Windows 10   OpenCV Ref OpenCV: Install OpenCV-Python in Ubuntu EXTRACTING FACES AND FACIAL FEATURES FROM COLO">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://www.intowindows.com/wp-content/uploads/2013/05/enable-two-fingers-to-scroll-in-Windows-10.jpg.webp">
<meta property="og:image" content="https://miro.medium.com/max/1400/0*25MtyqPCQCSceRx7">
<meta property="og:image" content="https://miro.medium.com/max/1400/1*vnewG2OYRUij_b_zm5uNrQ.png">
<meta property="og:image" content="https://miro.medium.com/max/720/1*Ryd9J1YsNbQPgJkuG6jhSQ.gif">
<meta property="og:image" content="https://miro.medium.com/max/1400/1*OOIj2LNRHEFD5bT-FyNbqQ.png">
<meta property="og:image" content="https://miro.medium.com/max/508/1*82mRzl-khwb2hBzFi9P4vQ.png">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/opencv-face-recognition/opencv_face_reco_animation.gif">
<meta property="og:image" content="https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/source-code-icon.png?lossy=1&strip=1&webp=1">
<meta property="og:image" content="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_facenet.jpg?size=800x520&lossy=1&strip=1&webp=1">
<meta property="og:image" content="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_training.png?size=600x250&lossy=1&strip=1&webp=1">
<meta property="og:image" content="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_dataset.png?size=600x154&lossy=1&strip=1&webp=1">
<meta property="og:image" content="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_result01.jpg?size=500x663&lossy=1&strip=1&webp=1">
<meta property="og:image" content="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_result02.jpg?size=500x663&lossy=1&strip=1&webp=1">
<meta property="og:image" content="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_result03.jpg?size=500x516&lossy=1&strip=1&webp=1">
<meta property="og:image" content="https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/opencv-face-recognition/opencv_face_reco_animation.gif">
<meta property="og:image" content="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_misclassification.jpg?size=500x663&lossy=1&strip=1&webp=1">
<meta property="og:image" content="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_more_data.jpg?size=450x338&lossy=1&strip=1&webp=1">
<meta property="og:image" content="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_alignment.jpg?size=500x388&lossy=1&strip=1&webp=1">
<meta property="og:image" content="https://fast.wistia.com/embed/medias/kno0cmko2z/swatch">
<meta property="og:image" content="https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/cta-source-guide-1.png?lossy=1&strip=1&webp=1">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=240&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/98d96d0f4b4aa3b1b8b58009396f285f?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/19ac9a6370cf20163c098cc74d73ecf4?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/f7883fcf89a17b7716e875f5a48116c2?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/f0c43c8f73a82effb7435efb53899b36?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/b335b3d02b32263040bf7bb83300b4d5?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/3a63fea0fd7f13b5193e941e9c4f72ec?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/ce88ea36c4a0d4be49719fc288741c7f?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/90e8191efee98045d3a6cedc05f629a8?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/9b363c50a2c9e981f0068d275f2330fe?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/730b7f99bf79688ef6d6ac224fd33c6a?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/65b6542316cb29ebcc3c1d4c1c153e30?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/c2fbc71930b000756bd47fd2a295a510?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/00947f41f94298b175471fa22a454333?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/e4d4c96bff48a8f0041eddb3b1e4a07b?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/fe01bbd97203f36cdb327306e96b7f81?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/85b9253a21ac8be53718e2494969eb92?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/d4f8cc6b1b2f0f24382b7e7bbe6cae4c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/aec601bff9ade94745fbc00275df6d43?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/6689a119b178aaf896631cce3eb6dfa9?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/0ff5404071b72662bf2ac08a1fc0a663?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/6e6dbd9e3dbc7353b15a30564b8ff928?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/10bca9fead7f94940afca7f0968de452?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/0d4335ebf0a2f3b800ec9f46979e86e2?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/85239fd9da379f3e663b87323b6cc16e?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/537539638bf494db3bfc9455034bdbd3?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/436bc15408c327a1a6c5fb32f773a503?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/ce88ea36c4a0d4be49719fc288741c7f?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/5d28ff93e3eba927761a950a912ffe43?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/831ee93e10e3030b89deab53727173c2?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/a8d27b7cb84b4d05c00f1479d802330c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/300ccf641ac23e91899b047891b106c6?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/15651d7b8e171af94e5bd71bce549cab?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/2b47ca0cdb231e5b373b064e65d7dd96?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/078e1ce95ea0922270c55084008825e8?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/9fab474cf7b6037cc71c1d55f6fd8710?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/f3f369c16b2690b20eee3c538876e723?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/e44d6db487716a78d0452bf6d95dda2e?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/854626a8741852f22600aa0ecf3db113?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/98cca282ebf5cb7f9e06a9a6fcddc6f5?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/5e6363068241d3a91e4c5adac677f287?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/2255d113eedc5ebad187b2c31f7421ab?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/17ea2e2b2e8554ec30f4f869800d9969?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/0b2ff295c34443a97126dbb2f7a9290b?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/ce88ea36c4a0d4be49719fc288741c7f?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/9a9d21693da7ae487a486721adf881f6?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/f955dc66abe7cb218dc0a31b44d20b7a?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/aa63154f428c44ba2170d6ed3922fb1c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/d539c8d2da5b265526f72a801d994bd8?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/c1bc36fed66d402357a9aa23a9aa9c49?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/c1bc36fed66d402357a9aa23a9aa9c49?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/d0de25b5269a380a7808ab999f632509?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/c828df8607a337589cb3230037c0e7d3?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/a2aa236a27d546893fc2b4f041ff89a1?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/c5a614c796d471aea6466b4e6502813a?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/fb564acbaaf1e4be591c6e3375d94771?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/1a294f2b03ec8daefbac2e9e01023336?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/1a294f2b03ec8daefbac2e9e01023336?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/3fd87de1199debfb581d1be2991f7035?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/589f35f40cffd5ee7f4a26127c969a1e?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/e9e0d696461f09295c94202e7656e220?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/eeb4abbca8c9ae532ef80c699c22f2e5?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/b2907bd098c1c5d9f3a6c86139803361?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/bf4ab4f000e5fd158ce3bcade46f10ac?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/3a0f2964cc8e6836142088899ef84696?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/8de1512eb8040c4462e3445f3d93c3c9?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/411df6715671a6324f2f03467e5220ed?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/c8d6e426dbbe16b0f86529edd3077bed?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/6acc6215e646d233541c02622edffe80?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/4c9435780e30fdda2576808807770bef?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/ccf0d831f8e806bcaf6d11ebd868dea9?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/3c8a3e33695ffb8da8b3f0c657b9182f?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/429f49cb2d1764920122269d3dd01e48?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/da718568eca2d093384447dffcb58749?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/7a64e4011aee9b715141b4b32a9f8c51?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/2485595649da960672517be968ed626f?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/218951b0584347261665a6d23b9bef05?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/f1fbb43375a9e7805edc2e3c0e9e003f?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/a2aa236a27d546893fc2b4f041ff89a1?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/04522096a7f5637650d858adc5ec3cb8?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/4715a1d3021fab7df69092e063e97add?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/c509c13fe67e9993d8f37067071aa9d7?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/b859a4a6afb438c9e85e19b31a43bf84?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/ffeb4e0fe00f937bfaae73b90edf970a?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/6524061d38516004c8776d1a970eadeb?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/3e552c419c9f29c49102e873961035dc?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/f0ce5819789d41ef1b76f16d26cb7380?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/77fbe4d8767ea65790520be6c88f661c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/d328e41d366bb46b9551236eebdd1a26?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/4486e270fe69802ed1634cb4e5b4eaa4?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/bfd45482a0c9aae65511db2b481b99a5?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/bfd45482a0c9aae65511db2b481b99a5?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/c0911bfab60c73b1f6b3ecdf9c5a2f91?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/e169b1cb4aaaa948364488577de5869a?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/52d24164f316a4beb3868371ef8b8a10?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/52d24164f316a4beb3868371ef8b8a10?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/88a10ceb37f26fcdedba65b1257e6d84?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/f31e456bdf30334963e44e6fd95cbb74?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/656b1bf9757cef1c1ae7b37d15585b36?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/2e423ec864459f6c0b5bac8083109861?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/2eed04332db45fd687612df2cfdd8354?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/167e179e2c162ab9edb818172cb0b9b3?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/167e179e2c162ab9edb818172cb0b9b3?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/da74df27a053582cd102e8967c00c652?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/da74df27a053582cd102e8967c00c652?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/ecf65dbdcd8555ac1e3b605b4357d581?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/ecf65dbdcd8555ac1e3b605b4357d581?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/ecf65dbdcd8555ac1e3b605b4357d581?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/ecf65dbdcd8555ac1e3b605b4357d581?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/da74df27a053582cd102e8967c00c652?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/e3f3ecd952301d83f453eb8af7de29bc?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/e3f3ecd952301d83f453eb8af7de29bc?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/b880cd6ec34396048b6bcdf245ed07bd?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/e595fb67d3ba0ceabfdc15ca0368c982?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/d7f52723c6e058db6de8d575812c1789?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/d7f52723c6e058db6de8d575812c1789?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/0c20efc2c178862aa3ffaf6be7a3b464?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/e67d8f890dadcfd32ad3fef8a7298419?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/d289824d54baae6c7b900f9bd0a98fc4?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/ddc6793b79ee9fc718cf4f8c5bdfaabb?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/5c0984f434f8857cd952f0fa1e34aad0?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/32d98e0b564017a3213e26a993691dec?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/32d98e0b564017a3213e26a993691dec?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/a4bc224bb49119d319c20703453237c4?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/fd5332518b2fc5cf8240c792b2a62fa5?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/077be0f39a6a7c60ca2ecb468723621e?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/6c4ae254327c088cd3c528919e0116d0?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/030d98bc0721f87cfd5f87a91cd0d1e8?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/06066a56b243fb4dbc24ff62731323a2?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/6164e359a9fb5a1e747ca4b6504367b0?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/9ada9d76d56192c7abee93a8cc8c6f46?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/7f3c62278bccbaac09bcd3f830058e07?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/7f3c62278bccbaac09bcd3f830058e07?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/a0af53a27d84f929e1bd889ac1d85625?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/033912e2a7fb9fd1499af28b9fd283aa?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/e82277641ca237ee59b9b4ce03465cdb?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/80a2286a020e98bf7f46af5fec98f092?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/39ce36b5470dcd086f356208e1af4a7b?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/39ce36b5470dcd086f356208e1af4a7b?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/fc5db8e72efe50223bb3046b422a7b14?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/91ed5366e48112b91a116efadecf6cee?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/77be938c31a0df5e77feedaac8024824?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/f210f71654aadd78fd768f04e0892327?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/8b3bcc677b86c738f4b101c8119ee4e9?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/1d999c0924278eab9677b5818acc8679?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/d1b65d1602b4578055b667f913699a72?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/07e581b9b8196cc9dd8d9a52017a625a?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/5ec2d7c7a8523b7dc3281ad7232651ff?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/5ec2d7c7a8523b7dc3281ad7232651ff?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/5ec2d7c7a8523b7dc3281ad7232651ff?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/54d5b8e9208c10e60499d2d4279eaf60?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/795f1a46a158a7cd900581c80acd6da6?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/862a7fa2a346399b41484f7252e83573?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/36eab9d9ae30563e69b03af1df915332?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/ca7c7c2256b6729332a0a2e06c881d8f?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/950d1207c1f6be14d91eee39ac605149?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/e9f998df92b979c497155fe7325ccc34?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/5b72e542cc0edfa62b81434393063980?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/a0c010949f6ddd07b244c9b805a950aa?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/a0c010949f6ddd07b244c9b805a950aa?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/591547bb8a684e01255d7f703b7c3b6f?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/7a5b63029d238d750844971e7b8099b4?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/a8d13d40a4e9bc09904310fc74a68313?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/e142ea9ee3b7df2a20c48dfc14919858?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/6ed83c97290c420eedddea58f9ffd36e?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/6ed83c97290c420eedddea58f9ffd36e?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/688eac7077211067ae7a840e1f0b04b3?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/5d08cc9c16d1be4db348db058df1e2f7?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/d35c797a51d50787d326144245205610?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/3120253ce41a8a43a06ccb06f34e65ee?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/d7234df5deec5567e23c575aaf0f7dce?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/06f302054df1716601d88ef472798e08?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/4e33e2197422c731da797543d3108590?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/5348c75918f8f0928d9af5cc4c9c8674?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/5c0984f434f8857cd952f0fa1e34aad0?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/d01c231f9284829b458d6b305bcd4f62?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/8a83f2ab071ca1fd9c5d05ab55345c79?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/8d91404151760bd70274a67e8b5f64eb?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/38cd39c8e11c0a3f7024dd2a6d775d3e?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/5709ea0dc18a6509e9d31806eed7433c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/a4f588f318c8fab8fa1daec1bf4e9338?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/42cc35b843d37962c7e8a7916099ee75?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/ed1f7d13bcdc64c1ff00f62174d5e769?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/5c3fe2975aadc5f22ed393d31e62c4a8?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/a3111b30d53917cb4ad0d15a38936614?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/c1bc36fed66d402357a9aa23a9aa9c49?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/d2cf8e758c1f9bf09a3e68bea07ff384?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/17d4f287e7d456b2493ebeb721d3faf8?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/d2cf8e758c1f9bf09a3e68bea07ff384?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/3980fc782797298c6a79526211562850?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/2e8c2b2072f002ac31ed3094b837a61b?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/8acf0136329615c5cc834b792c1b9a80?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/17d4f287e7d456b2493ebeb721d3faf8?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/17d4f287e7d456b2493ebeb721d3faf8?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/a3111b30d53917cb4ad0d15a38936614?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/7893dfdb872cc8d4bd3a0fab8a982764?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/d0ec7c58e225d7de85eaf121b25ea4fa?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/5c0984f434f8857cd952f0fa1e34aad0?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/3506b4c89abda39cf0956e12883973e1?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/64a436a1f4a53de421d20cda470e064e?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/172d4f00689d259a709b23a4c071028c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/ae1288eb05c74189dd8c8f9236ddb6a3?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/bc4a2e2d6f818701459f0b3d4a43ffcb?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/7f17aa344c89591b5e7131de0d959d50?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/6702095f5a6461768ed0aabd6a05943c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/6e77761fad03a1e454df6815264f8442?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/578843c9c21df64ad2125b6e2f0a20c6?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/f4511fd4e33e8f21cdfa8ca676cf35c2?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/10a8bbb79635e9e760d01b81ba7e321b?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/d15ec2cd8fac468e2fb54d5b77c95cec?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/288d90d121caeb77304b1ed2ff7adb84?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/288d90d121caeb77304b1ed2ff7adb84?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/24876b2979b22a6e65c5532910d6e9f2?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/1e23615ff3bb76e04e57182ce176f7a7?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/1e23615ff3bb76e04e57182ce176f7a7?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/57d78783fffdd7596755a2ef730f7171?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/1bb95587f5169c3b2a69b3e092309778?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/e7723b7db5de3fe3440336123c921ed5?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/23090779df0bfe147c3a96d6ab304cb5?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/b08290e8d027c109c801846e91820980?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/d429d37d2c16d200b0158406ddcfe287?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/7101c780da72f6f509d2cfc7171b2501?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/d4a406b3b129e421976211e1373f3f4c?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/b584aa14257c001aaefc221984ab71fa?s=48&d=mm&r=g">
<meta property="og:image" content="https://secure.gravatar.com/avatar/5141893eba3d2baa314e61d812e8fae0?s=48&d=mm&r=g">
<meta property="article:published_time" content="2022-02-26T16:00:00.000Z">
<meta property="article:modified_time" content="2022-03-02T02:19:53.413Z">
<meta property="article:author" content="Hongtu Liu">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.intowindows.com/wp-content/uploads/2013/05/enable-two-fingers-to-scroll-in-Windows-10.jpg.webp">
  
    <link rel="alternative" href="/atom.xml" title="Hongtu Liu" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(200deg,#a0cfe4,#e8c37e);
    }
  </style>
  

  

<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/"></a></h1>
		</hgroup>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">Home</a></li>
	        
				<li><a href="/categories">Categories</a></li>
	        
				<li><a href="/tags/%E9%9A%8F%E7%AC%94/">随笔</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">All Items</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">Links</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">About Me</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="#" title="github"><i class="icon-github"></i></a>
		        
					<a class="weibo" target="_blank" href="#" title="weibo"><i class="icon-weibo"></i></a>
		        
					<a class="rss" target="_blank" href="#" title="rss"><i class="icon-rss"></i></a>
		        
					<a class="zhihu" target="_blank" href="#" title="zhihu"><i class="icon-zhihu"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author"></h1>
			</hgroup>
			
			
			
				
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="#" title="github"><i class="icon-github"></i></a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo"><i class="icon-weibo"></i></a>
			        
						<a class="rss" target="_blank" href="#" title="rss"><i class="icon-rss"></i></a>
			        
						<a class="zhihu" target="_blank" href="#" title="zhihu"><i class="icon-zhihu"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 70%">
				
				
					<li style="width: 33.333333333333336%"><a href="/">Home</a></li>
		        
					<li style="width: 33.333333333333336%"><a href="/categories">Categories</a></li>
		        
					<li style="width: 33.333333333333336%"><a href="/tags/%E9%9A%8F%E7%AC%94/">随笔</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            <article id="post-20220227" class="article article-type-post " itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      20220227
    </h1>
  

        
        <a href="/2022/02/27/20220227/" class="archive-article-date">
  	<time datetime="2022-02-26T16:00:00.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>20220227</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
    

      
<div id="toc" class="toc-article">
  <strong class="toc-title">Contents</strong>
  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Enable-two-finger-scroll-via-Settings-in-Windows-10"><span class="toc-number">1.</span> <span class="toc-text">Enable two-finger scroll via Settings in Windows 10</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#OpenCV-Ref"><span class="toc-number">2.</span> <span class="toc-text">OpenCV Ref</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Extract-faces-from-images"><span class="toc-number">3.</span> <span class="toc-text">Extract faces from images</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Extracting-faces-using-OpenCV-Face-Detection-Neural-Network"><span class="toc-number">3.1.</span> <span class="toc-text">Extracting faces using OpenCV Face Detection Neural Network</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Aim"><span class="toc-number">3.1.1.</span> <span class="toc-text">Aim</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Project"><span class="toc-number">3.1.2.</span> <span class="toc-text">Project</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Import-libraries"><span class="toc-number">3.1.2.1.</span> <span class="toc-text">Import libraries</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Define-paths-and-load-model"><span class="toc-number">3.1.2.2.</span> <span class="toc-text">Define paths and load model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Create-directory"><span class="toc-number">3.1.2.3.</span> <span class="toc-text">Create directory</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Read-images"><span class="toc-number">3.1.2.4.</span> <span class="toc-text">Read images</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Detect-faces"><span class="toc-number">3.1.2.5.</span> <span class="toc-text">Detect faces</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Create-boxes-around-faces"><span class="toc-number">3.1.2.6.</span> <span class="toc-text">1. Create boxes around faces</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Extracting-faces"><span class="toc-number">3.1.2.7.</span> <span class="toc-text">2. Extracting faces</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusion"><span class="toc-number">3.1.3.</span> <span class="toc-text">Conclusion</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#OpenCV-Face-Recognition"><span class="toc-number">3.2.</span> <span class="toc-text">OpenCV Face Recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Looking-for-the-source-code-to-this-post"><span class="toc-number">3.2.0.0.1.</span> <span class="toc-text">Looking for the source code to this post?</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#OpenCV-Face-Recognition-1"><span class="toc-number">3.2.1.</span> <span class="toc-text">OpenCV Face Recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#How-OpenCV%E2%80%99s-face-recognition-works"><span class="toc-number">3.2.1.1.</span> <span class="toc-text">How OpenCV’s face recognition works</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Our-face-recognition-dataset"><span class="toc-number">3.2.1.2.</span> <span class="toc-text">Our face recognition dataset</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Project-structure"><span class="toc-number">3.2.1.3.</span> <span class="toc-text">Project structure</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-1-Extract-embeddings-from-face-dataset"><span class="toc-number">3.2.1.4.</span> <span class="toc-text">Step #1: Extract embeddings from face dataset</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#import-the-necessary-packages"><span class="toc-number">3.3.</span> <span class="toc-text">import the necessary packages</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#construct-the-argument-parser-and-parse-the-arguments"><span class="toc-number">3.4.</span> <span class="toc-text">construct the argument parser and parse the arguments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-detector-from-disk"><span class="toc-number">3.5.</span> <span class="toc-text">load our serialized face detector from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-embedding-model-from-disk"><span class="toc-number">3.6.</span> <span class="toc-text">load our serialized face embedding model from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#grab-the-paths-to-the-input-images-in-our-dataset"><span class="toc-number">3.7.</span> <span class="toc-text">grab the paths to the input images in our dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#initialize-our-lists-of-extracted-facial-embeddings-and"><span class="toc-number">3.8.</span> <span class="toc-text">initialize our lists of extracted facial embeddings and</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#corresponding-people-names"><span class="toc-number">3.9.</span> <span class="toc-text">corresponding people names</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#initialize-the-total-number-of-faces-processed"><span class="toc-number">3.10.</span> <span class="toc-text">initialize the total number of faces processed</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#loop-over-the-image-paths"><span class="toc-number">3.11.</span> <span class="toc-text">loop over the image paths</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dump-the-facial-embeddings-names-to-disk"><span class="toc-number">3.12.</span> <span class="toc-text">dump the facial embeddings + names to disk</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-2-Train-face-recognition-model"><span class="toc-number">3.12.0.1.</span> <span class="toc-text">Step #2: Train face recognition model</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#import-the-necessary-packages-1"><span class="toc-number">3.13.</span> <span class="toc-text">import the necessary packages</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#construct-the-argument-parser-and-parse-the-arguments-1"><span class="toc-number">3.14.</span> <span class="toc-text">construct the argument parser and parse the arguments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-the-face-embeddings"><span class="toc-number">3.15.</span> <span class="toc-text">load the face embeddings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#encode-the-labels"><span class="toc-number">3.16.</span> <span class="toc-text">encode the labels</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#train-the-model-used-to-accept-the-128-d-embeddings-of-the-face-and"><span class="toc-number">3.17.</span> <span class="toc-text">train the model used to accept the 128-d embeddings of the face and</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#then-produce-the-actual-face-recognition"><span class="toc-number">3.18.</span> <span class="toc-text">then produce the actual face recognition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#write-the-actual-face-recognition-model-to-disk"><span class="toc-number">3.19.</span> <span class="toc-text">write the actual face recognition model to disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#write-the-label-encoder-to-disk"><span class="toc-number">3.20.</span> <span class="toc-text">write the label encoder to disk</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-3-Recognize-faces-with-OpenCV"><span class="toc-number">3.20.0.1.</span> <span class="toc-text">Step #3: Recognize faces with OpenCV</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#import-the-necessary-packages-2"><span class="toc-number">3.21.</span> <span class="toc-text">import the necessary packages</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#construct-the-argument-parser-and-parse-the-arguments-2"><span class="toc-number">3.22.</span> <span class="toc-text">construct the argument parser and parse the arguments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-detector-from-disk-1"><span class="toc-number">3.23.</span> <span class="toc-text">load our serialized face detector from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-embedding-model-from-disk-1"><span class="toc-number">3.24.</span> <span class="toc-text">load our serialized face embedding model from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-the-actual-face-recognition-model-along-with-the-label-encoder"><span class="toc-number">3.25.</span> <span class="toc-text">load the actual face recognition model along with the label encoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-the-image-resize-it-to-have-a-width-of-600-pixels-while"><span class="toc-number">3.26.</span> <span class="toc-text">load the image, resize it to have a width of 600 pixels (while</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#maintaining-the-aspect-ratio-and-then-grab-the-image-dimensions"><span class="toc-number">3.27.</span> <span class="toc-text">maintaining the aspect ratio), and then grab the image dimensions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#construct-a-blob-from-the-image"><span class="toc-number">3.28.</span> <span class="toc-text">construct a blob from the image</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#apply-OpenCV%E2%80%99s-deep-learning-based-face-detector-to-localize"><span class="toc-number">3.29.</span> <span class="toc-text">apply OpenCV’s deep learning-based face detector to localize</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#faces-in-the-input-image"><span class="toc-number">3.30.</span> <span class="toc-text">faces in the input image</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#loop-over-the-detections"><span class="toc-number">3.31.</span> <span class="toc-text">loop over the detections</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#show-the-output-image"><span class="toc-number">3.32.</span> <span class="toc-text">show the output image</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#BONUS-Recognize-faces-in-video-streams"><span class="toc-number">3.32.0.1.</span> <span class="toc-text">BONUS: Recognize faces in video streams</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#import-the-necessary-packages-3"><span class="toc-number">3.33.</span> <span class="toc-text">import the necessary packages</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#construct-the-argument-parser-and-parse-the-arguments-3"><span class="toc-number">3.34.</span> <span class="toc-text">construct the argument parser and parse the arguments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-detector-from-disk-2"><span class="toc-number">3.35.</span> <span class="toc-text">load our serialized face detector from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-embedding-model-from-disk-2"><span class="toc-number">3.36.</span> <span class="toc-text">load our serialized face embedding model from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-the-actual-face-recognition-model-along-with-the-label-encoder-1"><span class="toc-number">3.37.</span> <span class="toc-text">load the actual face recognition model along with the label encoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#initialize-the-video-stream-then-allow-the-camera-sensor-to-warm-up"><span class="toc-number">3.38.</span> <span class="toc-text">initialize the video stream, then allow the camera sensor to warm up</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#start-the-FPS-throughput-estimator"><span class="toc-number">3.39.</span> <span class="toc-text">start the FPS throughput estimator</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#loop-over-frames-from-the-video-file-stream"><span class="toc-number">3.40.</span> <span class="toc-text">loop over frames from the video file stream</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#stop-the-timer-and-display-FPS-information"><span class="toc-number">3.41.</span> <span class="toc-text">stop the timer and display FPS information</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#do-a-bit-of-cleanup"><span class="toc-number">3.42.</span> <span class="toc-text">do a bit of cleanup</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Drawbacks-limitations-and-how-to-obtain-higher-face-recognition-accuracy"><span class="toc-number">3.42.0.1.</span> <span class="toc-text">Drawbacks, limitations, and how to obtain higher face recognition accuracy</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#You-may-need-more-data"><span class="toc-number">3.42.0.1.1.</span> <span class="toc-text">You may need more data</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Perform-face-alignment"><span class="toc-number">3.42.0.1.2.</span> <span class="toc-text">Perform face alignment</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Tune-your-hyperparameters"><span class="toc-number">3.42.0.1.3.</span> <span class="toc-text">Tune your hyperparameters</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Use-dlib%E2%80%99s-embedding-model-but-not-it%E2%80%99s-k-NN-for-face-recognition"><span class="toc-number">3.42.0.1.4.</span> <span class="toc-text">Use dlib’s embedding model (but not it’s k-NN for face recognition)</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Did-you-encounter-a-%E2%80%9CUSAGE%E2%80%9D-error-running-today%E2%80%99s-Python-face-recognition-scripts"><span class="toc-number">3.42.0.2.</span> <span class="toc-text">Did you encounter a “USAGE” error running today’s Python face recognition scripts?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Alternative-OpenCV-face-recognition-methods"><span class="toc-number">3.42.0.3.</span> <span class="toc-text">Alternative OpenCV face recognition methods</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#What%E2%80%99s-next-I-recommend-PyImageSearch-University"><span class="toc-number">3.42.0.4.</span> <span class="toc-text">What’s next? I recommend PyImageSearch University.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary"><span class="toc-number">3.42.1.</span> <span class="toc-text">Summary</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Download-the-Source-Code-and-FREE-17-page-Resource-Guide"><span class="toc-number">3.42.1.0.1.</span> <span class="toc-text">Download the Source Code and FREE 17-page Resource Guide</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#About-the-Author"><span class="toc-number">3.42.1.0.2.</span> <span class="toc-text">About the Author</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reader-Interactions"><span class="toc-number">3.42.2.</span> <span class="toc-text">Reader Interactions</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#pip-install-OpenCV"><span class="toc-number">3.42.2.1.</span> <span class="toc-text">pip install OpenCV</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Install-OpenCV-4-on-your-Raspberry-Pi"><span class="toc-number">3.42.2.2.</span> <span class="toc-text">Install OpenCV 4 on your Raspberry Pi</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#359-responses-to-OpenCV-Face-Recognition"><span class="toc-number">3.42.2.3.</span> <span class="toc-text">359 responses to: OpenCV Face Recognition</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#do-a-bit-of-cleanup-1"><span class="toc-number">3.43.</span> <span class="toc-text">do a bit of cleanup</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Comment-section"><span class="toc-number">3.43.0.1.</span> <span class="toc-text">Comment section</span></a></li></ol></li></ol></li></ol></li></ol>
</div>

        <h1 id="Enable-two-finger-scroll-via-Settings-in-Windows-10"><a href="#Enable-two-finger-scroll-via-Settings-in-Windows-10" class="headerlink" title="Enable two-finger scroll via Settings in Windows 10"></a>Enable two-finger scroll via Settings in Windows 10</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.intowindows.com/enable-two-finger-scrolling-windows-8/">How To Enable Two Finger Scrolling In Windows 10</a></li>
</ul>
<p><img src="https://www.intowindows.com/wp-content/uploads/2013/05/enable-two-fingers-to-scroll-in-Windows-10.jpg.webp"></p>
<h1 id="OpenCV-Ref"><a href="#OpenCV-Ref" class="headerlink" title="OpenCV Ref"></a>OpenCV Ref</h1><ul>
<li><a target="_blank" rel="noopener" href="https://docs.opencv.org/3.4/d2/de6/tutorial_py_setup_in_ubuntu.html">OpenCV: Install OpenCV-Python in Ubuntu</a></li>
<li><a target="_blank" rel="noopener" href="https://www.worldscientific.com/doi/10.1142/S0218001408006296">EXTRACTING FACES AND FACIAL FEATURES FROM COLOR IMAGES | International Journal of Pattern Recognition and Artificial Intelligence</a></li>
<li><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/">OpenCV Face Recognition - PyImageSearch</a></li>
<li><a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/cropping-faces-from-images-using-opencv-python/">Cropping Faces from Images using OpenCV - Python - GeeksforGeeks</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/extracting-faces-using-opencv-face-detection-neural-network-475c5cd0c260">Extracting faces using OpenCV Face Detection Neural Network | by Karan Bhanot | Towards Data Science</a></li>
</ul>
<h1 id="Extract-faces-from-images"><a href="#Extract-faces-from-images" class="headerlink" title="Extract faces from images"></a>Extract faces from images</h1><h2 id="Extracting-faces-using-OpenCV-Face-Detection-Neural-Network"><a href="#Extracting-faces-using-OpenCV-Face-Detection-Neural-Network" class="headerlink" title="Extracting faces using OpenCV Face Detection Neural Network"></a>Extracting faces using OpenCV Face Detection Neural Network</h2><ul>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/extracting-faces-using-opencv-face-detection-neural-network-475c5cd0c260">Extracting faces using OpenCV Face Detection Neural Network | by Karan Bhanot | Towards Data Science</a><br><img src="https://miro.medium.com/max/1400/0*25MtyqPCQCSceRx7"></li>
</ul>
<p>Photo by <a target="_blank" rel="noopener" href="https://unsplash.com/@zoner?utm_source=medium&utm_medium=referral">Maxim Dužij</a> on <a target="_blank" rel="noopener" href="https://unsplash.com/?utm_source=medium&utm_medium=referral">Unsplash</a></p>
<p>Recently, I came across the website <a target="_blank" rel="noopener" href="https://www.pyimagesearch.com/">https://www.pyimagesearch.com/</a> which has some of the greatest tutorials on OpenCV. While reading through its numerous articles, I found that OpenCV has its own Face Detection Neural Network with really high accuracy.</p>
<p>So I decided to work on a project using this Neural Network from OpenCV and extract faces from images. Such a process would come handy whenever someone is working with faces and needs to extract them from a number of images.</p>
<p>The complete project is available as a <a target="_blank" rel="noopener" href="https://github.com/kb22/Create-Face-Data-from-Images">GitHub repository</a>. For this article, I’ve taken a picture from my Instagram account.</p>
<p><img src="https://miro.medium.com/max/1400/1*vnewG2OYRUij_b_zm5uNrQ.png"></p>
<p>Image used for extracting face</p>
<h3 id="Aim"><a href="#Aim" class="headerlink" title="Aim"></a>Aim</h3><p>The project has two essential elements:  </p>
<ol>
<li><strong>Box around faces:</strong> Show white boxes around all the faces recognised in the image. The Python file is _data_generator.py_  </li>
<li><strong>Extracted faces:</strong> Extract faces from all images in a folder and save each face into a destination folder to create a handy dataset. The Python file is _face_extractor.py_</li>
</ol>
<p><img src="https://miro.medium.com/max/720/1*Ryd9J1YsNbQPgJkuG6jhSQ.gif"></p>
<p>Face detection and extraction</p>
<p>First, let’s perform the common steps for the two parts, i.e. importing libraries, loading the face detection model, creating output directory, reading images and detecting faces.</p>
<h3 id="Project"><a href="#Project" class="headerlink" title="Project"></a>Project</h3><h4 id="Import-libraries"><a href="#Import-libraries" class="headerlink" title="Import libraries"></a>Import libraries</h4><p>I import <code>os</code> to access various files in the directory. Then, <code>cv2</code> will be used to work with images. <code>numpy</code> helps to easily work with multi-dimensional arrays.</p>
<h4 id="Define-paths-and-load-model"><a href="#Define-paths-and-load-model" class="headerlink" title="Define paths and load model"></a>Define paths and load model</h4><p>The model’s <code>prototxt</code> and <code>caffemodel</code> is provided in the OpenCV repo itself. I used the same and placed them in the <code>model_data</code> directory in my project. <code>prototxt</code> file includes the text description of the network and <code>caffemodel</code> includes the weights. I read the two files and loaded my <code>model</code> using <code>cv2</code>.</p>
<h4 id="Create-directory"><a href="#Create-directory" class="headerlink" title="Create directory"></a>Create directory</h4><p>If the directory where the resultant images will get stored does not exist, I’ll create the directory. The output folder is <strong>updated_images</strong>.</p>
<p>When working with extracting faces, I’ll save the faces into the directory <strong>faces</strong>. If it is not present, I’ll create it.</p>
<h4 id="Read-images"><a href="#Read-images" class="headerlink" title="Read images"></a>Read images</h4><p>I loop through all images inside the <strong>images</strong> folder. After extracting the extension, I check that the files are either of the type <code>.png</code> or <code>.jpg</code> and just operate with those files only.</p>
<h4 id="Detect-faces"><a href="#Detect-faces" class="headerlink" title="Detect faces"></a>Detect faces</h4><p>Using <code>cv2.imread</code>, I read the image, and create a blob using <code>cv2.dnn.blobFromImage</code>. Then, I input this blob into the model and get back the detections from the page using <code>model.forward()</code>.</p>
<p>The common steps are now complete. For the first task, I’ll plot white rectangles around faces and save them in <strong>updated_images</strong> directory. For the second task, I’ll save the extracted faces in <strong>faces</strong> directory.</p>
<h4 id="1-Create-boxes-around-faces"><a href="#1-Create-boxes-around-faces" class="headerlink" title="1. Create boxes around faces"></a>1. Create boxes around faces</h4><p>One by one, I iterate over all of the faces detected in the image and extract their start and end points. Then, I extract the confidence of detection. If the algorithm is more than 50% confident that the detection is a face, I show a rectangle around it.</p>
<p>Then, using <code>cv2.imwrite</code>, I save the image to the <code>updated_images</code> folder with the same name.</p>
<p><img src="https://miro.medium.com/max/1400/1*OOIj2LNRHEFD5bT-FyNbqQ.png"></p>
<p>Image with white rectangle around face</p>
<h4 id="2-Extracting-faces"><a href="#2-Extracting-faces" class="headerlink" title="2. Extracting faces"></a>2. Extracting faces</h4><p>As described above, I iterate all faces, calculate the confidence of detection and if it is more than 50%, I extract the face. Notice the line <code>frame = image[startY:endY, startX:endX]</code>. It extracts the face from the image.</p>
<p>Then, I dump this new image into the <code>faces</code> folder with the name as face number, followed by <code>_</code>, and then the name of the file. If we extracted the first face from an image named <code>sampleImage.png</code>, the name of the face file will be <code>0_sampleImage.png</code>. With each face, I increment the <code>count</code> and after complete execution, I print the count to the console.</p>
<p><img src="https://miro.medium.com/max/508/1*82mRzl-khwb2hBzFi9P4vQ.png"></p>
<p>Extracted face</p>
<p>Finally, the project is ready. You can feed in as many images as possible and generate datasets which can be used for further projects.</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>In this article, I discussed using OpenCV Face Detection Neural Network to detect faces in an image, label them with white rectangles and extract faces into separate images.</p>
<p>As always, I’d love to hear about your thoughts and suggestions.</p>
<p><a target="_blank" rel="noopener" href="https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/towards-data-science/475c5cd0c260&operation=register&redirect=https://towardsdatascience.com/extracting-faces-using-opencv-face-detection-neural-network-475c5cd0c260&user=Karan+Bhanot&userId=10df94b13417&source=post_actions_footer-----475c5cd0c260---------------------clap_footer--------------"></a></p>
<hr>
<h2 id="OpenCV-Face-Recognition"><a href="#OpenCV-Face-Recognition" class="headerlink" title="OpenCV Face Recognition"></a>OpenCV Face Recognition</h2><ul>
<li><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/">OpenCV Face Recognition - PyImageSearch</a></li>
</ul>
<p>by <a target="_blank" rel="noopener" href="https://pyimagesearch.com/author/adrian/">Adrian Rosebrock</a> on September 24, 2018</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#download-the-code">Click here to download the source code to this post</a></p>
<p>Last updated on July 4, 2021.</p>
<p><img src="https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/opencv-face-recognition/opencv_face_reco_animation.gif"></p>
<p><strong>In this tutorial, you will learn how to use OpenCV to perform face recognition.</strong> To build our face recognition system, we’ll first perform face detection, extract face embeddings from each face using deep learning, train a face recognition model on the embeddings, and then finally <strong>recognize faces in both images and video streams with OpenCV.</strong></p>
<p>Today’s tutorial is also a special gift for my fiancée, Trisha (who is now officially my wife). Our wedding was over the weekend, and by the time you’re reading this blog post, we’ll be at the airport preparing to board our flight for the honeymoon.</p>
<p>To celebrate the occasion, and show her how much her support of myself, the PyImageSearch blog, and the PyImageSearch community means to me, I decided to use OpenCV to perform face recognition on a dataset of our faces.</p>
<p><strong>You can swap in your own dataset of faces of course!</strong> All you need to do is follow my directory structure in insert your own face images.</p>
<p>As a bonus, I’ve also included how to label “unknown” faces that cannot be classified with sufficient confidence.</p>
<p><strong>To learn how to perform OpenCV face recognition, <em>just keep reading!</em></strong></p>
<ul>
<li>  <strong>Update July 2021:</strong> Added section on alternative face recognition methods to consider, including how siamese networks can be used for face recognition.</li>
</ul>
<p><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/source-code-icon.png?lossy=1&strip=1&webp=1"></p>
<h5 id="Looking-for-the-source-code-to-this-post"><a href="#Looking-for-the-source-code-to-this-post" class="headerlink" title="Looking for the source code to this post?"></a>Looking for the source code to this post?</h5><p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#download-the-code">JUMP RIGHT TO THE DOWNLOADS SECTION</a> </p>
<h3 id="OpenCV-Face-Recognition-1"><a href="#OpenCV-Face-Recognition-1" class="headerlink" title="OpenCV Face Recognition"></a>OpenCV Face Recognition</h3><p>In today’s tutorial, you will learn how to perform face recognition using the OpenCV library.</p>
<p>You might be wondering how this tutorial is different from the one I wrote a few months back on <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">face recognition with dlib</a>?</p>
<p>Well, keep in mind that the dlib face recognition post relied on two important external libraries:</p>
<ol>
<li> <a target="_blank" rel="noopener" href="http://dlib.net/">dlib</a> (obviously)</li>
<li> <a target="_blank" rel="noopener" href="https://github.com/ageitgey/face_recognition">face_recognition</a> (which is an easy to use set of face recognition utilities that wraps around dlib)</li>
</ol>
<p>While we used OpenCV to <em>facilitate</em> face recognition, OpenCV <em>itself</em> was not responsible for identifying faces.</p>
<p>In today’s tutorial, we’ll learn how we can apply deep learning and OpenCV together (with no other libraries other than scikit-learn) to:</p>
<ol>
<li> Detect faces</li>
<li> Compute 128-d face embeddings to quantify a face</li>
<li> Train a Support Vector Machine (SVM) on top of the embeddings</li>
<li> Recognize faces in images and video streams</li>
</ol>
<p>All of these tasks will be accomplished with OpenCV, enabling us to obtain a “pure” OpenCV face recognition pipeline.</p>
<h4 id="How-OpenCV’s-face-recognition-works"><a href="#How-OpenCV’s-face-recognition-works" class="headerlink" title="How OpenCV’s face recognition works"></a>How OpenCV’s face recognition works</h4><p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_facenet.jpg"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_facenet.jpg?size=800x520&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 1:</strong> An overview of the OpenCV face recognition pipeline. The key step is a CNN feature extractor that generates 128-d facial embeddings. (<a target="_blank" rel="noopener" href="https://cmusatyalab.github.io/openface/">source</a>)</p>
<p>In order to build our OpenCV face recognition pipeline, we’ll be applying deep learning in two key steps:</p>
<ol>
<li> To apply <em>face detection</em>, which detects the <em>presence</em> and location of a face in an image, but does not identify it</li>
<li> To extract the 128-d feature vectors (called “embeddings”) that <em>quantify</em> each face in an image</li>
</ol>
<p>I’ve discussed <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/">how OpenCV’s face detection works</a> previously, so please refer to it if you have not detected faces before.</p>
<p>The model responsible for actually quantifying each face in an image is from the <a target="_blank" rel="noopener" href="https://cmusatyalab.github.io/openface/">OpenFace project</a>, a Python and Torch implementation of face recognition with deep learning. This implementation comes from Schroff et al.’s 2015 CVPR publication, <a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/app/1A_089.pdf"><em>FaceNet: A</em> <em>Unified Embedding for Face Recognition and Clustering</em></a>.</p>
<p>Reviewing the entire FaceNet implementation is outside the scope of this tutorial, but the gist of the pipeline can be seen in <strong>Figure 1</strong> above.</p>
<p>First, we input an image or video frame to our face recognition pipeline. Given the input image, we apply face detection to detect the location of a face in the image.</p>
<p>Optionally we can compute <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/">facial landmarks</a>, enabling us to <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/">preprocess and align the face</a>.</p>
<p>Face alignment, as the name suggests, is the process of (1) identifying the geometric structure of the faces and (2) attempting to obtain a canonical alignment of the face based on translation, rotation, and scale.</p>
<p>While optional, face alignment has been demonstrated to increase face recognition accuracy in some pipelines.</p>
<p>After we’ve (optionally) applied face alignment and cropping, we pass the input face through our deep neural network:</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_training.png"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_training.png?size=600x250&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 2:</strong> How the deep learning face recognition model computes the face embedding.</p>
<p>The FaceNet deep learning model computes a 128-d embedding that quantifies the face itself.</p>
<p>But how does the network actually compute the face embedding?</p>
<p>The answer lies in the training process itself, including:</p>
<ol>
<li> The input data to the network</li>
<li> The triplet loss function</li>
</ol>
<p>To train a face recognition model with deep learning, each input batch of data includes three images:</p>
<ol>
<li> The <em>anchor</em></li>
<li> The <em>positive</em> image</li>
<li> The <em>negative</em> image</li>
</ol>
<p>The anchor is our current face and has identity <em>A</em>.</p>
<p>The second image is our positive image — this image also contains a face of person <em>A</em>.</p>
<p>The negative image, on the other hand, <em><strong>does not have the same identity</strong></em>, and could belong to person <em>B</em>, <em>C</em>, or even <em>Y</em>!</p>
<p>The point is that the anchor and positive image both belong to the same person/face while the negative image does not contain the same face.</p>
<p>The neural network computes the 128-d embeddings for each face and then tweaks the weights of the network (via the triplet loss function) such that:</p>
<ol>
<li> The 128-d embeddings of the anchor and positive image lie closer together</li>
<li> While at the same time, pushing the embeddings for the negative image father away</li>
</ol>
<p>In this manner, the network is able to learn to quantify faces and return highly robust and discriminating embeddings suitable for face recognition.</p>
<p><strong>And furthermore, we can actually</strong> <em>reuse</em> <strong>the OpenFace model for our own applications without having to explicitly train it!</strong></p>
<p>Even though the deep learning model we’re using today has (very likely) <em>never</em> seen the faces we’re about to pass through it, the model will still be able to compute embeddings for each face — ideally, these face embeddings will be sufficiently different such that we can train a “standard” machine learning classifier (SVM, SGD classifier, Random Forest, etc.) on top of the face embeddings, and therefore obtain our OpenCV face recognition pipeline.</p>
<p>If you are interested in learning more about the details surrounding triplet loss and how it can be used to train a face embedding model, be sure to refer to my <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">previous blog post</a> as well as the <a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/app/1A_089.pdf">Schroff et al. publication</a>.</p>
<h4 id="Our-face-recognition-dataset"><a href="#Our-face-recognition-dataset" class="headerlink" title="Our face recognition dataset"></a>Our face recognition dataset</h4><p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_dataset.png"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_dataset.png?size=600x154&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 3:</strong> A small example face dataset for face recognition with OpenCV.</p>
<p>The dataset we are using today contains three people:</p>
<ul>
<li>  Myself</li>
<li>  Trisha (my wife)</li>
<li>  “Unknown”, which is used to represent faces of people we do not know and wish to label as such (here I just sampled faces from the movie <em>Jurassic Park</em> which I used in a <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">previous post</a> — you may want to insert your own “unknown” dataset).</li>
</ul>
<p>As I mentioned in the introduction to today’s face recognition post, I was just married over the weekend, so this post is a “gift” to my new wife ?.</p>
<p>Each class contains a total of six images.</p>
<p>If you are <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/11/how-to-build-a-custom-face-recognition-dataset/">building your own face recognition dataset</a>, ideally, I would suggest having 10-20 images per person you wish to recognize — be sure to refer to the <em>“Drawbacks, limitations, and how to obtain higher face recognition accuracy”</em> section of this blog post for more details.</p>
<h4 id="Project-structure"><a href="#Project-structure" class="headerlink" title="Project structure"></a>Project structure</h4><p>Once you’ve grabbed the zip from the <em><strong>“Downloads”</strong></em> section of this post, go ahead and unzip the archive and navigate into the directory.</p>
<p>From there, you may use the <code>tree</code> command to have the directory structure printed in your terminal:</p>
<p>$ tree –dirsfirst<br>.<br>├── dataset<br>│   ├── adrian [6 images]<br>│   ├── trisha [6 images]<br>│   └── unknown [6 images]<br>├── images<br>│   ├── adrian.jpg<br>│   ├── patrick_bateman.jpg<br>│   └── trisha_adrian.jpg<br>├── face_detection_model<br>│   ├── deploy.prototxt<br>│   └── res10_300x300_ssd_iter_140000.caffemodel<br>├── output<br>│   ├── embeddings.pickle<br>│   ├── le.pickle<br>│   └── recognizer.pickle<br>├── extract_embeddings.py<br>├── openface_nn4.small2.v1.t7<br>├── train_model.py<br>├── recognize.py<br>└── recognize_video.py</p>
<p>7 directories, 31 files</p>
<p>There are quite a few moving parts for this project — <strong>take the time now to carefully read this section so you become familiar with all the files in today’s project.</strong></p>
<p>Our project has four directories in the root folder:</p>
<ul>
<li>  <code>dataset/</code> : Contains our face images organized into subfolders by name.</li>
<li>  <code>images/</code> : Contains three test images that we’ll use to verify the operation of our model.</li>
<li>  <code>face_detection_model/</code> : Contains a pre-trained Caffe deep learning model provided by OpenCV to <em>detect</em> faces. This model <em>detects</em> and <em>localizes</em> faces in an image.</li>
<li><code>output/</code> : Contains my output pickle files. If you’re working with your own dataset, you can store your output files here as well. The output files include:<ul>
<li>  <code>embeddings.pickle</code> : A serialized facial embeddings file. Embeddings have been computed for every face in the dataset and are stored in this file.</li>
<li>  <code>le.pickle</code> : Our label encoder. Contains the name labels for the people that our model can recognize.</li>
<li>  <code>recognizer.pickle</code> : Our Linear Support Vector Machine (SVM) model. This is a machine learning model rather than a deep learning model and it is responsible for actually <em>recognizing</em> faces.</li>
</ul>
</li>
</ul>
<p>Let’s summarize the five files in the root directory:</p>
<ul>
<li>  <code>extract_embeddings.py</code> : We’ll review this file in <strong>Step #1</strong> which is responsible for using a deep learning feature extractor to generate a 128-D vector describing a face. All faces in our dataset will be passed through the neural network to generate embeddings.</li>
<li>  <code>openface_nn4.small2.v1.t7</code> : A Torch deep learning model which produces the 128-D facial embeddings. We’ll be using this deep learning model in <strong>Steps #1, #2, and #3</strong> as well as the <strong>Bonus</strong> section.</li>
<li>  <code>train_model.py</code> : Our Linear SVM model will be trained by this script in <strong>Step #2</strong>. We’ll <em>detect</em> faces, <em>extract</em> embeddings, and <em>fit</em> our SVM model to the embeddings data.</li>
<li>  <code>recognize.py</code> : In <strong>Step #3</strong> and we’ll <em>recognize</em> faces in images. We’ll <em>detect</em> faces, <em>extract</em> embeddings, and <em>query</em> our SVM model to determine <em>who</em> is in an image. We’ll draw boxes around faces and annotate each box with a name.</li>
<li>  <code>recognize_video.py</code> : Our <strong>Bonus</strong> section describes how to <em>recognize who</em> is in frames of a video stream just as we did in <strong>Step #3</strong> on static images.</li>
</ul>
<p>Let’s move on to the first step!</p>
<h4 id="Step-1-Extract-embeddings-from-face-dataset"><a href="#Step-1-Extract-embeddings-from-face-dataset" class="headerlink" title="Step #1: Extract embeddings from face dataset"></a>Step #1: Extract embeddings from face dataset</h4><p>Now that we understand how face recognition works and reviewed our project structure, let’s get started building our OpenCV face recognition pipeline.</p>
<p>Open up the <code>extract_embeddings.py</code> file and insert the following code:</p>
<h2 id="import-the-necessary-packages"><a href="#import-the-necessary-packages" class="headerlink" title="import the necessary packages"></a>import the necessary packages</h2><p>from imutils import paths<br>import numpy as np<br>import argparse<br>import imutils<br>import pickle<br>import cv2<br>import os</p>
<h2 id="construct-the-argument-parser-and-parse-the-arguments"><a href="#construct-the-argument-parser-and-parse-the-arguments" class="headerlink" title="construct the argument parser and parse the arguments"></a>construct the argument parser and parse the arguments</h2><p>ap = argparse.ArgumentParser()<br>ap.add_argument(“-i”, “–dataset”, required=True,<br>    help=”path to input directory of faces + images”)<br>ap.add_argument(“-e”, “–embeddings”, required=True,<br>    help=”path to output serialized db of facial embeddings”)<br>ap.add_argument(“-d”, “–detector”, required=True,<br>    help=”path to OpenCV’s deep learning face detector”)<br>ap.add_argument(“-m”, “–embedding-model”, required=True,<br>    help=”path to OpenCV’s deep learning face embedding model”)<br>ap.add_argument(“-c”, “–confidence”, type=float, default=0.5,<br>    help=”minimum probability to filter weak detections”)<br>args = vars(ap.parse_args())</p>
<p>We import our required packages on <strong>Lines 2-8</strong>. You’ll need to have OpenCV and <code>imutils</code> installed. To install OpenCV, <a target="_blank" rel="noopener" href="https://pyimagesearch.com/opencv-tutorials-resources-guides/">simply follow one of my guides</a> (I recommend OpenCV 3.4.2, so be sure to download the right version while you follow along). My <a target="_blank" rel="noopener" href="https://github.com/jrosebr1/imutils">imutils</a> package can be installed with pip:</p>
<p>$ pip install –upgrade imutils</p>
<p>Next, we process our command line arguments:</p>
<ul>
<li>  <code>--dataset</code> : The path to our input dataset of face images.</li>
<li>  <code>--embeddings</code> : The path to our output embeddings file. Our script will compute face embeddings which we’ll serialize to disk.</li>
<li>  <code>--detector</code> : Path to OpenCV’s Caffe-based deep learning face detector used to actually <em>localize</em> the faces in the images.</li>
<li>  <code>--embedding-model</code> : Path to the OpenCV deep learning Torch embedding model. This model will allow us to <em>extract</em> a 128-D facial embedding vector.</li>
<li>  <code>--confidence</code> : Optional threshold for filtering week face detections.</li>
</ul>
<p>Now that we’ve imported our packages and parsed command line arguments, lets load the face detector and embedder from disk:</p>
<h2 id="load-our-serialized-face-detector-from-disk"><a href="#load-our-serialized-face-detector-from-disk" class="headerlink" title="load our serialized face detector from disk"></a>load our serialized face detector from disk</h2><p>print(“[INFO] loading face detector…”)<br>protoPath = os.path.sep.join([args[“detector”], “deploy.prototxt”])<br>modelPath = os.path.sep.join([args[“detector”],<br>    “res10_300x300_ssd_iter_140000.caffemodel”])<br>detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)</p>
<h2 id="load-our-serialized-face-embedding-model-from-disk"><a href="#load-our-serialized-face-embedding-model-from-disk" class="headerlink" title="load our serialized face embedding model from disk"></a>load our serialized face embedding model from disk</h2><p>print(“[INFO] loading face recognizer…”)<br>embedder = cv2.dnn.readNetFromTorch(args[“embedding_model”])</p>
<p>Here we load the face detector and embedder:</p>
<ul>
<li>  <code>detector</code> : Loaded via <strong>Lines 26-29</strong>. We’re using a Caffe based DL face detector to <em>localize</em> faces in an image.</li>
<li>  <code>embedder</code> : Loaded on <strong>Line 33</strong>. This model is Torch-based and is responsible for <em>extracting</em> facial embeddings via deep learning feature extraction.</li>
</ul>
<p>Notice that we’re using the respective <code>cv2.dnn</code> functions to load the two separate models. The <code>dnn</code> module wasn’t made available like this until <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/08/21/deep-learning-with-opencv/">OpenCV 3.3</a>, but I recommend that you are <a target="_blank" rel="noopener" href="https://pyimagesearch.com/opencv-tutorials-resources-guides/">using OpenCV 3.4.2 or higher</a> for this blog post.</p>
<p>Moving forward, let’s grab our image paths and perform initializations:</p>
<h2 id="grab-the-paths-to-the-input-images-in-our-dataset"><a href="#grab-the-paths-to-the-input-images-in-our-dataset" class="headerlink" title="grab the paths to the input images in our dataset"></a>grab the paths to the input images in our dataset</h2><p>print(“[INFO] quantifying faces…”)<br>imagePaths = list(paths.list_images(args[“dataset”]))</p>
<h2 id="initialize-our-lists-of-extracted-facial-embeddings-and"><a href="#initialize-our-lists-of-extracted-facial-embeddings-and" class="headerlink" title="initialize our lists of extracted facial embeddings and"></a>initialize our lists of extracted facial embeddings and</h2><h2 id="corresponding-people-names"><a href="#corresponding-people-names" class="headerlink" title="corresponding people names"></a>corresponding people names</h2><p>knownEmbeddings = []<br>knownNames = []</p>
<h2 id="initialize-the-total-number-of-faces-processed"><a href="#initialize-the-total-number-of-faces-processed" class="headerlink" title="initialize the total number of faces processed"></a>initialize the total number of faces processed</h2><p>total = 0</p>
<p>The <code>imagePaths</code> list, built on <strong>Line 37</strong>, contains the path to each image in the dataset. I’ve made this easy via my <code>imutils</code> function, <code>paths.list_images</code> .</p>
<p>Our embeddings and corresponding names will be held in two lists: <code>knownEmbeddings</code> and <code>knownNames</code> (<strong>Lines 41 and 42</strong>).</p>
<p>We’ll also be keeping track of how many faces we’ve processed via a variable called <code>total</code> (<strong>Line 45</strong>).</p>
<p>Let’s begin looping over the image paths — this loop will be responsible for extracting embeddings from faces found in each image:</p>
<h2 id="loop-over-the-image-paths"><a href="#loop-over-the-image-paths" class="headerlink" title="loop over the image paths"></a>loop over the image paths</h2><p>for (i, imagePath) in enumerate(imagePaths):<br>    ## extract the person name from the image path<br>    print(“[INFO] processing image {}/{}”.format(i + 1,<br>        len(imagePaths)))<br>    name = imagePath.split(os.path.sep)[-2]</p>
<pre><code>## load the image, resize it to have a width of 600 pixels (while
## maintaining the aspect ratio), and then grab the image
## dimensions
image = cv2.imread(imagePath)
image = imutils.resize(image, width=600)
(h, w) = image.shape[:2]</code></pre>
<p>We begin looping over <code>imagePaths</code> on <strong>Line 48</strong>.</p>
<p>First, we extract the <code>name</code> of the person from the path (<strong>Line 52</strong>). To explain how this works, consider the following example in my Python shell:</p>
<p>$ python</p>
<blockquote>
<blockquote>
<blockquote>
<p>from imutils import paths<br>import os<br>imagePaths = list(paths.list_images(“dataset”))<br>imagePath = imagePaths[0]<br>imagePath<br>‘dataset/adrian/00004.jpg’<br>imagePath.split(os.path.sep)<br>[‘dataset’, ‘adrian’, ‘00004.jpg’]<br>imagePath.split(os.path.sep)[-2]<br>‘adrian’</p>
</blockquote>
</blockquote>
</blockquote>
<p>Notice how by using <code>imagePath.split</code> and providing the split character (the OS path separator — “/” on unix and “\” on Windows), the function produces a list of folder/file names (strings) which walk down the directory tree. We grab the second-to-last index, the persons <code>name</code> , which in this case is <code>&#39;adrian&#39;</code> .</p>
<p>Finally, we wrap up the above code block by loading the <code>image</code> and <code>resize</code> it to a known <code>width</code> (<strong>Lines 57 and 58</strong>).</p>
<p>Let’s detect and localize faces:</p>
<pre><code>## construct a blob from the image
imageBlob = cv2.dnn.blobFromImage(
    cv2.resize(image, (300, 300)), 1.0, (300, 300),
    (104.0, 177.0, 123.0), swapRB=False, crop=False)

## apply OpenCV&#39;s deep learning-based face detector to localize
## faces in the input image
detector.setInput(imageBlob)
detections = detector.forward()</code></pre>
<p>On <strong>Lines 62-64</strong>, we construct a blob. To learn more about this process, please read <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/"><em>Deep learning: How OpenCV’s blobFromImage works</em></a>.</p>
<p>From there we detect faces in the image by passing the <code>imageBlob</code> through the <code>detector</code> network (<strong>Lines 68 and 69</strong>).</p>
<p>Let’s process the <code>detections</code> :</p>
<pre><code>## ensure at least one face was found
if len(detections) &gt; 0:
    ## we&#39;re making the assumption that each image has only ONE
    ## face, so find the bounding box with the largest probability
    i = np.argmax(detections[0, 0, :, 2])
    confidence = detections[0, 0, i, 2]

    ## ensure that the detection with the largest probability also
    ## means our minimum probability test (thus helping filter out
    ## weak detections)
    if confidence &gt; args[&quot;confidence&quot;]:
        ## compute the (x, y)-coordinates of the bounding box for
        ## the face
        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
        (startX, startY, endX, endY) = box.astype(&quot;int&quot;)

        ## extract the face ROI and grab the ROI dimensions
        face = image[startY:endY, startX:endX]
        (fH, fW) = face.shape[:2]

        ## ensure the face width and height are sufficiently large
        if fW &lt; 20 or fH &lt; 20:
            continue</code></pre>
<p>The <code>detections</code> list contains probabilities and coordinates to localize faces in an image.</p>
<p>Assuming we have at least one detection, we’ll proceed into the body of the if-statement (<strong>Line 72</strong>).</p>
<p>We make the assumption that there is only <em>one</em> face in the image, so we extract the detection with the highest <code>confidence</code> and check to make sure that the confidence meets the minimum probability threshold used to filter out weak detections (<strong>Lines 75-81</strong>).</p>
<p>Assuming we’ve met that threshold, we extract the <code>face</code> ROI and grab/check dimensions to make sure the <code>face</code> ROI is sufficiently large (<strong>Lines 84-93</strong>).</p>
<p>From there, we’ll take advantage of our <code>embedder</code> CNN and extract the face embeddings:</p>
<pre><code>        ## construct a blob for the face ROI, then pass the blob
        ## through our face embedding model to obtain the 128-d
        ## quantification of the face
        faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,
            (96, 96), (0, 0, 0), swapRB=True, crop=False)
        embedder.setInput(faceBlob)
        vec = embedder.forward()

        ## add the name of the person + corresponding face
        ## embedding to their respective lists
        knownNames.append(name)
        knownEmbeddings.append(vec.flatten())
        total += 1</code></pre>
<p>We construct another blob, this time from the face ROI (not the whole image as we did before) on <strong>Lines 98 and 99</strong>.</p>
<p>Subsequently, we pass the <code>faceBlob</code> through the embedder CNN (<strong>Lines 100 and 101</strong>). This generates a 128-D vector (<code>vec</code> ) which describes the face. We’ll leverage this data to recognize new faces via machine learning.</p>
<p>And then we simply add the <code>name</code> and embedding <code>vec</code> to <code>knownNames</code> and <code>knownEmbeddings</code> , respectively (<strong>Lines 105 and 106</strong>).</p>
<p>We also can’t forget about the variable we set to track the <code>total</code> number of faces either — we go ahead and increment the value on <strong>Line 107</strong>.</p>
<p>We continue this process of looping over images, detecting faces, and extracting face embeddings for <em>each and every image</em> in our dataset.</p>
<p>All that’s left when the loop finishes is to dump the data to disk:</p>
<h2 id="dump-the-facial-embeddings-names-to-disk"><a href="#dump-the-facial-embeddings-names-to-disk" class="headerlink" title="dump the facial embeddings + names to disk"></a>dump the facial embeddings + names to disk</h2><p>print(“[INFO] serializing {} encodings…”.format(total))<br>data = {“embeddings”: knownEmbeddings, “names”: knownNames}<br>f = open(args[“embeddings”], “wb”)<br>f.write(pickle.dumps(data))<br>f.close()</p>
<p>We add the name and embedding data to a dictionary and then serialize the <code>data</code> in a pickle file on <strong>Lines 110-114</strong>.</p>
<p>At this point we’re ready to extract embeddings by running our script.</p>
<p>To follow along with this face recognition tutorial, use the <em><strong>“Downloads”</strong></em> section of the post to download the source code, OpenCV models, and example face recognition dataset.</p>
<p>From there, open up a terminal and execute the following command to compute the face embeddings with OpenCV:</p>
<p>$ python extract_embeddings.py –dataset dataset <br>    –embeddings output/embeddings.pickle <br>    –detector face_detection_model <br>    –embedding-model openface_nn4.small2.v1.t7<br>[INFO] loading face detector…<br>[INFO] loading face recognizer…<br>[INFO] quantifying faces…<br>[INFO] processing image 1/18<br>[INFO] processing image 2/18<br>[INFO] processing image 3/18<br>[INFO] processing image 4/18<br>[INFO] processing image 5/18<br>[INFO] processing image 6/18<br>[INFO] processing image 7/18<br>[INFO] processing image 8/18<br>[INFO] processing image 9/18<br>[INFO] processing image 10/18<br>[INFO] processing image 11/18<br>[INFO] processing image 12/18<br>[INFO] processing image 13/18<br>[INFO] processing image 14/18<br>[INFO] processing image 15/18<br>[INFO] processing image 16/18<br>[INFO] processing image 17/18<br>[INFO] processing image 18/18<br>[INFO] serializing 18 encodings…</p>
<p>Here you can see that we have extracted 18 face embeddings, one for each of the images (6 per class) in our input face dataset.</p>
<h4 id="Step-2-Train-face-recognition-model"><a href="#Step-2-Train-face-recognition-model" class="headerlink" title="Step #2: Train face recognition model"></a>Step #2: Train face recognition model</h4><p>At this point we have extracted 128-d embeddings for each face — <em>but how do we actually recognize a person based on these embeddings?</em> The answer is that we need to train a “standard” machine learning model (such as an SVM, k-NN classifier, Random Forest, etc.) on top of the embeddings.</p>
<p>In my <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">previous face recognition tutorial</a> we discovered how a modified version of k-NN can be used for face recognition on 128-d embeddings created via the <a target="_blank" rel="noopener" href="http://dlib.net/">dlib</a> and <a target="_blank" rel="noopener" href="https://github.com/ageitgey/face_recognition">face_recognition</a> libraries.</p>
<p>Today, I want to share how we can build a more powerful classifier on top of the embeddings — you’ll be able to use this same method in your dlib-based face recognition pipelines as well if you are so inclined.</p>
<p>Open up the <code>train_model.py</code> file and insert the following code:</p>
<h2 id="import-the-necessary-packages-1"><a href="#import-the-necessary-packages-1" class="headerlink" title="import the necessary packages"></a>import the necessary packages</h2><p>from sklearn.preprocessing import LabelEncoder<br>from sklearn.svm import SVC<br>import argparse<br>import pickle</p>
<h2 id="construct-the-argument-parser-and-parse-the-arguments-1"><a href="#construct-the-argument-parser-and-parse-the-arguments-1" class="headerlink" title="construct the argument parser and parse the arguments"></a>construct the argument parser and parse the arguments</h2><p>ap = argparse.ArgumentParser()<br>ap.add_argument(“-e”, “–embeddings”, required=True,<br>    help=”path to serialized db of facial embeddings”)<br>ap.add_argument(“-r”, “–recognizer”, required=True,<br>    help=”path to output model trained to recognize faces”)<br>ap.add_argument(“-l”, “–le”, required=True,<br>    help=”path to output label encoder”)<br>args = vars(ap.parse_args())</p>
<p>We’ll need <a target="_blank" rel="noopener" href="http://scikit-learn.org/stable/">scikit-learn</a>, a machine learning library, installed in our environment prior to running this script. You can install it via pip:</p>
<p>$ pip install scikit-learn</p>
<p>We import our packages and modules on <strong>Lines 2-5</strong>. We’ll be using scikit-learn’s implementation of Support Vector Machines (SVM), a common machine learning model.</p>
<p>From there we parse our command line arguments:</p>
<ul>
<li>  <code>--embeddings</code> : The path to the serialized embeddings (we exported it by running the previous <code>extract_embeddings.py</code> script).</li>
<li>  <code>--recognizer</code> : This will be our output model that <em>recognizes</em> faces. It is based on SVM. We’ll be saving it so we can use it in the next two recognition scripts.</li>
<li>  <code>--le</code> : Our label encoder output file path. We’ll serialize our label encoder to disk so that we can use it and the recognizer model in our image/video face recognition scripts.</li>
</ul>
<p>Each of these arguments is <em>required</em>.</p>
<p>Let’s load our facial embeddings and encode our labels:</p>
<h2 id="load-the-face-embeddings"><a href="#load-the-face-embeddings" class="headerlink" title="load the face embeddings"></a>load the face embeddings</h2><p>print(“[INFO] loading face embeddings…”)<br>data = pickle.loads(open(args[“embeddings”], “rb”).read())</p>
<h2 id="encode-the-labels"><a href="#encode-the-labels" class="headerlink" title="encode the labels"></a>encode the labels</h2><p>print(“[INFO] encoding labels…”)<br>le = LabelEncoder()<br>labels = le.fit_transform(data[“names”])</p>
<p>Here we load our embeddings from <strong>Step #1</strong> on <strong>Line 19</strong>. We won’t be generating any embeddings in this model training script — we’ll use the embeddings previously generated and serialized.</p>
<p>Then we initialize our scikit-learn <code>LabelEncoder</code> and encode our name <code>labels</code> (<strong>Lines 23 and 24</strong>).</p>
<p>Now it’s time to train our SVM model for recognizing faces:</p>
<h2 id="train-the-model-used-to-accept-the-128-d-embeddings-of-the-face-and"><a href="#train-the-model-used-to-accept-the-128-d-embeddings-of-the-face-and" class="headerlink" title="train the model used to accept the 128-d embeddings of the face and"></a>train the model used to accept the 128-d embeddings of the face and</h2><h2 id="then-produce-the-actual-face-recognition"><a href="#then-produce-the-actual-face-recognition" class="headerlink" title="then produce the actual face recognition"></a>then produce the actual face recognition</h2><p>print(“[INFO] training model…”)<br>recognizer = SVC(C=1.0, kernel=”linear”, probability=True)<br>recognizer.fit(data[“embeddings”], labels)</p>
<p>On <strong>Line 29</strong> we initialize our SVM model, and on <strong>Line 30</strong> we <code>fit</code> the model (also known as “training the model”).</p>
<p>Here we are using a Linear Support Vector Machine (SVM) but you can try experimenting with other machine learning models if you so wish.</p>
<p>After training the model we output the model and label encoder to disk as pickle files.</p>
<h2 id="write-the-actual-face-recognition-model-to-disk"><a href="#write-the-actual-face-recognition-model-to-disk" class="headerlink" title="write the actual face recognition model to disk"></a>write the actual face recognition model to disk</h2><p>f = open(args[“recognizer”], “wb”)<br>f.write(pickle.dumps(recognizer))<br>f.close()</p>
<h2 id="write-the-label-encoder-to-disk"><a href="#write-the-label-encoder-to-disk" class="headerlink" title="write the label encoder to disk"></a>write the label encoder to disk</h2><p>f = open(args[“le”], “wb”)<br>f.write(pickle.dumps(le))<br>f.close()</p>
<p>We write two pickle files to disk in this block — the <em>face recognizer model</em> and the <em>label encoder</em>.</p>
<p>At this point, be sure you executed the code from <strong>Step #1</strong> first. You can grab the zip containing the code and data from the <em><strong>“Downloads”</strong></em> section.</p>
<p>Now that we have finished coding <code>train_model.py</code> as well, let’s apply it to our extracted face embeddings:</p>
<p>$ python train_model.py –embeddings output/embeddings.pickle <br>    –recognizer output/recognizer.pickle <br>    –le output/le.pickle<br>[INFO] loading face embeddings…<br>[INFO] encoding labels…<br>[INFO] training model…<br>$ ls output/<br>embeddings.pickle    le.pickle        recognizer.pickle</p>
<p>Here you can see that our SVM has been trained on the embeddings and both the (1) SVM itself and (2) the label encoding have been written to disk, enabling us to apply them to input images and video.</p>
<h4 id="Step-3-Recognize-faces-with-OpenCV"><a href="#Step-3-Recognize-faces-with-OpenCV" class="headerlink" title="Step #3: Recognize faces with OpenCV"></a>Step #3: Recognize faces with OpenCV</h4><p>We are now ready to perform face recognition with OpenCV!</p>
<p>We’ll start with recognizing faces in images in this section and then move on to recognizing faces in video streams in the following section.</p>
<p>Open up the <code>recognize.py</code> file in your project and insert the following code:</p>
<h2 id="import-the-necessary-packages-2"><a href="#import-the-necessary-packages-2" class="headerlink" title="import the necessary packages"></a>import the necessary packages</h2><p>import numpy as np<br>import argparse<br>import imutils<br>import pickle<br>import cv2<br>import os</p>
<h2 id="construct-the-argument-parser-and-parse-the-arguments-2"><a href="#construct-the-argument-parser-and-parse-the-arguments-2" class="headerlink" title="construct the argument parser and parse the arguments"></a>construct the argument parser and parse the arguments</h2><p>ap = argparse.ArgumentParser()<br>ap.add_argument(“-i”, “–image”, required=True,<br>    help=”path to input image”)<br>ap.add_argument(“-d”, “–detector”, required=True,<br>    help=”path to OpenCV’s deep learning face detector”)<br>ap.add_argument(“-m”, “–embedding-model”, required=True,<br>    help=”path to OpenCV’s deep learning face embedding model”)<br>ap.add_argument(“-r”, “–recognizer”, required=True,<br>    help=”path to model trained to recognize faces”)<br>ap.add_argument(“-l”, “–le”, required=True,<br>    help=”path to label encoder”)<br>ap.add_argument(“-c”, “–confidence”, type=float, default=0.5,<br>    help=”minimum probability to filter weak detections”)<br>args = vars(ap.parse_args())</p>
<p>We <code>import</code> our required packages on <strong>Lines 2-7</strong>. At this point, you should have each of these packages installed.</p>
<p>Our six command line arguments are parsed on <strong>Lines 10-23</strong>:</p>
<ul>
<li>  <code>--image</code> : The path to the input image. We will attempt to recognize the faces in this image.</li>
<li>  <code>--detector</code> : The path to OpenCV’s deep learning face detector. We’ll use this model to <em>detect</em> where in the image the face ROIs are.</li>
<li>  <code>--embedding-model</code> : The path to OpenCV’s deep learning face embedding model. We’ll use this model to <em>extract</em> the 128-D face embedding from the face ROI — we’ll feed the data into the recognizer.</li>
<li>  <code>--recognizer</code> : The path to our recognizer model. We trained our SVM recognizer in <strong>Step #2</strong>. This is what will actually <em>determine who</em> a face is.</li>
<li>  <code>--le</code> : The path to our label encoder. This contains our face labels such as <code>&#39;adrian&#39;</code> or <code>&#39;trisha&#39;</code> .</li>
<li>  <code>--confidence</code> : The optional threshold to filter weak face <em>detections</em>.</li>
</ul>
<p>Be sure to study these command line arguments — it is important to know the difference between the two deep learning models and the SVM model. If you find yourself confused later in this script, you should refer back to here.</p>
<p>Now that we’ve handled our imports and command line arguments, let’s load the three models from disk into memory:</p>
<h2 id="load-our-serialized-face-detector-from-disk-1"><a href="#load-our-serialized-face-detector-from-disk-1" class="headerlink" title="load our serialized face detector from disk"></a>load our serialized face detector from disk</h2><p>print(“[INFO] loading face detector…”)<br>protoPath = os.path.sep.join([args[“detector”], “deploy.prototxt”])<br>modelPath = os.path.sep.join([args[“detector”],<br>    “res10_300x300_ssd_iter_140000.caffemodel”])<br>detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)</p>
<h2 id="load-our-serialized-face-embedding-model-from-disk-1"><a href="#load-our-serialized-face-embedding-model-from-disk-1" class="headerlink" title="load our serialized face embedding model from disk"></a>load our serialized face embedding model from disk</h2><p>print(“[INFO] loading face recognizer…”)<br>embedder = cv2.dnn.readNetFromTorch(args[“embedding_model”])</p>
<h2 id="load-the-actual-face-recognition-model-along-with-the-label-encoder"><a href="#load-the-actual-face-recognition-model-along-with-the-label-encoder" class="headerlink" title="load the actual face recognition model along with the label encoder"></a>load the actual face recognition model along with the label encoder</h2><p>recognizer = pickle.loads(open(args[“recognizer”], “rb”).read())<br>le = pickle.loads(open(args[“le”], “rb”).read())</p>
<p>We load three models in this block. At the risk of being redundant, I want to explicitly remind you of the differences among the models:</p>
<ol>
<li> <code>detector</code> : A <em>pre-trained</em> Caffe DL model to <em>detect where in the image the faces are</em> (<strong>Lines 27-30</strong>).</li>
<li> <code>embedder</code> : A <em>pre-trained</em> Torch DL model to <em>calculate our 128-D face embeddings</em> (<strong>Line 34</strong>).</li>
<li> <code>recognizer</code> : Our Linear SVM <em>face recognition</em> model (<strong>Line 37</strong>). We trained this model in <strong>Step 2</strong>.</li>
</ol>
<p>Both 1 &amp; 2 are <em>pre-trained</em> meaning that they are provided to you as-is by OpenCV. They are buried in the OpenCV project on GitHub, but I’ve included them for your convenience in the <em><strong>“Downloads”</strong></em> section of today’s post. I’ve also numbered the models in the order that we’ll apply them to recognize faces with OpenCV.</p>
<p>We also load our label encoder which holds the names of the people our model can recognize (<strong>Line 38</strong>).</p>
<p>Now let’s load our image and <em>detect</em> faces:</p>
<h2 id="load-the-image-resize-it-to-have-a-width-of-600-pixels-while"><a href="#load-the-image-resize-it-to-have-a-width-of-600-pixels-while" class="headerlink" title="load the image, resize it to have a width of 600 pixels (while"></a>load the image, resize it to have a width of 600 pixels (while</h2><h2 id="maintaining-the-aspect-ratio-and-then-grab-the-image-dimensions"><a href="#maintaining-the-aspect-ratio-and-then-grab-the-image-dimensions" class="headerlink" title="maintaining the aspect ratio), and then grab the image dimensions"></a>maintaining the aspect ratio), and then grab the image dimensions</h2><p>image = cv2.imread(args[“image”])<br>image = imutils.resize(image, width=600)<br>(h, w) = image.shape[:2]</p>
<h2 id="construct-a-blob-from-the-image"><a href="#construct-a-blob-from-the-image" class="headerlink" title="construct a blob from the image"></a>construct a blob from the image</h2><p>imageBlob = cv2.dnn.blobFromImage(<br>    cv2.resize(image, (300, 300)), 1.0, (300, 300),<br>    (104.0, 177.0, 123.0), swapRB=False, crop=False)</p>
<h2 id="apply-OpenCV’s-deep-learning-based-face-detector-to-localize"><a href="#apply-OpenCV’s-deep-learning-based-face-detector-to-localize" class="headerlink" title="apply OpenCV’s deep learning-based face detector to localize"></a>apply OpenCV’s deep learning-based face detector to localize</h2><h2 id="faces-in-the-input-image"><a href="#faces-in-the-input-image" class="headerlink" title="faces in the input image"></a>faces in the input image</h2><p>detector.setInput(imageBlob)<br>detections = detector.forward()</p>
<p>Here we:</p>
<ul>
<li>  Load the image into memory and construct a blob (<strong>Lines 42-49</strong>). Learn about <code>cv2.dnn.blobFromImage</code> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/">here</a>.</li>
<li>  Localize faces in the image via our <code>detector</code> (<strong>Lines 53 and 54</strong>).</li>
</ul>
<p>Given our new <code>detections</code> , let’s recognize faces in the image. But first we need to filter weak <code>detections</code> and extract the <code>face</code> ROI:</p>
<h2 id="loop-over-the-detections"><a href="#loop-over-the-detections" class="headerlink" title="loop over the detections"></a>loop over the detections</h2><p>for i in range(0, detections.shape[2]):<br>    ## extract the confidence (i.e., probability) associated with the<br>    ## prediction<br>    confidence = detections[0, 0, i, 2]</p>
<pre><code>## filter out weak detections
if confidence &gt; args[&quot;confidence&quot;]:
    ## compute the (x, y)-coordinates of the bounding box for the
    ## face
    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
    (startX, startY, endX, endY) = box.astype(&quot;int&quot;)

    ## extract the face ROI
    face = image[startY:endY, startX:endX]
    (fH, fW) = face.shape[:2]

    ## ensure the face width and height are sufficiently large
    if fW &lt; 20 or fH &lt; 20:
        continue</code></pre>
<p>You’ll recognize this block from <strong>Step #1</strong>. I’ll explain it here once more:</p>
<ul>
<li>  We loop over the <code>detections</code> on <strong>Line 57</strong> and extract the <code>confidence</code> of each on <strong>Line 60</strong>.</li>
<li>  Then we compare the <code>confidence</code> to the minimum probability detection threshold contained in our command line <code>args</code> dictionary, ensuring that the computed probability is larger than the minimum probability (<strong>Line 63</strong>).</li>
<li>  From there, we extract the <code>face</code> ROI (<strong>Lines 66-70</strong>) as well as ensure it’s spatial dimensions are sufficiently large (<strong>Lines 74 and 75</strong>).</li>
</ul>
<p>Recognizing the name of the <code>face</code> ROI requires just a few steps:</p>
<pre><code>    ## construct a blob for the face ROI, then pass the blob
    ## through our face embedding model to obtain the 128-d
    ## quantification of the face
    faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96),
        (0, 0, 0), swapRB=True, crop=False)
    embedder.setInput(faceBlob)
    vec = embedder.forward()

    ## perform classification to recognize the face
    preds = recognizer.predict_proba(vec)[0]
    j = np.argmax(preds)
    proba = preds[j]
    name = le.classes_[j]</code></pre>
<p>First, we construct a <code>faceBlob</code> (from the <code>face</code> ROI) and pass it through the <code>embedder</code> to generate a 128-D vector which describes the face (<strong>Lines 80-83</strong>)</p>
<p>Then, we pass the <code>vec</code> through our SVM recognizer model (<strong>Line 86</strong>), the result of which is our predictions for <em>who</em> is in the face ROI.</p>
<p>We take the highest probability index (<strong>Line 87</strong>) and query our label encoder to find the <code>name</code> (<strong>Line 89</strong>). In between, I extract the probability on <strong>Line 88</strong>.</p>
<p><em><strong>Note:</strong> You cam further filter out weak face recognitions by applying an additional threshold test on the probability. For example, inserting <code>if proba &lt; T</code> (where <code>T</code> is a variable you define) can provide an additional layer of filtering to ensure there are less false-positive face recognitions.</em></p>
<p>Now, let’s display OpenCV face recognition results:</p>
<pre><code>    ## draw the bounding box of the face along with the associated
    ## probability
    text = &quot;&#123;&#125;: &#123;:.2f&#125;%&quot;.format(name, proba * 100)
    y = startY - 10 if startY - 10 &gt; 10 else startY + 10
    cv2.rectangle(image, (startX, startY), (endX, endY),
        (0, 0, 255), 2)
    cv2.putText(image, text, (startX, y),
        cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)</code></pre>
<h2 id="show-the-output-image"><a href="#show-the-output-image" class="headerlink" title="show the output image"></a>show the output image</h2><p>cv2.imshow(“Image”, image)<br>cv2.waitKey(0)</p>
<p>For every face we recognize in the loop (including the “unknown”) people:</p>
<ul>
<li>  We construct a <code>text</code> string containing the <code>name</code> and probability on <strong>Line 93</strong>.</li>
<li>  And then we draw a rectangle around the face and place the text above the box (<strong>Lines 94-98</strong>).</li>
</ul>
<p>And then finally we visualize the results on the screen until a key is pressed (<strong>Lines 101 and 102</strong>).</p>
<p>It is time to recognize faces in images with OpenCV!</p>
<p>To apply our OpenCV face recognition pipeline to my provided images (or your own dataset + test images), make sure you use the <em><strong>“Downloads”</strong></em> section of the blog post to download the code, trained models, and example images.</p>
<p>From there, open up a terminal and execute the following command:</p>
<p>$ python recognize.py –detector face_detection_model <br>    –embedding-model openface_nn4.small2.v1.t7 <br>    –recognizer output/recognizer.pickle <br>    –le output/le.pickle <br>    –image images/adrian.jpg<br>[INFO] loading face detector…<br>[INFO] loading face recognizer…</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_result01.jpg"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_result01.jpg?size=500x663&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 4:</strong> OpenCV face recognition has recognized <em>me</em> at the <a target="_blank" rel="noopener" href="https://www.imdb.com/title/tt4881806/"><em>Jurassic World: Fallen Kingdom</em></a> movie showing.</p>
<p>Here you can see me sipping on a beer and sporting one of my favorite <em>Jurassic Park</em> shirts, along with a special <em>Jurassic World</em> pint glass and commemorative book. My face prediction only has 47.15% confidence; however, that confidence is higher than the <em>“Unknown”</em> class.</p>
<p>Let’s try another OpenCV face recognition example:</p>
<p>$ python recognize.py –detector face_detection_model <br>    –embedding-model openface_nn4.small2.v1.t7 <br>    –recognizer output/recognizer.pickle <br>    –le output/le.pickle <br>    –image images/trisha_adrian.jpg<br>[INFO] loading face detector…<br>[INFO] loading face recognizer…</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_result02.jpg"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_result02.jpg?size=500x663&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 5:</strong> My wife, Trisha, and I are recognized in a selfie picture on an airplane with OpenCV + deep learning facial recognition.</p>
<p>Here are Trisha and I, ready to start our vacation!</p>
<p>In a final example, let’s look at what happens when our model is unable to recognize the actual face:</p>
<p>$ python recognize.py –detector face_detection_model <br>    –embedding-model openface_nn4.small2.v1.t7 <br>    –recognizer output/recognizer.pickle <br>    –le output/le.pickle <br>    –image images/patrick_bateman.jpg<br>[INFO] loading face detector…<br>[INFO] loading face recognizer…</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_result03.jpg"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_result03.jpg?size=500x516&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 6:</strong> Facial recognition with OpenCV has determined that this person is “unknown”.</p>
<p>The third image is an example of an “unknown” person who is actually Patrick Bateman from <a target="_blank" rel="noopener" href="https://www.imdb.com/title/tt0144084/"><em>American Psycho</em></a> — believe me, this is not a person you would want to see show up in your images or video streams!</p>
<h4 id="BONUS-Recognize-faces-in-video-streams"><a href="#BONUS-Recognize-faces-in-video-streams" class="headerlink" title="BONUS: Recognize faces in video streams"></a>BONUS: Recognize faces in video streams</h4><p>As a bonus, I decided to include a section dedicated to OpenCV face recognition in video streams!</p>
<p>The actual pipeline itself is near identical to recognizing faces in images, with only a few updates which we’ll review along the way.</p>
<p>Open up the <code>recognize_video.py</code> file and let’s get started:</p>
<h2 id="import-the-necessary-packages-3"><a href="#import-the-necessary-packages-3" class="headerlink" title="import the necessary packages"></a>import the necessary packages</h2><p>from imutils.video import VideoStream<br>from imutils.video import FPS<br>import numpy as np<br>import argparse<br>import imutils<br>import pickle<br>import time<br>import cv2<br>import os</p>
<h2 id="construct-the-argument-parser-and-parse-the-arguments-3"><a href="#construct-the-argument-parser-and-parse-the-arguments-3" class="headerlink" title="construct the argument parser and parse the arguments"></a>construct the argument parser and parse the arguments</h2><p>ap = argparse.ArgumentParser()<br>ap.add_argument(“-d”, “–detector”, required=True,<br>    help=”path to OpenCV’s deep learning face detector”)<br>ap.add_argument(“-m”, “–embedding-model”, required=True,<br>    help=”path to OpenCV’s deep learning face embedding model”)<br>ap.add_argument(“-r”, “–recognizer”, required=True,<br>    help=”path to model trained to recognize faces”)<br>ap.add_argument(“-l”, “–le”, required=True,<br>    help=”path to label encoder”)<br>ap.add_argument(“-c”, “–confidence”, type=float, default=0.5,<br>    help=”minimum probability to filter weak detections”)<br>args = vars(ap.parse_args())</p>
<p>Our imports are the same as the <strong>Step #3</strong> section above, except for <strong>Lines 2 and 3</strong> where we use the <code>imutils.video</code> module. We’ll use <code>VideoStream</code> to capture frames from our camera and <code>FPS</code> to calculate frames per second statistics.</p>
<p>The command line arguments are also the same except we aren’t passing a path to a static image via the command line. Rather, we’ll grab a reference to our webcam and then process the video. Refer to <strong>Step #3</strong> if you need to review the arguments.</p>
<p>Our three models and label encoder are loaded here:</p>
<h2 id="load-our-serialized-face-detector-from-disk-2"><a href="#load-our-serialized-face-detector-from-disk-2" class="headerlink" title="load our serialized face detector from disk"></a>load our serialized face detector from disk</h2><p>print(“[INFO] loading face detector…”)<br>protoPath = os.path.sep.join([args[“detector”], “deploy.prototxt”])<br>modelPath = os.path.sep.join([args[“detector”],<br>    “res10_300x300_ssd_iter_140000.caffemodel”])<br>detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)</p>
<h2 id="load-our-serialized-face-embedding-model-from-disk-2"><a href="#load-our-serialized-face-embedding-model-from-disk-2" class="headerlink" title="load our serialized face embedding model from disk"></a>load our serialized face embedding model from disk</h2><p>print(“[INFO] loading face recognizer…”)<br>embedder = cv2.dnn.readNetFromTorch(args[“embedding_model”])</p>
<h2 id="load-the-actual-face-recognition-model-along-with-the-label-encoder-1"><a href="#load-the-actual-face-recognition-model-along-with-the-label-encoder-1" class="headerlink" title="load the actual face recognition model along with the label encoder"></a>load the actual face recognition model along with the label encoder</h2><p>recognizer = pickle.loads(open(args[“recognizer”], “rb”).read())<br>le = pickle.loads(open(args[“le”], “rb”).read())</p>
<p>Here we load face <code>detector</code> , face <code>embedder</code> model, face <code>recognizer</code> model (Linear SVM), and label encoder.</p>
<p>Again, be sure to refer to <strong>Step #3</strong> if you are confused about the three models or label encoder.</p>
<p>Let’s initialize our video stream and begin processing frames:</p>
<h2 id="initialize-the-video-stream-then-allow-the-camera-sensor-to-warm-up"><a href="#initialize-the-video-stream-then-allow-the-camera-sensor-to-warm-up" class="headerlink" title="initialize the video stream, then allow the camera sensor to warm up"></a>initialize the video stream, then allow the camera sensor to warm up</h2><p>print(“[INFO] starting video stream…”)<br>vs = VideoStream(src=0).start()<br>time.sleep(2.0)</p>
<h2 id="start-the-FPS-throughput-estimator"><a href="#start-the-FPS-throughput-estimator" class="headerlink" title="start the FPS throughput estimator"></a>start the FPS throughput estimator</h2><p>fps = FPS().start()</p>
<h2 id="loop-over-frames-from-the-video-file-stream"><a href="#loop-over-frames-from-the-video-file-stream" class="headerlink" title="loop over frames from the video file stream"></a>loop over frames from the video file stream</h2><p>while True:<br>    ## grab the frame from the threaded video stream<br>    frame = vs.read()</p>
<pre><code>## resize the frame to have a width of 600 pixels (while
## maintaining the aspect ratio), and then grab the image
## dimensions
frame = imutils.resize(frame, width=600)
(h, w) = frame.shape[:2]

## construct a blob from the image
imageBlob = cv2.dnn.blobFromImage(
    cv2.resize(frame, (300, 300)), 1.0, (300, 300),
    (104.0, 177.0, 123.0), swapRB=False, crop=False)

## apply OpenCV&#39;s deep learning-based face detector to localize
## faces in the input image
detector.setInput(imageBlob)
detections = detector.forward()</code></pre>
<p>Our <code>VideoStream</code> object is initialized and started on <strong>Line 43</strong>. We wait for the camera sensor to warm up on <strong>Line 44</strong>.</p>
<p>We also initialize our frames per second counter (<strong>Line 47</strong>) and begin looping over frames on <strong>Line 50</strong>. We grab a <code>frame</code> from the webcam on <strong>Line 52</strong>.</p>
<p>From here everything is the same as <strong>Step 3</strong>. We <code>resize</code> the frame (<strong>L**</strong>ine 57**) and then we construct a blob from the frame + detect where the faces are (<strong>Lines 61-68</strong>).</p>
<p>Now let’s process the detections:</p>
<pre><code>## loop over the detections
for i in range(0, detections.shape[2]):
    ## extract the confidence (i.e., probability) associated with
    ## the prediction
    confidence = detections[0, 0, i, 2]

    ## filter out weak detections
    if confidence &gt; args[&quot;confidence&quot;]:
        ## compute the (x, y)-coordinates of the bounding box for
        ## the face
        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
        (startX, startY, endX, endY) = box.astype(&quot;int&quot;)

        ## extract the face ROI
        face = frame[startY:endY, startX:endX]
        (fH, fW) = face.shape[:2]

        ## ensure the face width and height are sufficiently large
        if fW &lt; 20 or fH &lt; 20:
            continue</code></pre>
<p>Just as in the previous section, we begin looping over <code>detections</code> and filter out weak ones (<strong>Lines 71-77</strong>). Then we extract the <code>face</code> ROI as well as ensure the spatial dimensions are sufficiently large enough for the next steps (<strong>Lines 84-89</strong>).</p>
<p>Now it’s time to perform OpenCV face recognition:</p>
<pre><code>        ## construct a blob for the face ROI, then pass the blob
        ## through our face embedding model to obtain the 128-d
        ## quantification of the face
        faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,
            (96, 96), (0, 0, 0), swapRB=True, crop=False)
        embedder.setInput(faceBlob)
        vec = embedder.forward()

        ## perform classification to recognize the face
        preds = recognizer.predict_proba(vec)[0]
        j = np.argmax(preds)
        proba = preds[j]
        name = le.classes_[j]

        ## draw the bounding box of the face along with the
        ## associated probability
        text = &quot;&#123;&#125;: &#123;:.2f&#125;%&quot;.format(name, proba * 100)
        y = startY - 10 if startY - 10 &gt; 10 else startY + 10
        cv2.rectangle(frame, (startX, startY), (endX, endY),
            (0, 0, 255), 2)
        cv2.putText(frame, text, (startX, y),
            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)

## update the FPS counter
fps.update()</code></pre>
<p>Here we:</p>
<ul>
<li>  Construct the <code>faceBlob</code> (<strong>Lines 94 and 95</strong>) and calculate the facial embeddings via deep learning (<strong>Lines 96 and 97</strong>).</li>
<li>  Recognize the most-likely <code>name</code> of the face while calculating the probability (<strong>Line 100-103</strong>).</li>
<li>  Draw a bounding box around the face and the person’s <code>name</code> + probability (<strong>Lines 107 -112</strong>).</li>
</ul>
<p>Our <code>fps</code> counter is updated on <strong>Line 115</strong>.</p>
<p>Let’s display the results and clean up:</p>
<pre><code>## show the output frame
cv2.imshow(&quot;Frame&quot;, frame)
key = cv2.waitKey(1) &amp; 0xFF

## if the `q` key was pressed, break from the loop
if key == ord(&quot;q&quot;):
    break</code></pre>
<h2 id="stop-the-timer-and-display-FPS-information"><a href="#stop-the-timer-and-display-FPS-information" class="headerlink" title="stop the timer and display FPS information"></a>stop the timer and display FPS information</h2><p>fps.stop()<br>print(“[INFO] elasped time: {:.2f}”.format(fps.elapsed()))<br>print(“[INFO] approx. FPS: {:.2f}”.format(fps.fps()))</p>
<h2 id="do-a-bit-of-cleanup"><a href="#do-a-bit-of-cleanup" class="headerlink" title="do a bit of cleanup"></a>do a bit of cleanup</h2><p>cv2.destroyAllWindows()<br>vs.stop()</p>
<p>To close out the script, we:</p>
<ul>
<li>  Display the annotated <code>frame</code> (<strong>Line 118</strong>) and wait for the “q” key to be pressed at which point we break out of the loop (<strong>Lines 119-123</strong>).</li>
<li>  Stop our <code>fps</code> counter and print statistics in the terminal (<strong>Lines 126-128</strong>).</li>
<li>  Cleanup by closing windows and releasing pointers (<strong>Lines 131 and 132</strong>).</li>
</ul>
<p>To execute our OpenCV face recognition pipeline on a video stream, open up a terminal and execute the following command:</p>
<p>$ python recognize_video.py –detector face_detection_model <br>    –embedding-model openface_nn4.small2.v1.t7 <br>    –recognizer output/recognizer.pickle <br>    –le output/le.pickle<br>[INFO] loading face detector…<br>[INFO] loading face recognizer…<br>[INFO] starting video stream…<br>[INFO] elasped time: 12.52<br>[INFO] approx. FPS: 16.13</p>
<p><a target="_blank" rel="noopener" href="https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/opencv-face-recognition/opencv_face_reco_animation.gif"><img src="https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/opencv-face-recognition/opencv_face_reco_animation.gif"></a></p>
<p><strong>Figure 7:</strong> Face recognition in video with OpenCV.</p>
<p>As you can see, both Trisha and my face are correctly identified! Our OpenCV face recognition pipeline is also obtaining ~16 FPS on my iMac. On my MacBook Pro I was getting ~14 FPS throughput rate.</p>
<h4 id="Drawbacks-limitations-and-how-to-obtain-higher-face-recognition-accuracy"><a href="#Drawbacks-limitations-and-how-to-obtain-higher-face-recognition-accuracy" class="headerlink" title="Drawbacks, limitations, and how to obtain higher face recognition accuracy"></a>Drawbacks, limitations, and how to obtain higher face recognition accuracy</h4><p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_misclassification.jpg"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_misclassification.jpg?size=500x663&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 8:</strong> All face recognition systems are error-prone. There will never be a 100% accurate face recognition system.</p>
<p>Inevitably, you’ll run into a situation where OpenCV does not recognize a face correctly.</p>
<p>What do you do in those situations?</p>
<p>And how do you improve your OpenCV face recognition accuracy? In this section, I’ll detail a few of the suggested methods to increase the accuracy of your face recognition pipeline</p>
<h5 id="You-may-need-more-data"><a href="#You-may-need-more-data" class="headerlink" title="You may need more data"></a>You may need more data</h5><p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_more_data.jpg"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_more_data.jpg?size=450x338&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 9:</strong> Most people aren’t training their OpenCV face recognition models with enough data. (<a target="_blank" rel="noopener" href="http://chenlab.ece.cornell.edu/projects/KinshipVerification/">image source</a>)</p>
<p>My first suggestion is likely the most obvious one, but it’s worth sharing.</p>
<p>In my <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">previous tutorial on face recognition</a>, a handful of PyImageSearch readers asked why their face recognition accuracy was low and faces were being misclassified — the conversation went something like this (paraphrased):</p>
<blockquote>
<p><strong>Them:</strong> Hey Adrian, I am trying to perform face recognition on a dataset of my classmate’s faces, but the accuracy is really low. What can I do to increase face recognition accuracy?</p>
<p><strong>Me:</strong> How many face images do you have per person?</p>
<p><strong>Them:</strong> Only one or two.</p>
<p><strong>Me:</strong> Gather more data.</p>
</blockquote>
<p>I get the impression that most readers already know they need more face images when they only have one or two example faces per person, but I suspect they are hoping for me to pull a computer vision technique out of my bag of tips and tricks to solve the problem.</p>
<p>It doesn’t work like that.</p>
<p>If you find yourself with low face recognition accuracy and only have a few example faces per person, gather more data — there are no “computer vision tricks” that will save you from the data gathering process.</p>
<p><strong>Invest in your data and you’ll have a better OpenCV face recognition pipeline.</strong> In general, I would recommend a <strong>minimum of 10-20 faces per person.</strong></p>
<p><em><strong>Note:</strong> You may be thinking, “But Adrian, you only gathered 6 images per person in today’s post!” Yes, you are right — and I did that to prove a point. The OpenCV face recognition system we discussed here today worked but can always be improved. There are times when smaller datasets will give you your desired results, and there’s nothing wrong with trying a small dataset — but when you don’t achieve your desired accuracy you’ll want to gather more data.</em></p>
<h5 id="Perform-face-alignment"><a href="#Perform-face-alignment" class="headerlink" title="Perform face alignment"></a>Perform face alignment</h5><p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_alignment.jpg"><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2018/09/opencv_face_reco_alignment.jpg?size=500x388&lossy=1&strip=1&webp=1"></a></p>
<p><strong>Figure 9:</strong> Performing face alignment for OpenCV facial recognition can dramatically improve face recognition performance.</p>
<p>The face recognition model OpenCV uses to compute the 128-d face embeddings comes from the <a target="_blank" rel="noopener" href="https://cmusatyalab.github.io/openface/">OpenFace project</a>.</p>
<p>The OpenFace model will perform better on faces that have been aligned.</p>
<p>Face alignment is the process of:</p>
<ol>
<li> Identifying the geometric structure of faces in images.</li>
<li> Attempting to obtain a canonical alignment of the face based on translation, rotation, and scale.</li>
</ol>
<p>As you can see from <strong>Figure 9</strong> at the top of this section, I have:</p>
<ol>
<li> Detected a faces in the image and extracted the ROIs (based on the bounding box coordinates).</li>
<li> Applied <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/">facial landmark detection</a> to extract the coordinates of the eyes.</li>
<li> Computed the centroid for each respective eye along with the midpoint between the eyes.</li>
<li> And based on these points, applied an affine transform to resize the face to a fixed size and dimension.</li>
</ol>
<p>If we apply face alignment to every face in our dataset, then in the output coordinate space, all faces should:</p>
<ol>
<li> Be centered in the image.</li>
<li> Be rotated such the eyes lie on a horizontal line (i.e., the face is rotated such that the eyes lie along the same <em>y</em>-coordinates).</li>
<li> Be scaled such that the size of the faces is approximately identical.</li>
</ol>
<p>Applying face alignment to our OpenCV face recognition pipeline was outside the scope of today’s tutorial, but if you would like to further increase your face recognition accuracy using OpenCV and OpenFace, I would recommend you apply face alignment.</p>
<p>Check out my blog post, <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/"><em>Face Alignment with OpenCV and Python</em></a>.</p>
<h5 id="Tune-your-hyperparameters"><a href="#Tune-your-hyperparameters" class="headerlink" title="Tune your hyperparameters"></a>Tune your hyperparameters</h5><p>My second suggestion is for you to attempt to tune your hyperparameters on whatever machine learning model you are using (i.e., the model trained on top of the extracted face embeddings).</p>
<p>For this tutorial, we used a Linear SVM; however, we did not tune the <code>C</code> value, which is typically the most important value of an SVM to tune.</p>
<p>The <code>C</code> value is a “strictness” parameter and controls how much you want to avoid misclassifying each data point in the training set.</p>
<p>Larger values of <code>C</code> will be more strict and try harder to classify every input data point correctly, even at the risk of overfitting.</p>
<p>Smaller values of <code>C</code> will be more “soft”, allowing some misclassifications in the training data, but ideally generalizing better to testing data.</p>
<p>It’s interesting to note that according to one of the classification examples in the <a target="_blank" rel="noopener" href="https://github.com/cmusatyalab/openface">OpenFace GitHub</a>, they actually recommend to <em>not</em> tune the hyperparameters, as, from their experience, they found that setting <code>C=1</code> obtains satisfactory face recognition results in most settings.</p>
<p>Still, if your face recognition accuracy is not sufficient, it may be worth the extra effort and computational cost of tuning your hyperparameters via either a grid search or random search.</p>
<h5 id="Use-dlib’s-embedding-model-but-not-it’s-k-NN-for-face-recognition"><a href="#Use-dlib’s-embedding-model-but-not-it’s-k-NN-for-face-recognition" class="headerlink" title="Use dlib’s embedding model (but not it’s k-NN for face recognition)"></a>Use dlib’s embedding model (but not it’s k-NN for face recognition)</h5><p>In my experience using both OpenCV’s face recognition model along with <a target="_blank" rel="noopener" href="http://blog.dlib.net/2017/02/high-quality-face-recognition-with-deep.html">dlib’s face recognition model</a>, I’ve found that dlib’s face embeddings are more discriminative, especially for smaller datasets.</p>
<p>Furthermore, I’ve found that dlib’s model is less dependent on:</p>
<ol>
<li> Preprocessing such as face alignment</li>
<li> Using a more powerful machine learning model on top of extracted face embeddings</li>
</ol>
<p>If you take a look at <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">my original face recognition tutorial</a>, you’ll notice that we utilized a simple k-NN algorithm for face recognition (with a small modification to throw out nearest neighbor votes whose distance was above a threshold).</p>
<p>The k-NN model worked extremely well, but as we know, more powerful machine learning models exist.</p>
<p>To improve accuracy further, you may want to use <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">dlib’s embedding model</a>, and then instead of applying k-NN, follow <strong>Step #2</strong> from today’s post and train a more powerful classifier on the face embeddings.</p>
<h4 id="Did-you-encounter-a-“USAGE”-error-running-today’s-Python-face-recognition-scripts"><a href="#Did-you-encounter-a-“USAGE”-error-running-today’s-Python-face-recognition-scripts" class="headerlink" title="Did you encounter a “USAGE” error running today’s Python face recognition scripts?"></a>Did you encounter a “USAGE” error running today’s Python face recognition scripts?</h4><p>Each week I receive emails that (paraphrased) go something like this:</p>
<blockquote>
<p>Hi Adrian, I can’t run the code from the blog post.</p>
<p>My error looks like this:</p>
</blockquote>
<p>usage: extract_embeddings.py [-h] -i DATASET -e EMBEDDINGS<br>    -d DETECTOR -m EMBEDDING_MODEL [-c CONFIDENCE]<br>extract_embeddings.py: error: the following arguments are required:<br>    -i/–dataset, -e/–embeddings, -d/–detector, -m/–embedding-model</p>
<p>Or this:</p>
<blockquote>
<p>I’m using Spyder IDE to run the code. It isn’t running as I encounter a “usage” message in the command box.</p>
</blockquote>
<p>There are three separate Python scripts in this tutorial, and furthermore, each of them requires that you (correctly) supply the respective command line arguments.</p>
<p><strong>If you’re new to command line arguments, that’s fine, <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/03/12/python-argparse-command-line-arguments/">but you need to read up on how Python, argparse, and command line arguments work</a> <em>before</em> you try to run these scripts!</strong></p>
<p>I’ll be honest with you — face recognition is an <em>advanced</em> technique. Command line arguments are a <em>very beginner/novice</em> concept. Make sure you walk before you run, otherwise you will trip up. <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/03/12/python-argparse-command-line-arguments/">Take the time now to educate yourself on how command line arguments.</a></p>
<p>Secondly, I always include the <em>exact</em> command you can copy and paste into your terminal or command line and run the script. You might want to modify the command line arguments to accommodate your own image or video data, <em>but essentially I’ve done the work for you</em>. With a knowledge of command line arguments you can update the arguments to <em>point to your own data</em>, <em>without having to modify a single line of code.</em></p>
<p>For the readers that want to use an IDE like Spyder or PyCharm my recommendation is that you learn how to use command line arguments in the command line/terminal <em><strong>first</strong></em>. Program in the IDE, but use the command line to execute your scripts.</p>
<p>I also recommend that you don’t bother trying to configure your IDE for command line arguments until you understand how they work by typing them in first. In fact, <em>you’ll probably learn to love the command line as it is faster than clicking through a GUI menu</em> to input the arguments each time you want to change them. Once you have a good handle on how command line arguments work, you can then configure them separately in your IDE.</p>
<p>From a quick search through my inbox, I see that I’ve answered over 500-1,000 of command line argument-related questions. I’d estimate that I’d answered another 1,000+ such questions replying to comments on the blog.</p>
<p><em>Don’t let me discourage you from commenting on a post or emailing me for assistance — please do.</em> <strong>But if you are new to programming, I urge you to read and try the concepts discussed in my <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/03/12/python-argparse-command-line-arguments/">command line arguments blog post</a></strong> as that will be the tutorial I’ll link you to if you need help.</p>
<h4 id="Alternative-OpenCV-face-recognition-methods"><a href="#Alternative-OpenCV-face-recognition-methods" class="headerlink" title="Alternative OpenCV face recognition methods"></a><strong>Alternative OpenCV face recognition methods</strong></h4><p>In this tutorial, you learned how to perform face recognition using OpenCV and a pre-trained FaceNet model.</p>
<p>Unlike our <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">previous tutorial on deep learning-based face recognition</a>, which utilized two other libraries/packages (dlib and face_recognition), the method covered here today utilizes <em>just</em> OpenCV, therefore removing other dependencies.</p>
<p>However, it’s worth noting that there are other methods that you can utilize when creating your own face recognition systems.</p>
<p><strong>I suggest starting with siamese networks.</strong> Siamese networks are specialized deep learning models that:</p>
<ul>
<li>  Can be successfully trained with very little data</li>
<li>  Learn a similarity score between two images (i.e., how similar two faces are)</li>
<li>  Are the cornerstone of modern face recognition systems</li>
</ul>
<p>I have an entire series of tutorials on siamese networks that I suggest you read to become familiar with them:</p>
<ol>
<li> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2020/11/23/building-image-pairs-for-siamese-networks-with-python/"><em>Building image pairs for siamese networks with Python</em></a></li>
<li> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2020/11/30/siamese-networks-with-keras-tensorflow-and-deep-learning/"><em>Siamese networks with Keras, TensorFlow, and Deep Learning</em></a></li>
<li> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2020/12/07/comparing-images-for-similarity-using-siamese-networks-keras-and-tensorflow/"><em>Comparing images for similarity using siamese networks, Keras, and TensorFlow</em></a></li>
<li> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2021/01/18/contrastive-loss-for-siamese-networks-with-keras-and-tensorflow/"><em>Contrastive Loss for Siamese Networks with Keras and TensorFlow</em></a></li>
</ol>
<p>Additionally, there are non-deep learning-based face recognition methods you may want to consider:</p>
<ul>
<li>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2021/05/03/face-recognition-with-local-binary-patterns-lbps-and-opencv/"><em>Face Recognition with Local Binary Patterns (LBPs) and OpenCV</em></a></li>
<li>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2021/05/10/opencv-eigenfaces-for-face-recognition/"><em>OpenCV Eigenfaces for Face Recognition</em></a></li>
</ul>
<p>These methods are less accurate than their deep learning-based face recognition counterparts, but tend to be much more computationally efficient and will run faster on embedded systems.</p>
<h4 id="What’s-next-I-recommend-PyImageSearch-University"><a href="#What’s-next-I-recommend-PyImageSearch-University" class="headerlink" title="What’s next? I recommend PyImageSearch University."></a>What’s next? I recommend <a target="_blank" rel="noopener" href="https://pyimagesearch.com/pyimagesearch-university/?utm_source=blogPost&utm_medium=bottomBanner&utm_campaign=What%27s%20next?%20I%20recommend">PyImageSearch University</a>.</h4><p><img src="https://fast.wistia.com/embed/medias/kno0cmko2z/swatch"></p>
<p><strong>Course information:</strong><br>30+ total classes • 39h 44m video • Last updated: 12/2021<br>★★★★★ 4.84 (128 Ratings) • 3,000+ Students Enrolled</p>
<p><strong>I strongly believe that if you had the right teacher you could <em>master</em> computer vision and deep learning.</strong></p>
<p>Do you think learning computer vision and deep learning has to be time-consuming, overwhelming, and complicated? Or has to involve complex mathematics and equations? Or requires a degree in computer science?</p>
<p>That’s <em>not</em> the case.</p>
<p>All you need to master computer vision and deep learning is for someone to explain things to you in <em>simple, intuitive</em> terms. <em>And that’s exactly what I do</em>. My mission is to change education and how complex Artificial Intelligence topics are taught.</p>
<p>If you’re serious about learning computer vision, your next stop should be PyImageSearch University, the most comprehensive computer vision, deep learning, and OpenCV course online today. Here you’ll learn how to <em>successfully</em> and <em>confidently</em> apply computer vision to your work, research, and projects. Join me in computer vision mastery.</p>
<p><strong>Inside PyImageSearch University you’ll find:</strong></p>
<ul>
<li>  ✓ <strong>30+ courses</strong> on essential computer vision, deep learning, and OpenCV topics</li>
<li>  ✓ 30+ Certificates of Completion</li>
<li>  ✓ <strong>39h 44m</strong> on-demand video</li>
<li>  ✓ <strong>Brand new courses released <em>every month</em></strong>, ensuring you can keep up with state-of-the-art techniques</li>
<li>  ✓ <strong>Pre-configured Jupyter Notebooks in Google Colab</strong></li>
<li>  ✓ Run all code examples in your web browser — works on Windows, macOS, and Linux (no dev environment configuration required!)</li>
<li>  ✓ Access to <strong>centralized code repos for <em>all</em> 500+ tutorials</strong> on PyImageSearch</li>
<li>  ✓ <strong>Easy one-click downloads</strong> for code, datasets, pre-trained models, etc.</li>
<li>  ✓ Access on mobile, laptop, desktop, etc.</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/pyimagesearch-university/?utm_source=blogPost&utm_medium=bottomBanner&utm_campaign=What%27s%20next?%20I%20recommend">CLICK HERE TO JOIN PYIMAGESEARCH UNIVERSITY</a></p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>In today’s blog post we used OpenCV to perform face recognition.</p>
<p>Our OpenCV face recognition pipeline was created using a four-stage process:</p>
<ol>
<li> Create your dataset of face images</li>
<li> Extract face embeddings for each face in the image (again, using OpenCV)</li>
<li> Train a model on top of the face embeddings</li>
<li> Utilize OpenCV to recognize faces in images and video streams</li>
</ol>
<p>Since I was married over this past weekend, I used photos of myself and Trisha (my now wife) to keep the tutorial fun and festive.</p>
<p>You can, of course, swap in your own face dataset provided you follow the directory structure of the project detailed above.</p>
<p>If you need help gathering your own face dataset, be sure to refer to this post on <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/11/how-to-build-a-custom-face-recognition-dataset/">building a face recognition dataset</a>.</p>
<p>I hope you enjoyed today’s tutorial on OpenCV face recognition!</p>
<p><strong>To download the source code, models, and example dataset for this post (and be notified when future blog posts are published here on PyImageSearch), <em>just enter your email address in the form below!</em></strong></p>
<p><img src="https://929687.smushcdn.com/2633864/wp-content/uploads/2020/01/cta-source-guide-1.png?lossy=1&strip=1&webp=1"></p>
<h5 id="Download-the-Source-Code-and-FREE-17-page-Resource-Guide"><a href="#Download-the-Source-Code-and-FREE-17-page-Resource-Guide" class="headerlink" title="Download the Source Code and FREE 17-page Resource Guide"></a>Download the Source Code and FREE 17-page Resource Guide</h5><p>Enter your email address below to get a .zip of the code and a <strong>FREE 17-page Resource Guide on Computer Vision, OpenCV, and Deep Learning.</strong> Inside you’ll find my hand-picked tutorials, books, courses, and libraries to help you master CV and DL!</p>
<p>DOWNLOAD THE CODE!</p>
<p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=240&d=mm&r=g"></p>
<h5 id="About-the-Author"><a href="#About-the-Author" class="headerlink" title="About the Author"></a><strong>About the Author</strong></h5><p>Hi there, I’m Adrian Rosebrock, PhD. All too often I see developers, students, and researchers wasting their time, studying the wrong things, and generally struggling to get started with Computer Vision, Deep Learning, and OpenCV. I created this website to show you what I believe is the best possible way to get your start.</p>
<h3 id="Reader-Interactions"><a href="#Reader-Interactions" class="headerlink" title="Reader Interactions"></a>Reader Interactions</h3><p>[</p>
<p>Previous Article:</p>
<h4 id="pip-install-OpenCV"><a href="#pip-install-OpenCV" class="headerlink" title="pip install OpenCV"></a>pip install OpenCV</h4><p>](<a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/19/pip-install-opencv/)[">https://pyimagesearch.com/2018/09/19/pip-install-opencv/)[</a></p>
<p>Next Article:</p>
<h4 id="Install-OpenCV-4-on-your-Raspberry-Pi"><a href="#Install-OpenCV-4-on-your-Raspberry-Pi" class="headerlink" title="Install OpenCV 4 on your Raspberry Pi"></a>Install OpenCV 4 on your Raspberry Pi</h4><p>](<a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/26/install-opencv-4-on-your-raspberry-pi/">https://pyimagesearch.com/2018/09/26/install-opencv-4-on-your-raspberry-pi/</a>)</p>
<h4 id="359-responses-to-OpenCV-Face-Recognition"><a href="#359-responses-to-OpenCV-Face-Recognition" class="headerlink" title="359 responses to: OpenCV Face Recognition"></a>359 responses to: OpenCV Face Recognition</h4><ol>
<li><p><img src="https://secure.gravatar.com/avatar/98d96d0f4b4aa3b1b8b58009396f285f?s=48&d=mm&r=g">Harald Vaessin</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479427">September 24, 2018 at 10:53 am</a></p>
<p> My heartfelt congratulations and best wishes for your future together.<br> And thank you for your wonderful tutorials!<br> HV</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/19ac9a6370cf20163c098cc74d73ecf4?s=48&d=mm&r=g">Jesudas</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479428">September 24, 2018 at 10:56 am</a></p>
<p> Congratulations Adrian on your marriage. Wishing you and Trisha the Very Best in Life !</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/f7883fcf89a17b7716e875f5a48116c2?s=48&d=mm&r=g">Bhavesh kacha</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766619">March 17, 2020 at 5:16 am</a></p>
<p>  Can we live stream that over a network??? If yes, then how ???</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766926">March 19, 2020 at 9:49 am</a></p>
<p>  You can follow <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2019/09/02/opencv-stream-video-to-web-browser-html-page/">this tutorial.</a></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/f0c43c8f73a82effb7435efb53899b36?s=48&d=mm&r=g">Tosho Futami</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479431">September 24, 2018 at 11:09 am</a></p>
<p> I am very appreciated for your weekly new code support. Conglaturation your marriage, please enjoy your forepufule future…</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/b335b3d02b32263040bf7bb83300b4d5?s=48&d=mm&r=g">raj shah</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479564">September 25, 2018 at 3:01 am</a></p>
<p>  hey can u help me to figure out this module (Opencv) ,i m getting an error i know its command line argument can u tell me the configuration parts of ur file.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/3a63fea0fd7f13b5193e941e9c4f72ec?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://ayushpant1998@gmail.com/">Ayush</a></p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479435">September 24, 2018 at 11:22 am</a></p>
<p> Can this be used for detecting and recognising faces in a classroom with many students?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/ce88ea36c4a0d4be49719fc288741c7f?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://twitter.com/drhoffma">David Hoffman</a></p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479461">September 24, 2018 at 2:23 pm</a></p>
<p>  Hi Ayush, potentially it can be used for a classroom. There are several considerations to make:</p>
<ol>
<li> Due to the camera angle, some students’ faces may be obscured if the camera is positioned at the front of the classroom.</li>
<li> Scaling of faces especially for low resolution cameras (depends on camera placement).</li>
<li> Privacy concerns — especially since students/children are involved.</li>
</ol>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/90e8191efee98045d3a6cedc05f629a8?s=48&d=mm&r=g">Huseyn</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498900">February 1, 2019 at 5:42 am</a></p>
<p>  What is the maximum number of people i can trai and this system will work accurately?</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/9b363c50a2c9e981f0068d275f2330fe?s=48&d=mm&r=g">falahgs</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479441">September 24, 2018 at 11:38 am</a></p>
<p> congratulations Adrian ..<br> i like all you are posts in geat blog<br> u really great prof.<br> thanks for this post</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/730b7f99bf79688ef6d6ac224fd33c6a?s=48&d=mm&r=g">Nika</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479445">September 24, 2018 at 12:06 pm</a></p>
<p> Congratulations Adrian and thanks for the tutorial!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/65b6542316cb29ebcc3c1d4c1c153e30?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://none/">mohamed</a></p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479446">September 24, 2018 at 12:17 pm</a></p>
<p> Congratulations Adrian<br> happy Days</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c2fbc71930b000756bd47fd2a295a510?s=48&d=mm&r=g">Jesus Hdz Soberon</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479454">September 24, 2018 at 1:28 pm</a></p>
<p> Congratulations Adrian for you and now for your wife. My best wishes in this new stage of your lives.<br> Best regards from México.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/00947f41f94298b175471fa22a454333?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="https://raypack.ai/">Gary</a></p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479459">September 24, 2018 at 2:04 pm</a></p>
<p> Hello Adrian,<br> I got married in February this year and it feels very good and right 🙂 Nerds like us need great women on our side. Take good care of them and congratulation.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e4d4c96bff48a8f0041eddb3b1e4a07b?s=48&d=mm&r=g">Cyprian</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479460">September 24, 2018 at 2:07 pm</a></p>
<p>Congrats on getting married!<br>Thank you again for this great tutorial on face recognition!<br>Have a nice honeymoon.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/fe01bbd97203f36cdb327306e96b7f81?s=48&d=mm&r=g">Yinon Bloch</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479463">September 24, 2018 at 3:14 pm</a></p>
<p>Congratulations Adrian,<br>I wish you both a happy life together!<br>I read your blog from time to time and enjoy it a lot, I gain a lot of knowledge and ideas from your posts. thank you very much!<br>Regarding your comments about improving the accuracy of the identity, I would like to share with you that I also play a lot with the various libraries of facial identification.<br>I’v tried the code I found in Martin Krasser’s post: <a target="_blank" rel="noopener" href="http://krasserm.github.io/2018/02/07/deep-face-recognition/">http://krasserm.github.io/2018/02/07/deep-face-recognition/</a><br>Which is very similar to what you’ve shown in this post. I would like to know if there are any significant differences between the two.<br>After a lot of poking around and testing I also came to conclusion that the dlib library gives the best results (at least for my needs), but without GPU – we get very slow performance.</p>
<p>I wanted to know if you tried to use the facenet library, which uses a vector of 512D, from my experiments it seems to have the same accuracy as nn4 (more or less), but maybe I’m doing something wrong here.</p>
<p>I would appreciate a response from your experience ,</p>
<p>Great appreciation,<br>Yinon Bloch</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/85b9253a21ac8be53718e2494969eb92?s=48&d=mm&r=g">Horelvis</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479467">September 24, 2018 at 4:23 pm</a></p>
<p>Congratulations Adrian!<br>But now you will don’t have more free time! 😉<br>Enjoy with your wife for all life!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d4f8cc6b1b2f0f24382b7e7bbe6cae4c?s=48&d=mm&r=g">Hossein</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479468">September 24, 2018 at 4:39 pm</a></p>
<p>Congratulations<br>I wish a green life for you.<br>great Thanks</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/aec601bff9ade94745fbc00275df6d43?s=48&d=mm&r=g">Nico</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479469">September 24, 2018 at 4:41 pm</a></p>
<p>Hi Adrian,<br>first of all congrats.</p>
<p>Regarding the code, I tend to agree with Yinon about the fact that the version that uses dlib seems to work better. In particular this version sometimes finds inexistent faces.<br>What is your opinion ?<br>Thanks<br>Nico</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/6689a119b178aaf896631cce3eb6dfa9?s=48&d=mm&r=g">Naser</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479473">September 24, 2018 at 5:33 pm</a></p>
<p>Congratulations Adrian and thank you for good tutorial!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/0ff5404071b72662bf2ac08a1fc0a663?s=48&d=mm&r=g">Hugues</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479477">September 24, 2018 at 6:10 pm</a></p>
<p>Very nice postings, and congratulations on your wedding.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/6e6dbd9e3dbc7353b15a30564b8ff928?s=48&d=mm&r=g">Prateek Xaxa</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479478">September 24, 2018 at 6:50 pm</a></p>
<p>Thanks for the great contents</p>
<p>Wishing Happy Life Together!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/10bca9fead7f94940afca7f0968de452?s=48&d=mm&r=g">Sinh Huynh</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479481">September 24, 2018 at 8:27 pm</a></p>
<p>Wishing you and Trisha all the best in your marriage.<br>Many thanks for your tutorials, they are really great, easy to understand for beginner like me.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/0d4335ebf0a2f3b800ec9f46979e86e2?s=48&d=mm&r=g">kus</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479482">September 24, 2018 at 8:28 pm</a></p>
<p>Congratulations!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/85239fd9da379f3e663b87323b6cc16e?s=48&d=mm&r=g">brett</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479486">September 24, 2018 at 9:23 pm</a></p>
<p>Congratulations, wish you both the best! Thank-you for this post,ill be attempting it in the next few days, great tutorials always worth a read.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/537539638bf494db3bfc9455034bdbd3?s=48&d=mm&r=g">Guanghui Yang</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479487">September 24, 2018 at 9:25 pm</a></p>
<p>Congratulations Adrian！</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/436bc15408c327a1a6c5fb32f773a503?s=48&d=mm&r=g">Tran Tuan Vu</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479494">September 24, 2018 at 10:41 pm</a></p>
<p>Hi Adrian,<br>I have tried on my big dataset (250 persons with ~30 image/person). But when I run recogization scripts, I got very low accuracy? So I think I should not use Linear-SVM for training on the big dataset.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/ce88ea36c4a0d4be49719fc288741c7f?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://twitter.com/drhoffma">David Hoffman</a></p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479654">September 25, 2018 at 11:25 am</a></p>
<p>  Hi Tran, I believe that you need more training data. Thirty images per class isn’t likely enough.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5d28ff93e3eba927761a950a912ffe43?s=48&d=mm&r=g">Keesh</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479501">September 24, 2018 at 11:52 pm</a></p>
<p>Congrats Adrian and Trisha!<br>I hope you have a wonderful Honeymoon and life together.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/831ee93e10e3030b89deab53727173c2?s=48&d=mm&r=g">Emmanuel Girard</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479506">September 25, 2018 at 12:02 am</a></p>
<p>Félicitations. Nous vous souhaitons du bonheur, de la joie, de l’amour et beaucoup de souvenirs. / Congratulations. We wish you happiness, joy, love and many memories.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a8d27b7cb84b4d05c00f1479d802330c?s=48&d=mm&r=g">Namdev</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479513">September 25, 2018 at 12:23 am</a></p>
<p>Many congratulations, Adrian and Trisha</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/300ccf641ac23e91899b047891b106c6?s=48&d=mm&r=g">andreas</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479531">September 25, 2018 at 1:06 am</a></p>
<p>Hi Adrian,<br>Thank you for your tutorial. Could you please point out where non max suppression is solved in this pipeline?<br>Thanks,<br>Andreas</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/15651d7b8e171af94e5bd71bce549cab?s=48&d=mm&r=g">Abhishek Thanki</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479870">September 26, 2018 at 1:23 pm</a></p>
<p>  Hi Andreas,</p>
<p>  There was no non-maxima suppression applied explicitly in the pipeline. Instead, it’s applied by the deep learning based face detector used (which uses a SSD model).</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/2b47ca0cdb231e5b373b064e65d7dd96?s=48&d=mm&r=g">Waheed</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479532">September 25, 2018 at 1:08 am</a></p>
<p>Congratulation Adrian. You deserve it! Thanks for all your posts. I really enjoy them</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/078e1ce95ea0922270c55084008825e8?s=48&d=mm&r=g">Evgeny</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479551">September 25, 2018 at 2:31 am</a></p>
<p>Congratulations Adrian! Thanks for your great post. Wish you a happy life together!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/9fab474cf7b6037cc71c1d55f6fd8710?s=48&d=mm&r=g">Pardis</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479572">September 25, 2018 at 3:31 am</a></p>
<p>Wishing you both a lifetime of love and happiness. And thank you for this great tutorial.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/f3f369c16b2690b20eee3c538876e723?s=48&d=mm&r=g">Chunan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479589">September 25, 2018 at 3:53 am</a></p>
<p>Congratulations! Happy wedding.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e44d6db487716a78d0452bf6d95dda2e?s=48&d=mm&r=g">MD Khan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479604">September 25, 2018 at 5:05 am</a></p>
<p>Congratulations Dr!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/854626a8741852f22600aa0ecf3db113?s=48&d=mm&r=g">siavash</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479629">September 25, 2018 at 6:56 am</a></p>
<p>&lt;3</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/98cca282ebf5cb7f9e06a9a6fcddc6f5?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://www.hash0k.com/">Srinivasan Ramachandran</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479837">September 26, 2018 at 8:00 am</a></p>
<p>Hello Adrian,</p>
<p>Hearty congratulations and best wishes to you and your wife.</p>
<p>Regards,</p>
<p>#0K</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5e6363068241d3a91e4c5adac677f287?s=48&d=mm&r=g">Devkar</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479847">September 26, 2018 at 8:42 am</a></p>
<p>Congratulations….</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/2255d113eedc5ebad187b2c31f7421ab?s=48&d=mm&r=g">Zak Zebrowski</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479871">September 26, 2018 at 1:38 pm</a></p>
<p>Congrats!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/17ea2e2b2e8554ec30f4f869800d9969?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://torun4ever.com/">Murthy Udupa</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479905">September 26, 2018 at 11:08 pm</a></p>
<p>Congratulations Adrian and Trisha. Wish you a wonderful life ahead.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/0b2ff295c34443a97126dbb2f7a9290b?s=48&d=mm&r=g">PFC</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479959">September 27, 2018 at 8:33 am</a></p>
<p>If I want to add a person’s face model, do I just need to add that person’s face data set to the dataset folder?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/ce88ea36c4a0d4be49719fc288741c7f?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://twitter.com/drhoffma">David Hoffman</a></p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-479963">September 27, 2018 at 8:58 am</a></p>
<p>  Hi Peng — you’ll need a folder of face pictures for each person in the dataset directory. Then you’ll need to extract embeddings for the dataset and continue with the next steps.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/9a9d21693da7ae487a486721adf881f6?s=48&d=mm&r=g">noura</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-489647">November 28, 2018 at 3:01 pm</a></p>
<p>  how do the extract embedding ?</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/f955dc66abe7cb218dc0a31b44d20b7a?s=48&d=mm&r=g">Arya</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514866">April 26, 2019 at 8:20 am</a></p>
<p>  Tried that. Still shows uknown</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/aa63154f428c44ba2170d6ed3922fb1c?s=48&d=mm&r=g">wayne</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480272">September 30, 2018 at 5:16 am</a></p>
<p>Thanks for your course and congrats!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481269">October 8, 2018 at 12:07 pm</a></p>
<p>  Thanks Wayne, I’m glad you’re enjoying the course 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d539c8d2da5b265526f72a801d994bd8?s=48&d=mm&r=g">Hariprasad</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480399">October 1, 2018 at 2:13 am</a></p>
<p>Happy Married Life</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481252">October 8, 2018 at 10:48 am</a></p>
<p>  Thanks Hariprasad!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c1bc36fed66d402357a9aa23a9aa9c49?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="https://www.caramanual.com/">Cara Manual</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480512">October 2, 2018 at 3:12 am</a></p>
<p>Thank you, this really helped me …</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481238">October 8, 2018 at 10:39 am</a></p>
<p>  Thanks Cara, I’m happy the tutorial has helped you 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c1bc36fed66d402357a9aa23a9aa9c49?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="https://iprint.id/">Jasa Print Kain Jakarta</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480513">October 2, 2018 at 3:13 am</a></p>
<p>Congratulations Adrian and thanks for the tutorial, this is ver usefull…</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481237">October 8, 2018 at 10:38 am</a></p>
<p>  Thank you 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d0de25b5269a380a7808ab999f632509?s=48&d=mm&r=g">Hermy Cruz</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480532">October 2, 2018 at 10:43 am</a></p>
<p>Hi Adrian! First of all Congratulations!!</p>
<p>I have a question, how can I run this at startup if it has command line arguments(crontab).<br>Thank you in advance!!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481230">October 8, 2018 at 10:33 am</a></p>
<p>  I would suggest creating a shell script that calls your Python script. Then call the shell script from the crontab.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c828df8607a337589cb3230037c0e7d3?s=48&d=mm&r=g">Stephen Fischer</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480545">October 2, 2018 at 5:11 pm</a></p>
<p>Congratulations to you and Trisha! Many of your readers got a chance to meet both of you at PyImageConf, and you make a great couple! Here’s to many happy years ahead!</p>
<p>One quick suggestion – I had been receiving an error as follows in the sample code:<br>[INFO] loading face detector…<br>[INFO] loading face recognizer…<br>[INFO] starting video stream…<br>[INFO] elasped time: 8.33<br>[INFO] approx. FPS: 22.09<br>FATAL: exception not rethrown<br>Aborted (core dumped)</p>
<p>I’m wondering if this is related to imutils Bug #86? Anyways, I put a sleep command in and it addressed the “waiting producer/stream issue”:  </p>
<h2 id="do-a-bit-of-cleanup-1"><a href="#do-a-bit-of-cleanup-1" class="headerlink" title="do a bit of cleanup"></a>do a bit of cleanup</h2><p>cv2.destroyAllWindows()<br>time.sleep(1.0)<br>vs.stop()</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481226">October 8, 2018 at 10:32 am</a></p>
<p>  Thanks Stephen 🙂 And yes, I believe the error is due to the threading bug.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a2aa236a27d546893fc2b4f041ff89a1?s=48&d=mm&r=g">tommy</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485415">November 4, 2018 at 10:00 pm</a></p>
<p>  Dear Stephen,</p>
<p>  How about trying to chage code excution order as below?</p>
<p>  vs.stop()<br>  time.sleep(0.5)<br>  cv2.destroyAllWindows()</p>
<p>  It worked for me.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c5a614c796d471aea6466b4e6502813a?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://this-mysterious-world.blogspot.com/">Luis M</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480546">October 2, 2018 at 5:12 pm</a></p>
<p>Congratulations, Adrian! 😀</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481225">October 8, 2018 at 10:32 am</a></p>
<p>  Thank you Luis!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/fb564acbaaf1e4be591c6e3375d94771?s=48&d=mm&r=g">Ravindran</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480591">October 3, 2018 at 1:20 am</a></p>
<p>Congratulations Adrian and Trisha! Happy wedding!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481217">October 8, 2018 at 10:28 am</a></p>
<p>  Thanks so much Ravindran! 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/1a294f2b03ec8daefbac2e9e01023336?s=48&d=mm&r=g">Francisco Rodriguez</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480633">October 3, 2018 at 12:28 pm</a></p>
<p>Hello Adrian, excellent post I want to ask you a question if I follow your course pyimagesearch-gurus or buy the most extensive version of ImageNet Bundle. I could have support and the necessary information to start a project of face-recognition at a distance for example more than 8 meters</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481208">October 8, 2018 at 10:22 am</a></p>
<p>  Hi Francisco, I always do my best to help readers and certainly prioritize customers. I provide the best support I possibly can but do keep in mind that I expect you to put in the hard work, read the books/courses, and run your own experiments. I’m more than happy to keep you going in the right direction but do keep in mind that I cannot do the hard work for you. Keep up the great work! 🙂</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/1a294f2b03ec8daefbac2e9e01023336?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="https://www.linkedin.com/in/francisco-rodr%C3%ADguez-mgs-4772527a/">Francisco Rodriguez</a></p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-487722">November 18, 2018 at 5:32 pm</a></p>
<p>  Thanks Adrian, I know that the effort should be mine, the important thing is to have good bibliography and information, thank you I am very motivated and tis post are of great help especially to developing countries like in which I live</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/3fd87de1199debfb581d1be2991f7035?s=48&d=mm&r=g">Chintan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480678">October 4, 2018 at 1:55 am</a></p>
<p>Congratulations to both of you!!</p>
<p>I want to use this face recognition method in form of a mobile application. Currently I have used <a target="_blank" rel="noopener" href="https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/#0">https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/#0</a> article for developing mobile application from tensorflow for face detection.</p>
<p>Can you suggest me a direction?</p>
<p>Thanks</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/589f35f40cffd5ee7f4a26127c969a1e?s=48&d=mm&r=g">Kalicharan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480728">October 4, 2018 at 12:01 pm</a></p>
<p>I dont have 30+ pictures for each person, can i use the data augmentation tool to create many pictures of the pictures i have by blur, shifting etc</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481196">October 8, 2018 at 10:11 am</a></p>
<p>  Yes, but make sure your data augmentation is realistic of how a face would look. For example, don’t use too much shearing or you’ll overly distort the face.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e9e0d696461f09295c94202e7656e220?s=48&d=mm&r=g">Neleesh</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480814">October 5, 2018 at 3:01 pm</a></p>
<p>Congratulations Adrian, thank you for the tutorial. I am starting to follow you more regularly. I am amazed with the detail in your blogs. I am just curious how long each of these tutorial takes you to plan and author.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481183">October 8, 2018 at 9:50 am</a></p>
<p>  Thanks Neleesh. As far as how long it takes to create each tutorial, it really depends. Some tutorials take less than half a day. Others are larger, on-going projects that can span days to weeks.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/eeb4abbca8c9ae532ef80c699c22f2e5?s=48&d=mm&r=g">Huy Ngo</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480929">October 6, 2018 at 11:58 am</a></p>
<p>Hi Adrian.<br>How to apply this model on my own dataset?<br>Thank you in advance.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481169">October 8, 2018 at 9:41 am</a></p>
<p>  This tutorial actually covers how to build your own face recognition system on your own dataset. Just refer to the directory structure I provided and insert your own images.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/b2907bd098c1c5d9f3a6c86139803361?s=48&d=mm&r=g">dadiouf</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480947">October 6, 2018 at 2:51 pm</a></p>
<p>You both make a lovely couple</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481164">October 8, 2018 at 9:39 am</a></p>
<p>  Thank you 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/bf4ab4f000e5fd158ce3bcade46f10ac?s=48&d=mm&r=g">Q</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-480972">October 6, 2018 at 6:52 pm</a></p>
<p>Adrian,<br>Congratulations on your marriage!<br>Take some time off for your honeymoon and enjoy the best time of your life!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481165">October 8, 2018 at 9:39 am</a></p>
<p>  Thank you so much! 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/3a0f2964cc8e6836142088899ef84696?s=48&d=mm&r=g">Rayomond</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481459">October 8, 2018 at 10:59 pm</a></p>
<p>Hearty Congratulations! Wish you both the very best</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481522">October 9, 2018 at 6:05 am</a></p>
<p>  Thanks Rayomond 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/8de1512eb8040c4462e3445f3d93c3c9?s=48&d=mm&r=g">dauey</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-481698">October 10, 2018 at 4:21 am</a></p>
<p>have you liveness detection for face recognition systems?its necessary for face recognition systems.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-482190">October 12, 2018 at 9:17 am</a></p>
<p>  I do not have any liveliness detection tutorials but I will try to cover the topic in the future.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/411df6715671a6324f2f03467e5220ed?s=48&d=mm&r=g">Nguyen Anh Tuan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-482842">October 16, 2018 at 11:34 am</a></p>
<p>Congratulation man</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-483332">October 20, 2018 at 8:07 am</a></p>
<p>  Thank you!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c8d6e426dbbe16b0f86529edd3077bed?s=48&d=mm&r=g">Eric</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-483042">October 17, 2018 at 11:17 pm</a></p>
<p>Hi Adrian, Congratulations on the marriage!</p>
<p>Thank you for all the interesting posts!</p>
<p>I wonder if Adrian or anyone else has actually combined the dlib landmarks with the training described in this post? It seems to require additional steps which are not that easy to infer.</p>
<p>I have successfully created embeddings/encodings from the older posts dlib instructions but when I combine them with this posts training 100% of the faces get recognized as the same face with very high accurace despite my dataset containing several different faces. When I changed up the model I saw that it basically only recognized the first name in the dict that is created and then matches every found face to that name (in one case it even matched a backpack).</p>
<p>I spotted a difference between the dicts that get pickled. The one from this post has a text: dtype=float32 at the end of every array but the dlib dict does not have this text. Maybe this is a problem cause? In any case I can’t spot anything else I could change. But I also don’t know how to change that. (Another small difference is that this post uses embeddings in its code and the previous one calls them encodings).</p>
<p>Also, in the text above, shouldn’t it be proba &gt; T?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/6acc6215e646d233541c02622edffe80?s=48&d=mm&r=g">Sebastian</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-522283">June 19, 2019 at 9:26 am</a></p>
<p>  I’m also trying to combine those two. Did you manage to get it to work?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/4c9435780e30fdda2576808807770bef?s=48&d=mm&r=g">Naresh</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-527098">July 24, 2019 at 9:03 am</a></p>
<p>  I was also trying to combine both, Had you done that ?</p>
<p>  Please let me know.</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/ccf0d831f8e806bcaf6d11ebd868dea9?s=48&d=mm&r=g">Varun</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-483637">October 22, 2018 at 9:47 pm</a></p>
<p>Thanks a lot man</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-484505">October 29, 2018 at 2:15 pm</a></p>
<p>  You are welcome, Varun 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/3c8a3e33695ffb8da8b3f0c657b9182f?s=48&d=mm&r=g">Arvand Homer</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-483842">October 24, 2018 at 12:14 pm</a></p>
<p>Hey Adrian, thanks for the tutorial.</p>
<p>We are trying to run the code off an Nvidia Jetson TX2 with a 2.1 mm fisheye lens camera, but the frame rate of our video stream is very low and there is significant lag. Is there any way to resolve these problems?</p>
<p>Best wishes.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/429f49cb2d1764920122269d3dd01e48?s=48&d=mm&r=g">Praveen</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-484249">October 28, 2018 at 4:52 am</a></p>
<p>hi adrian, will this algo is useful for faceliveliness detection..</p>
<p>Thanq</p>
<p>with regards,<br>praveen</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-484447">October 29, 2018 at 1:25 pm</a></p>
<p>  No, face recognition and liveliness detection are two separate subjects. You would need a dedicated liveliness detector.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/da718568eca2d093384447dffcb58749?s=48&d=mm&r=g">Somo</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-484616">October 30, 2018 at 5:48 am</a></p>
<p>Hi Adrian,</p>
<p>First of all thanks for the tutorial.<br>If I were going to use the dlib’s embedding model, but wanting to change from k-NN to SVM how do I do that.</p>
<p>Thanks,<br>Somo</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485044">November 2, 2018 at 8:27 am</a></p>
<p>  You would replace use the model from <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">dlib face recognition tutorial</a> instead of the OpenCV face embedder. Just swap out the models and relevant code. Give it a try!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/7a64e4011aee9b715141b4b32a9f8c51?s=48&d=mm&r=g">akhil alexander</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-484758">October 31, 2018 at 6:21 am</a></p>
<p>Hi Andrian, your posts are always inspiring.Congratulations and wishing you a Happy married life… I invite both of you to my state, you should visit Kerala at least once in your lifetime <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=gpTMhLWUZCQ">https://www.youtube.com/watch?v=gpTMhLWUZCQ</a></p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485019">November 2, 2018 at 7:38 am</a></p>
<p>  Thank you Akhil, I really appreciate your kind words 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/2485595649da960672517be968ed626f?s=48&d=mm&r=g">Zong</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-484846">October 31, 2018 at 10:17 pm</a></p>
<p>hi Adrian，thanks for your tutorial!<br>I’m trying to replace the resnet caffemodel with squeezenet caffemodel. Simply replace the caffemodel file seems not work. How should I rewrite the code?<br>PS: Congratulations on your marriage!<br>Thanks again<br>Zong</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485013">November 2, 2018 at 7:22 am</a></p>
<p>  Hey Zong — which SqueezeNet model are you using? Keep in mind that OpenCV doesn’t support all Caffe models.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/218951b0584347261665a6d23b9bef05?s=48&d=mm&r=g">M O Leong</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485296">November 4, 2018 at 5:19 am</a></p>
<p>Hi Adrian.</p>
<p>Having attempted the 1st few sections of your post (recognize.py), surprisingly, when I run patrack_bateman.jpg it appears to recognise the photo as “adrian”. Did you actually add more photos to your dataset so that “patrick bateman” doesn’t get recognised wrongly?</p>
<p>Yes, I read further down the post that more datasets will eventually lead to much-needed accuracy. But I was just wondering how u got to the part to achieve “patrick bateman’ being ‘unknown’ or unrecognized in your tutorial example. Look forward to your feedback.</p>
<p>Many thanks!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485783">November 6, 2018 at 1:26 pm</a></p>
<p>  That is quite strange. What version of OpenCV, dlib, and scikit-learn are you using?</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/f1fbb43375a9e7805edc2e3c0e9e003f?s=48&d=mm&r=g">Harshpal</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485310">November 4, 2018 at 7:58 am</a></p>
<p>Hi Adrian, Thanks for the informative article on Face Recognition. Loved it!!!</p>
<p>I have a question on this. What if, I already have pre-trained model for face recognition (say FaceNet) and on top of it I want to train the same model for a few more faces. Is it possible to retrain the same model by updating the weights file.</p>
<p>Or how can this be done. Please suggest ideas.</p>
<p>Regards,<br>Harshpal</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485781">November 6, 2018 at 1:25 pm</a></p>
<p>  Yes. What you are referring to is called “fine-tuning” the model and can be used to take a model trained on one dataset and ideally tune the weights to work on another dataset as well.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a2aa236a27d546893fc2b4f041ff89a1?s=48&d=mm&r=g">tommy</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485420">November 4, 2018 at 10:31 pm</a></p>
<p>Hi, Adrian.<br>Always thanks for your wonderful article.</p>
<p>I have tested your code for a week.<br>It was working for small dataset(1~2 people face).<br>But when I increased number of people(upto 10), it looked unstable sometims.</p>
<p>1.In my test, sometimes, face naming was too fluctuated, I mean,<br>real name and other name was switched too frequently.<br>sometimes it worked a bit stable, but sometimes looked very unstable<br>or gave wrong face-name.<br>So as you said before, I added more pictures(more than 30 pieces)<br>to each person’s directory to increase accuracy.<br>After that, face naming seemed to get more stable, but there are<br>still fluctuated output or wrong naming output frequenty.<br>Is there any method to increase accuracy?</p>
<ol start="2">
<li>Is there possibility on a relation-formula of between face landmark points to distinguish each face more accurately? ( I tried ti find ,but I still failed.)</li>
</ol>
<p>Thanks in advance for your advice.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485774">November 6, 2018 at 1:21 pm</a></p>
<ol>
<li><p>Once you start getting more and more people in your dataset this method will start to fail. Keep in mind that we’re leveraging a <em>pre-trained</em> network here to compute the 128-d facial embeddings. Try instead fine-tuning the network itself on the people you want to recognize to increase accuracy.</p>
</li>
<li><p>2D facial landmarks in some cases can be used for face recognition but realistically they aren’t good for face recognition. The models covered in this post will give you better accuracy.</p>
</li>
</ol>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/04522096a7f5637650d858adc5ec3cb8?s=48&d=mm&r=g">Vijay</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485502">November 5, 2018 at 11:30 am</a></p>
<p>What happend if any person other than the one in data set entered in to the frame….</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-485765">November 6, 2018 at 1:14 pm</a></p>
<p>  The person would be marked as “unknown”.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/4715a1d3021fab7df69092e063e97add?s=48&d=mm&r=g">Ankita</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-486925">November 13, 2018 at 1:32 pm</a></p>
<p>Hi Adrian,</p>
<p>Firstly, I would like to Congratulate you on your wedding though it’s pretty late!<br>I wish to know do you follow any algorithms, kindly mention, if any?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-486946">November 13, 2018 at 4:10 pm</a></p>
<p>  I’m not sure what you mean by “follow any algorithms” — could you clarify?</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c509c13fe67e9993d8f37067071aa9d7?s=48&d=mm&r=g">Vijay</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-487243">November 15, 2018 at 6:24 am</a></p>
<p>“Try instead fine-tuning the network itself on the people you want to recognize to increase accuracy”</p>
<p>Can u plz tell me how to do that ☺️</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/b859a4a6afb438c9e85e19b31a43bf84?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://none/">Ray</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-487976">November 20, 2018 at 2:37 am</a></p>
<p>Hi Adrian,<br>Thanks for the info.<br>I have 2 questions related to this:</p>
<p>First, How would I use an RTSP stream instead of the webcam as input. My rtsp source is in the following format:<br><a href="rtsp://username:password@IP:port/videoMain">rtsp://username:password@IP:port/videoMain</a></p>
<p>I can see this stream in vlc on any computer on my network, so i should be able to use that as the source in your script</p>
<p>Second, instead of viewing the results on my screen, how can I can Output it in a format so I can watch it from another computer. Example, How can I create a stream that I can feed into a vlc server, so I can watch it from another computer on my network.</p>
<p>Thanks for your guidance</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-488047">November 20, 2018 at 9:07 am</a></p>
<p>  Hey Ray — I don’t have any tutorials on how to display or read an RTSP stream on the Pi but I will be covering it in my upcoming Raspberry Pi + computer vision book.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/ffeb4e0fe00f937bfaae73b90edf970a?s=48&d=mm&r=g">anu</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-488455">November 22, 2018 at 3:32 am</a></p>
<p>thanks a lot for this page…<br>how do we include our own pictures into this to recognize?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-489028">November 25, 2018 at 9:31 am</a></p>
<p>  Refer to the “Project structure” section of the tutorial where I describe the directory structure for adding your images. If you need help actually building the face dataset itself, refer to <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/11/how-to-build-a-custom-face-recognition-dataset/">this tutorial.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/6524061d38516004c8776d1a970eadeb?s=48&d=mm&r=g">Teresa DiMeola</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-488561">November 22, 2018 at 1:27 pm</a></p>
<p>Hi Adrian,</p>
<p>You are so kind and generous…you must be an amazing human being. Thank you for this tutorial. I cannot wait to use it (I’m still learning some python basics…so not quite ready yet).</p>
<p>But I do have a general question for you, which is – well – not off topic entirely, but also something which you may not know of the top of your head, but anyway here goes: Can you guess at or estimate at what camera resolution/focal length one would go from being a “resolved image” to a “low resolution image?” Let’s assume for the sake of the question/answer that it is a cooperative subject.</p>
<p>Thanks again, for all you do!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-489018">November 25, 2018 at 9:22 am</a></p>
<p>  Hi Teresa — each camera will have it’s own specific resolution and focal length so I don’t think there is “one true” resolution that will achieve the best results. The results are entirely dependent on the algorithm and the camera itself.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/3e552c419c9f29c49102e873961035dc?s=48&d=mm&r=g">hendrick</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-488592">November 22, 2018 at 5:49 pm</a></p>
<p>hi adrian. i got this error “File “extract_embeddings.py”, line 62, in<br>(h, w) = image.shape[:2]<br>AttributeError: ‘NoneType’ object has no attribute ‘shape’”. This code i ran in ubuntu. But in my Mac everything was fine. I used the same version python and opencv.<br>Thank you</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-489014">November 25, 2018 at 9:19 am</a></p>
<p>  It’s not an issue with Python and OpenCV, it’s an issue with your input path of images. The path to your input images does not exist on disk. Double-check your images and paths.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/f0ce5819789d41ef1b76f16d26cb7380?s=48&d=mm&r=g">Rob</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-494951">January 6, 2019 at 6:14 pm</a></p>
<p>  Hendrick, I had the same error but it was a problem with the webcam under Ubuntu. Once I set that up correctly everything worked fine.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/77fbe4d8767ea65790520be6c88f661c?s=48&d=mm&r=g">Tim</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-488812">November 24, 2018 at 2:43 am</a></p>
<p>Hi Adrian.<br>Great job u have done~<br>Here is my question.<br>How can I plot the decision boundaries for each class after train_model?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-488997">November 25, 2018 at 9:04 am</a></p>
<p>  The scikit-learn documentation has an <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/auto_examples/svm/plot_iris.html">excellent example</a> of plotting the decision boundaries from the SVM.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d328e41d366bb46b9551236eebdd1a26?s=48&d=mm&r=g">S M Yu</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-488950">November 25, 2018 at 4:54 am</a></p>
<p>What should I do if the camera recognizes a person who is not being trained, does not appear as ‘unknown’, but appears in the name of another person?</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/4486e270fe69802ed1634cb4e5b4eaa4?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://pyimagesearch.com/">Dorra</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-489630">November 28, 2018 at 12:47 pm</a></p>
<p>Hi Doctor Adrian<br>Great job<br>I don’t understand this error ” ValueError: unsupported pickle protocol: 3 ” ?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-490008">November 30, 2018 at 9:06 am</a></p>
<p>  Re-train your face recognition model and serialize it to disk. You are trying to use my pre-trained model and we’re using two different versions of Python, hence the error.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/bfd45482a0c9aae65511db2b481b99a5?s=48&d=mm&r=g">Rico</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-489736">November 29, 2018 at 3:38 am</a></p>
<p>LabelEncoder seems to be reversing the labels. If you try to print knownNames and le.classes_, the results are reversed. So when you call le.classes_[j], incorrect mapping is done. It seems to be causing misidentification on my datasets.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/bfd45482a0c9aae65511db2b481b99a5?s=48&d=mm&r=g">Rico</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-490564">December 3, 2018 at 10:21 pm</a></p>
<p>  This happens when the list of images are not sorted. After adding sorting of the list of dataset images, it works without problem.</p>
<p>  By the way, linear SVM seems to perform bad with few dataset images per person. Using other classification algorithms such as Naive Bayes are better suited few datasets.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-490667">December 4, 2018 at 9:50 am</a></p>
<p>  Thank you for sharing your experience, Rico!</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c0911bfab60c73b1f6b3ecdf9c5a2f91?s=48&d=mm&r=g">Gyho</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-492631">December 16, 2018 at 11:03 pm</a></p>
<p>Is it possible to represent the name in other languages, i.e. Chinese?<br>Thank you very much!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-492815">December 18, 2018 at 9:03 am</a></p>
<p>  You can use whatever names in whatever languages you wish, provided Python and OpenCV can handle the character set.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e169b1cb4aaaa948364488577de5869a?s=48&d=mm&r=g">Shaun</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-493200">December 21, 2018 at 6:39 pm</a></p>
<p>Dear Adrian,</p>
<p>Many thanks for your tutorials. Step by step following your instruction, I have successfully implemented 7 tutorials on my RPi. The most fun part is this opencv face recognition tutorial. I train the model by adding my family members. It works pretty accurate at most time but sometimes either your name or your wife name pops up. LOL Anyway, your professional tutorial makes me feel like a real coder, though I am actually a dummy :). Wish you and Trisha a Merry Christmas and Happy New Year.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-493867">December 27, 2018 at 10:59 am</a></p>
<p>  That’s awesome Shaun, I’m so happy to hear you’ve been able to apply the face recognizer to your own projects 🙂</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/52d24164f316a4beb3868371ef8b8a10?s=48&d=mm&r=g">Igor</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-493328">December 23, 2018 at 10:04 am</a></p>
<p>Adrian. Great job. Please tell me how to write a file to the file? Thank.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-493853">December 27, 2018 at 10:47 am</a></p>
<p>  I’m not sure what you mean by “write a file to the file”?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/52d24164f316a4beb3868371ef8b8a10?s=48&d=mm&r=g">Igor</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-494008">December 28, 2018 at 2:08 pm</a></p>
<p>  Sorry 🙂 Write frame to file.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-494513">January 2, 2019 at 9:39 am</a></p>
<p>  You can use the “cv2.imwrite” function to write a frame to disk.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/88a10ceb37f26fcdedba65b1257e6d84?s=48&d=mm&r=g">Yong Shen</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-493473">December 24, 2018 at 3:02 pm</a></p>
<p>hi Adrian，thanks for your tutorial!<br>I tried to run this project using opencv 3.3.0 instead of 3.4.2 to avoid lengthy reinstallation… can it work in opencv 3.3.0 ?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-493834">December 27, 2018 at 10:33 am</a></p>
<p>  I would highly recommend you use OpenCV 3.4.2. You can actually <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/19/pip-install-opencv/">install OpenCV via pip</a> and save yourself quite a bit of time.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/f31e456bdf30334963e44e6fd95cbb74?s=48&d=mm&r=g">Tejesh</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-493701">December 26, 2018 at 10:09 am</a></p>
<p>Happy Married life and thanks once again for such enriching article.</p>
<p>BTW, you had in one of your articles mentioned a link to the zip file containing the General Purpose Faces to be used with the code. Can you please share that link once again over here?</p>
<p>Thanks in Advamce</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-493807">December 27, 2018 at 10:10 am</a></p>
<p>  Thanks Tejesh, although I’m not sure what you mean by the “general purpose faces” — could you elaborate?</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/656b1bf9757cef1c1ae7b37d15585b36?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://www.excis3.be/">Excis3</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-494020">December 28, 2018 at 3:26 pm</a></p>
<p>Hi Adrian,<br>Thanks for the great tutorial and clear site.<br>Its a ton of information. I just started this afternoon after searching the web on how to start, and now i have my own small dataset, and the application is running great.</p>
<p>My next step is finetuning with Face Alignment, and put more data in my dataset.</p>
<p>Thanks.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-494511">January 2, 2019 at 9:38 am</a></p>
<p>  Congratulations on already being up and running with your face recognition system, nice job!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/2e423ec864459f6c0b5bac8083109861?s=48&d=mm&r=g">Muhammad Salman Ali Khan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-494116">December 29, 2018 at 3:20 pm</a></p>
<p>Hello Adrian,</p>
<p>I am facing this error when I run train model:<br>ValueError: The number of classes has to be greater than one; got 1 class</p>
<p>In line 34 =&gt; recognizer.fit(data[“embeddings”], labels)</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-494501">January 2, 2019 at 9:30 am</a></p>
<p>  Are you trying to train a face recognizer to recognize just a single person? Keep in mind that you need <em>at least</em> two classes to train a machine learning model. If you’re trying to train a face recognition system with more than 2 classes and you still received that error then you have an issue parsing your image paths and extracting the person name/ID.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/2eed04332db45fd687612df2cfdd8354?s=48&d=mm&r=g">Kyle Anderson</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-569478">November 3, 2019 at 3:19 pm</a></p>
<p>  What happens if you do want to just train one one person, at least for the time being? I’m trying to create a python script that takes pictures of a person, then trains itself to recognize that person. There may eventually be more than one person, after more people sign up, but for the first user there would only be one person.</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/167e179e2c162ab9edb818172cb0b9b3?s=48&d=mm&r=g">Bhanu Jamwal</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-495095">January 8, 2019 at 5:43 am</a></p>
<p>now the question is how to convert this folder opencv-face-recognition in a api which one can use in web application and websites to have this feature.i will be very thankful if you remove my doubts and lead me a way out of this<br>thank you<br>bhanu</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-495101">January 8, 2019 at 6:35 am</a></p>
<p>  I demonstrated how to create a REST API for computer vision and deep learning <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/">here.</a> You’ll need to modify the code to swap out the Keras code for the OpenCV face recognition code but the framework is there. Good luck!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/167e179e2c162ab9edb818172cb0b9b3?s=48&d=mm&r=g">Bhanu Jamwal</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-495233">January 9, 2019 at 5:50 am</a></p>
<p>thank you sir.<br>you are great<br>have a nice day.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-495936">January 11, 2019 at 9:50 am</a></p>
<p>  You are welcome!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/da74df27a053582cd102e8967c00c652?s=48&d=mm&r=g">Mike</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-495344">January 9, 2019 at 9:05 pm</a></p>
<p>Hi Adrian!</p>
<p>Thanks alot for these amazing tutorials, i’ve gained lot’s of interest about the computer vision subject and i’ve been enjoying your deep learning crash course.</p>
<p>I’m doing a class project for my University, which involves face recognition.</p>
<p>One of the requirements of the teacher is the installation of the scikit-learn package.. i’ve noticed that you have used it in this tutorial.</p>
<p>Now, my concern is, my teacher also expressed that people that use PyTorch or TensorFlow will get a better grade in their projects. I’m not familiar with PyTorch but i’ve noticed in this tutorial that you do indeed have a PyTorch implementation, am i right?</p>
<p>In that case, can scikit learning and PyTorch work together? Am i misunderstanding something about this? Also, what possibly could i add in terms of PyTorch usage that could improve this tutorial that you provided (besides the points that you mention in the end of the tutorial (face-aligment, more data, etc) ?</p>
<p>Thank you!<br>Happy 2019.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-495926">January 11, 2019 at 9:41 am</a></p>
<p>  This tutorial leverages a model trained with PyTorch but it’s not actually a PyTorch tutorial. I personally prefer Keras as my deep learning library of choice. If you’re interested in combining scikit-learn with Keras be sure to take a look at my book, <a target="_blank" rel="noopener" href="https://pyimagesearch.com/deep-learning-computer-vision-python-book/">Deep Learning for Computer Vision with Python</a>, which includes chapters using both Keras and scikit-learn.</p>
<p>  Best of luck with the project!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/da74df27a053582cd102e8967c00c652?s=48&d=mm&r=g">Mike</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-495961">January 11, 2019 at 11:59 am</a></p>
<p>  I see, so in this tutorial in particular we are indeed using PyTorch and scikit together, correct?</p>
<p>  Yes, i’ve asked my teacher about that and he also says Keras wouldve been better but it would also be harder (?), i am indeed very interested in this computer vision subject and your book in particular.. just would like to know if you’re planning to have some sort of discount for students or anything like that because paying 200+ € right now as a student isn’t as easy as it would be in the future 😀</p>
<p>  Thanks Andrian!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-495971">January 11, 2019 at 1:23 pm</a></p>
<p>  No, this tutorial is using OpenCV and scikit-learn. The model itself was trained with PyTorch there is no actual PyTorch code being utilized. Instead, we are using a model that has already been trained.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/ecf65dbdcd8555ac1e3b605b4357d581?s=48&d=mm&r=g">Gaurav</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-496396">January 14, 2019 at 11:26 pm</a></p>
<p>I found This technique is not gives output accurately ..<br>please can you have a more accurate technique to recognition???</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-496654">January 16, 2019 at 9:47 am</a></p>
<p>  How you tried my suggestions in the “Drawbacks, limitations, and how to obtain higher face recognition accuracy” section of the tutorial?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/ecf65dbdcd8555ac1e3b605b4357d581?s=48&d=mm&r=g">Gaurav</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497221">January 21, 2019 at 12:48 am</a></p>
<p>  Yes I followed your Suggestions.<br>  I take 70 samples per person.<br>  then also wrong match and not match scenarios happened more times.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497438">January 22, 2019 at 9:29 am</a></p>
<p>  How many unique people are in your database? 70 samples per person is a good number but I’m curious how many total people are in your dataset?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/ecf65dbdcd8555ac1e3b605b4357d581?s=48&d=mm&r=g">Gaurav</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497585">January 23, 2019 at 6:53 am</a></p>
<p>  Adrian<br>  i include 3 peoples in my dataset.<br>  it cannot display accurately names sometime it display right name but sometime it gives the name of another person.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497847">January 25, 2019 at 7:09 am</a></p>
<p>  For only 3 people the model should be performing better. Have you used the <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">dlib face recognizer</a> as well? Does that model perform any better?</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/ecf65dbdcd8555ac1e3b605b4357d581?s=48&d=mm&r=g">Gaurav</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498871">February 1, 2019 at 1:51 am</a></p>
<p>  yes i used it</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498912">February 1, 2019 at 6:36 am</a></p>
<p>  At that point if dlib and the FaceNet model are not achieving good accuracy you may need to consider fine-tuning an existing model. But for only 3 people either dlib or FaceNet should be performing much better. I think there may be a logic error in your code so I would go back and reinvestigate.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/da74df27a053582cd102e8967c00c652?s=48&d=mm&r=g">Mike</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497707">January 24, 2019 at 8:59 am</a></p>
<p>Hey Adrian!</p>
<p>How could one implement face alignment on this tutorial?</p>
<p>I can perform face alignment because of your other tutorial but don’t know what i’m supposed to do with the new aligned faces.. would you save them directly in your dataset? If so, how?</p>
<p>Thanks in advance.<br>Cumps!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497843">January 25, 2019 at 6:56 am</a></p>
<p>  Take a look at my <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/">face alignment tutorial</a> on how to properly align faces. You would want to align them before computing the 128-d face embeddings.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e3f3ecd952301d83f453eb8af7de29bc?s=48&d=mm&r=g">Abdull</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497855">January 25, 2019 at 7:17 am</a></p>
<p>Hi Adrian may i ask why do u resize the image in the first place?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497882">January 25, 2019 at 7:52 am</a></p>
<p>  High resolution images may look visually appealing to us but they do little to increase the accuracy of computer vision systems. We reduce image size to (1) reduce noise and thereby increase accuracy and (2) ensure our algorithms run faster. The smaller an image is, the less data there is to process and the faster the algorithm will run.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/e3f3ecd952301d83f453eb8af7de29bc?s=48&d=mm&r=g">Abdull</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-497886">January 25, 2019 at 8:49 am</a></p>
<p>  So it’s basicly about the dimension reduction feature of scikit-learn?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498379">January 29, 2019 at 7:07 am</a></p>
<p>  We are reducing the dimensions of the image/frame but I’d be careful calling it “dimensionality reduction”. Dimensionality reduction typically refers to a set of algorithms that reduce the dimensionality of an input set of features based on some sort algorithm that maximizes feature importance (PCA is a good example). Here aren’t removing pixels based on “importance”, we’re simply preprocessing the input image by reducing its size.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/b880cd6ec34396048b6bcdf245ed07bd?s=48&d=mm&r=g">Philipe Huan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498131">January 27, 2019 at 3:22 pm</a></p>
<p>greetings , i have a question, in the file of labels, their content has only a name per person recognized or are there names for represent each image file?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498359">January 29, 2019 at 6:47 am</a></p>
<p>  Sorry, I don’t think I understand your question. Could you elaborate?</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e595fb67d3ba0ceabfdc15ca0368c982?s=48&d=mm&r=g">Dario</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498239">January 28, 2019 at 3:56 pm</a></p>
<p>Hello Adrian, how we could train a model to recognize rotated faces in different angles??? I want to make facial recognition through a eye fish camera</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498254">January 28, 2019 at 5:43 pm</a></p>
<p>  You normally wouldn’t do that. You would detect the face and then perform <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/">face alignment</a> before performing face recognition.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d7f52723c6e058db6de8d575812c1789?s=48&d=mm&r=g">benny</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498446">January 29, 2019 at 7:42 pm</a></p>
<p>Hi Adrian, if I previously have many images trained using the SVM, and now I have several additional images (correspond to new people), I need to retrain the SVM by scanning through all 128-d vectors. It would take a lot of time when the number of images is kept increasing.</p>
<p>Is there any tricks to improve this scalability issue? Thank you</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-498954">February 1, 2019 at 7:21 am</a></p>
<p>  You are correct, you would need to re-train the SVM from scratch. If you expect that more and more faces will be added I suggest you look at “online learning algorithms”.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/d7f52723c6e058db6de8d575812c1789?s=48&d=mm&r=g">benny</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-501194">February 13, 2019 at 9:51 pm</a></p>
<p>  Thank you. Apart from the scalability issue, I would like to know the performance of SVM compared with other simple classifier. For example, L1, L2 distance, and cosine similarity. Any comments on this comparison? Thanks</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-501292">February 14, 2019 at 12:48 pm</a></p>
<p>  Are you asking me to run the comparison for you? While I’m happy to provide this code to you for free please keep in mind that I’m not going to run a bunch of additional experiments for you. This blog is here for you to learn from, to get value from, and better yourself as a deep learning and computer vision practitioner.</p>
<p>  I would highly encourage you to run the experiments and note the results. Let the empirical results guide you.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/0c20efc2c178862aa3ffaf6be7a3b464?s=48&d=mm&r=g">San Man</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-499181">February 2, 2019 at 12:06 pm</a></p>
<p>Hello Adrian,<br>Congratulations for your wedding!<br>Thanks so much for sharing your knowledge, it’s just incredible what you are doing.<br>I was going through your code. When I ran it, the faces which were there in the model were detected accurately. But the faces which were not there were detected wrongly as some one else.<br>I had about 10-12 images of each person.<br>Any idea on how I can reduce the false positives?</p>
<p>Thanks,<br>Sandeep</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-499743">February 5, 2019 at 9:33 am</a></p>
<p>  See <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/">this tutorial</a> for my suggestions on how to improve your face recognition model.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e67d8f890dadcfd32ad3fef8a7298419?s=48&d=mm&r=g">Hala</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-499325">February 3, 2019 at 10:35 am</a></p>
<p>hi Adrian,<br>many thanks for your efforts, i have a question please:<br>if i have many many images for many faces and i need to group it automatically by unique ids (grouping all faces to the same user in one unique id), how i can do it?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-499739">February 5, 2019 at 9:30 am</a></p>
<p>  See <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/07/09/face-clustering-with-python/">this tutorial on face clustering.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d289824d54baae6c7b900f9bd0a98fc4?s=48&d=mm&r=g">Roberto Marcoccio</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-499772">February 5, 2019 at 11:19 am</a></p>
<p>Hi, i tried to build my own face normalized dataset applying face alignment you described in the other topic, but that causes to have all 256×256 pixels “aligned” images ….Applying on that the extraction of embedding I noticed something was wrong because not all the images were processed. Debugging finally I got that the face detection of the extraction step of this module applied on the 256×256-sized images obtained cropping the ROI of the alignment step doesn’t work well. To confirm that I also just modified the routine cropping the ROI for each image from the face detection (without performing alignment) and saving it as new dataset and the extraction step just serialized 1 encoding !!!! Summarizing it seems that a further face detection applied on images already “detected” and saved with the dimension of the ROI doesn’t work. If that I don’t know how to apply alignment to normalize my face dataset. Could you pls help ???????????</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-500292">February 7, 2019 at 7:26 am</a></p>
<p>  I’m a bit confused regarding your pipeline. You performed face detection, aligned the faces, and saved the ROI of the face to disk, correct? From there all you need to do is train your model on the aligned ROIs (not the original images).</p>
<p>  If only 1 encoding is being computed then you likely have a bug in your code (such as the same filename is being used for each ROI and the files are overwriting each other). You may have a path-related issues as well. Double-check and triple-check your code as it’s likely a logic problem.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/ddc6793b79ee9fc718cf4f8c5bdfaabb?s=48&d=mm&r=g">Muhammad Hassam</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-499811">February 5, 2019 at 2:58 pm</a></p>
<p>hi Adrian thanks for this amazing post<br>i wondering if i could use this code on raspberry 3 pi b+ or not ?<br>i used this <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/25/raspberry-pi-face-recognition/">https://pyimagesearch.com/2018/06/25/raspberry-pi-face-recognition/</a> tutorial and this worked fine</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-500288">February 7, 2019 at 7:20 am</a></p>
<p>  Yes, but the face recognition will be very slow. You may also need to use a Haar cascade instead of a deep learning-based face detector.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5c0984f434f8857cd952f0fa1e34aad0?s=48&d=mm&r=g">Steve</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-569515">November 3, 2019 at 9:49 pm</a></p>
<p>  Muhammad, I have a raspberry pi and a camera located where I want to capture images and then the images are sent back to my main PC for processing. There are probably a lot of ways to do it, but this pyimagesearch topic got me on the right path: <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2019/04/15/live-video-streaming-over-network-with-opencv-and-imagezmq/">https://pyimagesearch.com/2019/04/15/live-video-streaming-over-network-with-opencv-and-imagezmq/</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/32d98e0b564017a3213e26a993691dec?s=48&d=mm&r=g">Vishal</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-501053">February 13, 2019 at 4:32 am</a></p>
<p>1)hey Adrian how can i self tuned this code can you guide?</p>
<p>2)and also in this code I not get Unknown Label to any unknown person?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-501305">February 14, 2019 at 12:59 pm</a></p>
<p>  Both of your questions can be address in <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/">this tutorial.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/32d98e0b564017a3213e26a993691dec?s=48&d=mm&r=g">Vishal</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-501206">February 13, 2019 at 11:56 pm</a></p>
<p>How the Self tuning will be done?<br>can you guide me about it.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a4bc224bb49119d319c20703453237c4?s=48&d=mm&r=g">Ucha Samadashvili</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-501430">February 14, 2019 at 6:55 pm</a></p>
<p>Hello Adrian,<br>firstly, I am grateful for your work. It has helped me for my Senior Design class project.<br>I want to ask you a question:<br>The way machine learning algorithms usually work (from what I understand) is, it gets trained on dataset allowing the algorithm to set weights. When training is done and we want to predict or classify we simply input the new data into a function which already has weights set. Effectively we do not have to compare the new data to all the previous data.<br>Now, the algorithm for face recognition you described has to look for a face at each frame and then encode it and then compare it to every single encoding in the database. While this is fine for my project since we are only 3 in the group and each has about 50 images in their face directories, it is relatively slow. Yes, I am running the program on a CPU and I understand it can be much faster. However, is there a way of training the machine in such a way that instead of going through each individual encodings (150 in my case) it can go through only 3 where each encoding is going to be some kind of average of one persons face. I know doing the avarage is kind of silly coz of angles and facial expressions etc. but there got to be a way for it work faster.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-501531">February 15, 2019 at 6:21 am</a></p>
<p>  There are a few questions here so let me answer them individually.</p>
<ol>
<li><p>Yes, many machine learning algorithms are trained on a set of data, any weights/parameters are set during the training, and the model is serialized to disk. Keep in mind we’re doing the same thing here though with just a few caveats. We have a pre-trained face recognizer that is capable of producing 128-d embeddings. We treat it as a “feature extractor” and then train a model on top of those 128-d embeddings. Think of the face embedder as a feature extractor and you’ll see how it’s just the same method.</p>
</li>
<li><p>You won’t obtain much of a speedup by training from scratch. The model will still need to perform a forward-pass to compute the 128-d embeddings. The only step you’re removing is the Linear SVM which will be pretty fast, regardless.</p>
</li>
<li><p>That said, if you want to train your own custom network refer to the documentation I have provided in the tutorial as well as the comments.</p>
</li>
</ol>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/fd5332518b2fc5cf8240c792b2a62fa5?s=48&d=mm&r=g">ucha</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-501581">February 15, 2019 at 1:14 pm</a></p>
<p>  I don’t think you understood my question. Perhaps, I did not phrase it correctly. Finding a face on each frame is very similar to what other machine learning algorithms do. What I was asking about is comparing the already embedded face to each and every face encoding in the database. To be precise, the efficiency of the voting system is under the question. I was wondering if it is possible to compare the encoded face from frame to some kind of average encoding of each person in the database.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-502538">February 20, 2019 at 12:52 pm</a></p>
<p>  It would be easier to instead <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/">perform face alignment</a>, average all faces in the database, and then compute the 128-d embedding for the face.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/077be0f39a6a7c60ca2ecb468723621e?s=48&d=mm&r=g">Neha Jain</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-502648">February 21, 2019 at 1:54 am</a></p>
<p>Hi Adrian.<br>Can this library be supported by Python 2.7 and Windows Operating system</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-502833">February 22, 2019 at 6:37 am</a></p>
<p> Technically yes, but you’ll need to install dlib on Windows. Please keep in mind that I don’t support Windows here on the PyImageSearch blog. I highly recommend you use a Unix-based OS such as Ubuntu or macOS for computer vision.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/6c4ae254327c088cd3c528919e0116d0?s=48&d=mm&r=g">Vasya</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-503059">February 23, 2019 at 1:53 pm</a></p>
<p>Hi Adrian!<br>Thank you so much for your work. Is there a way to add images of new people to an already trained system without running through all already existing images?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-503746">February 27, 2019 at 6:14 am</a></p>
<p> Yes, you can insert logic in the code to check and see if a face has already been quantified by the model (the file path would serve as a good image ID). If so, skip the image (but still keep the computed 128-d embedding for the face). The actual model will need to be retrained after extracting features.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/030d98bc0721f87cfd5f87a91cd0d1e8?s=48&d=mm&r=g">Huang-Yi Li</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-503462">February 26, 2019 at 1:34 am</a></p>
<p>If I want to update a new person into our model, whether this model can not be retrained.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-503719">February 27, 2019 at 5:45 am</a></p>
<p> Yes, the model will have to be re-trained if you add in a new person.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/06066a56b243fb4dbc24ff62731323a2?s=48&d=mm&r=g">Pankaj Kumar</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-504697">March 4, 2019 at 4:28 am</a></p>
<p>Hello Adrian,<br>can u please tell me why u passing unknown person images, this model itself should recognize unknown person if it not trained on that person….<br>i want to use SVC only..is their a way to achieve this</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-504982">March 5, 2019 at 8:47 am</a></p>
<p> The purpose of the unknown class is exactly that — label people as “unknown” if they are not in the training set. You can use an SVM with a linear kernel to obtain your goal.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/6164e359a9fb5a1e747ca4b6504367b0?s=48&d=mm&r=g">kharman</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-505787">March 9, 2019 at 10:35 am</a></p>
<p>will this post will help to create a project to recognize plants?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506570">March 13, 2019 at 3:54 pm</a></p>
<p> No, you should use a different type of machine learning or deep learning than that. If you’re new to the world of computer vision and image processing take a look at <a target="_blank" rel="noopener" href="https://pyimagesearch.com/practical-python-opencv/">Practical Python and OpenCV</a> which includes an introductory chapter on plant classification.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/9ada9d76d56192c7abee93a8cc8c6f46?s=48&d=mm&r=g">Sari</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-505794">March 9, 2019 at 11:21 am</a></p>
<p>Why linear svm classifier is better than knn classifier?<br>Which method is most effective when we have dataset and many faces?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506567">March 13, 2019 at 3:52 pm</a></p>
<p> Hey Sari — I cover machine learning concepts <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2019/01/14/machine-learning-in-python/">this tutorial.</a> That post will help address your question.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/7f3c62278bccbaac09bcd3f830058e07?s=48&d=mm&r=g">Mustapha Nakbi</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506080">March 11, 2019 at 10:09 am</a></p>
<p>Hi adrian,<br>I am not satisfied with the SVM trained model, can i define my own deepLearning network(using tensorflow) instead of svm to get better result?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506548">March 13, 2019 at 3:34 pm</a></p>
<p> Have you tried fine-tuning the existing face embedding model? that would be my primary suggestion.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/7f3c62278bccbaac09bcd3f830058e07?s=48&d=mm&r=g">Mustapha Nakbi</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506710">March 14, 2019 at 9:54 am</a></p>
<p>  I am using openface the same embedder model, how to make tuning ,please tell me.<br>  and, is’t possible after extracting the face region i will train the CNN with these regions?</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a0af53a27d84f929e1bd889ac1d85625?s=48&d=mm&r=g">Jaiganesh</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506264">March 12, 2019 at 5:12 am</a></p>
<p>Hi Adrain,<br>I am working for Face recognition feature implementation for Robot to recognize registered office members face. So, in order to recognize face, we can only capture “few pictures (max 5?)” from my office members and i will not able to collect more pictures of each and everyone. With these few samples, we will need to do the face recognition.</p>
<p>With this requirement in my hand, i found your previous post (<a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/</a>) on dlib with face_recogtion library and tested with few of my team member face pictures (“aligned” with <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/">https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/</a>), but it is not recognizing correctly as expected (identifying as wrong person). May be my team members are chinese and look similar?</p>
<p>So, here i need your advise and suggestion on which one to use?<br>Should you use this post application (OpenCV Face Recognition)? Or your previous post with dlib? for my above development scenario? Please suggest.</p>
<p>And regarding porting train_model.py script on dlib based face recognition application, i have copied recognizer.pickle and le.pickle from this post to other application with dlib on the same output directory. And also modified the train_model.py with “encodings” text to look for encodings.pickle file and ran the train_model.py script. But after this training model script, i see still the face recognition is not so accurate as expected for Robot. Please correct me if i did anything wrong here.</p>
<p>Please guide and help on this. Thank you.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506535">March 13, 2019 at 3:19 pm</a></p>
<p> I would try using dlib’s embedding model and then try training a Linear SVM or Logistic Regression model (from this post) on the extracted embeddings. I’ve found dlib’s model to be a bit more accurate.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/033912e2a7fb9fd1499af28b9fd283aa?s=48&d=mm&r=g">Johan</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-508301">March 21, 2019 at 7:55 am</a></p>
<p>  which post?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-508512">March 22, 2019 at 8:39 am</a></p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">This one.</a></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e82277641ca237ee59b9b4ce03465cdb?s=48&d=mm&r=g">tony</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506593">March 13, 2019 at 4:52 pm</a></p>
<p>Hi Adrian</p>
<p>Thanks a lot for such an informative post. I have followed the procedure to train my own set of images and recognize. I had put 6 images of a person in the folder dataset &gt; name of the person. My question is if the network cannot work effectively for the new set of images, how does it classify you or trisha for just 6 images ?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-507868">March 19, 2019 at 10:31 am</a></p>
<p> It’s not that the network will “never work” with a small image dataset — it’s that larger image datasets are always <em>preferred</em> for higher accuracy and reliability.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/80a2286a020e98bf7f46af5fec98f092?s=48&d=mm&r=g">Arbaz Pathan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-506848">March 15, 2019 at 10:33 am</a></p>
<p>I have done this project, and done it using webcam. Now when the frame window is opening it is giving an fps of 0.34 to 0.40 and it is lagging very much. Due to this we are not getting accurate output. The RAM consumption during this process is 92%. So please do tell us how to resolve this issue. Is this the problem of webcam or raspberry pi?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-507857">March 19, 2019 at 10:21 am</a></p>
<p> If you’re trying to perform face face recognition on the Raspberry Pi you should be following <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/25/raspberry-pi-face-recognition/">this tutorial instead.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/39ce36b5470dcd086f356208e1af4a7b?s=48&d=mm&r=g">Danny</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-508477">March 22, 2019 at 7:03 am</a></p>
<p>I have to use Deep learning classifiers instead of linear support vector classifier …how it can be done?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/39ce36b5470dcd086f356208e1af4a7b?s=48&d=mm&r=g">Danny</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-508482">March 22, 2019 at 7:37 am</a></p>
<p> Adrian , SVM is not satisfactory … could pls refer me a deep learning model to train on the embeddings…for better accuracy… and if any new face is detected it is not recognizing as unknown…</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-508494">March 22, 2019 at 8:24 am</a></p>
<p>  Hi Danny — you’ll definitely want to read <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/">this tutorial</a> where I share my suggestions on obtaining higher face recognition accuracy.</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/fc5db8e72efe50223bb3046b422a7b14?s=48&d=mm&r=g">Tara Prasad Tripathy</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-509008">March 25, 2019 at 4:17 am</a></p>
<p>Hi Adrian,<br>I was wondering whether the dlib pipeline which you wrote in another post, takes care of face alignment or do we have to incorporate it?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-509428">March 27, 2019 at 8:55 am</a></p>
<p> No, you need to manually perform face alignment yourself. Refer to <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/">this tutorial on face alignment.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/91ed5366e48112b91a116efadecf6cee?s=48&d=mm&r=g">fajar yuda pratama</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-509907">March 29, 2019 at 9:10 am</a></p>
<p>can we train new face without delete last train? for add face data without train all of face again</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-510611">April 2, 2019 at 6:15 am</a></p>
<p> I have addressed that comment in the comments section a few times, please give the comments a read.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/77be938c31a0df5e77feedaac8024824?s=48&d=mm&r=g">mithil</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-510817">April 3, 2019 at 1:21 am</a></p>
<p>how to improve the accuracy ??? for me the result is in-accurate. wrong prediction for faces</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-511128">April 4, 2019 at 1:27 pm</a></p>
<p> Kindly take the time to read the tutorial. I cover your question in the “Drawbacks, limitations, and how to obtain higher face recognition accuracy” section.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/f210f71654aadd78fd768f04e0892327?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://university/">qusay</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-512007">April 9, 2019 at 11:26 am</a></p>
<p>Hi Adrian .. i have project on face recognition but i can not know how to dteremine the percentage of error of algorithms that used in face recognition .and what is the best algorithm for face recognition .Thanx</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/8b3bcc677b86c738f4b101c8119ee4e9?s=48&d=mm&r=g">Victoria</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-512044">April 9, 2019 at 3:30 pm</a></p>
<p>Hey Adrian,</p>
<p>Thank you so much for this guide! My question is, do I need to input the names of the folders into the code where it says “name” or will that assign itself automatically? For example, in lines 47-53, can I leave it as “name” or should I say “adrian = imagePath.split(os.path.sep)[-2]” instead? I’m also having difficulty setting everything up, I’m new to OpenCV but I believe I have version 3.4.7. is this okay or should I get 3.4.2 instead? Thanks in advance!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-512506">April 12, 2019 at 12:18 pm</a></p>
<p> You don’t have to change the code, just update the names of the directories and the images inside each of the directories.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/1d999c0924278eab9677b5818acc8679?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://gmail.com/">sivaparvathi</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-512130">April 10, 2019 at 3:56 am</a></p>
<p>Hi Ardian,</p>
<p>Is it possible when the unknown person is came ,it detects unknown and generating Id for him/her. If again same that unknown person will come,It have to show previous generated Id .</p>
<p>If possible please resolve my issue.Thanks in advance</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d1b65d1602b4578055b667f913699a72?s=48&d=mm&r=g">Zheng Li</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514139">April 22, 2019 at 9:13 am</a></p>
<p>Hi, Adrian,</p>
<p>Did you had test the LightenCNN face recognizition model(<a target="_blank" rel="noopener" href="https://github.com/AlfredXiangWu/face_verification_experiment">https://github.com/AlfredXiangWu/face_verification_experiment</a>)? How about it compared to OpenFace,and dlib’s embedding mode?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514706">April 25, 2019 at 8:57 am</a></p>
<p> I have not tested that model, I am not familiar with it.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/07e581b9b8196cc9dd8d9a52017a625a?s=48&d=mm&r=g">唐国梁</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514182">April 22, 2019 at 1:50 pm</a></p>
<p>Thanks a lot . it is really helpful. It worked well.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514702">April 25, 2019 at 8:55 am</a></p>
<p> You’re welcome, I’m glad you found it helpful!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5ec2d7c7a8523b7dc3281ad7232651ff?s=48&d=mm&r=g">Quek Yao Jing</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514635">April 25, 2019 at 3:14 am</a></p>
<p>Hi, Adrian I am a fan of your blog. Your blog had really helped me learn OpenCV a lot. From the previous tutorial, DLIB is used for face detection and k-nearest neighbor is used for a face recognition/classification. While in this tutorial OpenFace is used for face detection and SVM is used for face recognition and classification.</p>
<p>From what I had experience from your code, face detection is more accurate using DLIB while SVM is better in the classification of faces. So, now I am planning on using 128-d embeddings generated from DLIB and use SVM for classification.</p>
<p>My question is if I used this method, will the false positive still occurs if I will need to recognize the 1000-10,000 of people? Because I just wonder how the API like AWS, Microsoft Azure can get really good accuracy despite so many people is using their API.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514664">April 25, 2019 at 8:36 am</a></p>
<p> For 1,000-10,000 people you should really consider fine-tuning the model rather than using the pre-trained model for embeddings. You will likely obtain far better accuracy.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/5ec2d7c7a8523b7dc3281ad7232651ff?s=48&d=mm&r=g">Quek Yao Jing</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514728">April 25, 2019 at 9:16 am</a></p>
<p>  Thanks for your reply Dr Adrian, what does fine-tuning the model means? Does it mean we need to retrain the K-NN or SVM model for the classification process or we need to retrain a custom model for face detection? Because it seems like dlib doing a good job detect face inside the image.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514755">April 25, 2019 at 9:40 am</a></p>
<p>  No, fine-tuning the model means taking the existing model weights and re-training it on faces/classes it was not originally trained on. <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/05/14/a-gentle-guide-to-deep-learning-object-detection/">This post</a> covers fine-tuning in the context of object detection — the same applies to face recognition as well.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/5ec2d7c7a8523b7dc3281ad7232651ff?s=48&d=mm&r=g">Quek Yao Jing</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-514758">April 25, 2019 at 9:49 am</a></p>
<p>  Thanks Dr Adrian. I will check on your post. I had googled a bit and found out that it seems to be related to Deep Transfer Learning (DTL).</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/54d5b8e9208c10e60499d2d4279eaf60?s=48&d=mm&r=g">Hamdi Abd</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-515180">April 28, 2019 at 8:31 pm</a></p>
<p>why i get this issue !!<br>serializing 0 encodings . . .<br>what i should do !!</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-515612">May 1, 2019 at 11:49 am</a></p>
<p> It sounds like the path to your input directory of images is not correct. Double-check your file paths.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/795f1a46a158a7cd900581c80acd6da6?s=48&d=mm&r=g">Elijah</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-516648">May 8, 2019 at 3:25 pm</a></p>
<p>Hi, which of the recognition methods is more efficient? This tutorial or the previous?</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/862a7fa2a346399b41484f7252e83573?s=48&d=mm&r=g">Marcello Beneventi</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-517276">May 12, 2019 at 6:25 am</a></p>
<p>Hi Adrian,</p>
<p>Thanks for such awesome blogs and I really learnt many concepts from you. You are kind of my Guru in computer vision.</p>
<p>I needed a little help, I am trying to combine face recognition and object detection both in single unit to preform detection on single video stream. How I am suppose to load 2 different model to process video in single frame? Kindly help.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-517809">May 15, 2019 at 3:01 pm</a></p>
<p> I would suggest you take a look at <a target="_blank" rel="noopener" href="https://pyimagesearch.com/deep-learning-computer-vision-python-book/">Raspberry Pi for Computer Vision</a> where I cover object detection (including video streams) in detail. I’d be happy to help but make sure you read the book to understand the concepts first.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/36eab9d9ae30563e69b03af1df915332?s=48&d=mm&r=g">Taka</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-517327">May 12, 2019 at 2:01 pm</a></p>
<p>Thanks Adrian, you’re just a rare being….</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-517804">May 15, 2019 at 2:58 pm</a></p>
<p> I assume that’s a good thing? 😉</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/ca7c7c2256b6729332a0a2e06c881d8f?s=48&d=mm&r=g">Yusuf Yasin</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-517384">May 13, 2019 at 12:04 am</a></p>
<p>Hey all,</p>
<p>I downloaded the code and made sure all the dependencies and libraries were installed. Unfortunately, whenever i run the code it works for the first couple of seconds identifying faces perfectly, then after a few seconds it causes the PC to crash resulting in a hard reboot.</p>
<p>Has anyone else been facing this problem. If so any help is much appreciated, thanks!</p>
<p>I’ve already posted this once before but it doesn’t seem to appear to be posted.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-517800">May 15, 2019 at 2:56 pm</a></p>
<p> That’s definitely odd behavior. What are the specs of your PC? And what operating system?</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/950d1207c1f6be14d91eee39ac605149?s=48&d=mm&r=g">bagas</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-517883">May 15, 2019 at 8:52 pm</a></p>
<p>Hey Adrian, i use 9500 class data train and success, but i running script recognize_video.py very slowly. can help?</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e9f998df92b979c497155fe7325ccc34?s=48&d=mm&r=g">quynhnttt</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-517915">May 16, 2019 at 3:26 am</a></p>
<p>Hi adrian<br>Can I do this project with IP camera?<br>thank you</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5b72e542cc0edfa62b81434393063980?s=48&d=mm&r=g">david</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-518191">May 18, 2019 at 11:35 am</a></p>
<p>Hi Adrian<br>Can I do this project with camera IP or camera USB</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a0c010949f6ddd07b244c9b805a950aa?s=48&d=mm&r=g">Mohamad</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-519197">May 25, 2019 at 6:31 am</a></p>
<p>Hi Adrian</p>
<p>when i run the code i get this error</p>
<p>AttributeError: ‘NoneType’ object has no attribute ‘shape’</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/a0c010949f6ddd07b244c9b805a950aa?s=48&d=mm&r=g">Mohamad</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-519199">May 25, 2019 at 6:33 am</a></p>
<p> the error located in line 50 image = imutils.resize(image, width=600)</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-519968">May 30, 2019 at 9:31 am</a></p>
<p>  Your path to the input image is correct and the returned image/frame is None. Double-check the path to your input file.</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/591547bb8a684e01255d7f703b7c3b6f?s=48&d=mm&r=g">jon</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-520474">June 2, 2019 at 2:23 am</a></p>
<p>thanks for this awesome tutorial!<br>would you point me to the right direction on how i can track the learning of this model?</p>
<p>thanks again</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-521039">June 6, 2019 at 8:30 am</a></p>
<p> What do you mean by “track the learning”?</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/7a5b63029d238d750844971e7b8099b4?s=48&d=mm&r=g">Michael Maher</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-520555">June 2, 2019 at 5:23 pm</a></p>
<p>Hey Mr.Adrian</p>
<p>Thank you so much for all your hard work, sharing with us all this knowledge.</p>
<p>You published many face recognition methods, which one would you consider the most accurate? I am building a project where I want to depend on “face unlock” to unlock/open,</p>
<p>What is the best method suitable for this in your opinion?</p>
<p>Thanks.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-521033">June 6, 2019 at 8:07 am</a></p>
<p> It depends on the project but I like using the <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">dlib face recognition embeddings</a> and then training a SVM or Logistic Regression model on top of the embeddings.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a8d13d40a4e9bc09904310fc74a68313?s=48&d=mm&r=g">kayle</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-520747">June 4, 2019 at 9:30 am</a></p>
<p>great work !! this program can recognise only human faces no ? cause i am wondering if i can use it to recognise my plants on real streaming time is it possible to do that just by giving pictures of my plants ? if its not the case can you give the path of a program that can do that please</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-521003">June 6, 2019 at 6:53 am</a></p>
<p> No, this method is used only for face recognition.</p>
<p> For plant recognition I would recommend either <a target="_blank" rel="noopener" href="https://pyimagesearch.com/deep-learning-computer-vision-python-book/">Deep Learning for Computer Vision with Python</a> or <a target="_blank" rel="noopener" href="https://pyimagesearch.com/pyimagesearch-gurus/">the PyImageSearch Gurus course</a> (both of which cover plant recognition).</p>
<p> I hope that helps!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e142ea9ee3b7df2a20c48dfc14919858?s=48&d=mm&r=g">Abay</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-521285">June 8, 2019 at 10:23 am</a></p>
<p>Adrian, thank you for such a great article!</p>
<p>I tried to use my own trained model on PyTorch and exported into ONNX. However when I try to read it with OpenCV I get errors. I found that overall people have problems with importing deep learning models into cv.dnn (Keras, PyTorch(Onnx), etc).</p>
<p>So my question is: isn’t it better to have a separate microservice (e,g. flask + PyTorch) which will serve requests coming from an app which probably uses OpenCV to send image/images(in case of video stream). How such architecture will differ in terms of speed compared to the case when open cv uses a pretrained model as you showed above.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-521691">June 12, 2019 at 1:49 pm</a></p>
<p> The OpenCV, PyTorch, Keras, TensorFlow, ONNX, etc. ecosystem has a long way to go. It’s a good initiative but it can be a real pain to convert your models.</p>
<p> You can technically use a microservice but that increases overhead due to HTTP requests, latency, etc.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/6ed83c97290c420eedddea58f9ffd36e?s=48&d=mm&r=g">Pyro</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-521736">June 12, 2019 at 9:06 pm</a></p>
<p>Hello Adrian, when i download and use your trains and code without changing anything with adrian.jpg. There are a lot of squares in your face and all like %50 adrian. There are like 7-8 squares (adrians). I gave it a try with my photos, added like 40 photos, removed outputs.</p>
<p>First i used extract_embeddings.py, then i trained it with the script and i tried to recognize and result is same. There are like 7-8 squares in my face and all are %40-50.</p>
<p>Help me please :/</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-521787">June 13, 2019 at 9:36 am</a></p>
<p> The fact that there are multiple face detections which is the root of the issue. What version of OpenCV are you using?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/6ed83c97290c420eedddea58f9ffd36e?s=48&d=mm&r=g">Pyro</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-521797">June 13, 2019 at 11:12 am</a></p>
<p>  Hello Adrian, i use OpenCV 4.0. i took a look at your <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/">https://pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/</a> but i couldn’t figure how to use both together.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-522364">June 19, 2019 at 2:28 pm</a></p>
<p>  I would suggest taking a step back. Start with a fresh project and apply <em>just</em> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/">face detection</a> and see if you are able to replicate the error.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/688eac7077211067ae7a840e1f0b04b3?s=48&d=mm&r=g">Khoa</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-522714">June 23, 2019 at 3:04 pm</a></p>
<p>Hello,</p>
<p>I ran your code successfully. However, in some cases, I want to filter the images with lower confidence. For example, the code recognizes two people as me with the confidence 98.05% and 92.90%. How can I filter the ones below 95% ?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-523165">June 26, 2019 at 1:30 pm</a></p>
<p> All you need is an “if” statement. Check the confidence and throw out the ones that are &lt; 95%.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5d08cc9c16d1be4db348db058df1e2f7?s=48&d=mm&r=g">kumar sahir</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-522842">June 24, 2019 at 11:33 am</a></p>
<p>Dear adrian,<br>first thank you for your excellent tutorial it is very helpful, I am PhD student in computer science, I saw your tutorial about facial recognition, I was very interested in your solution, and i want to know if it is possible to make the search on web application (From web Navigator) instead of using shell commande, thnak you very much</p>
<p>Cordially</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-523150">June 26, 2019 at 1:18 pm</a></p>
<p> Yes, absolutely. I would recommend you wrap the project in a REST API. <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/">This tutorial</a> would likely be a good start for you.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d35c797a51d50787d326144245205610?s=48&d=mm&r=g">sizbro</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-522849">June 24, 2019 at 12:30 pm</a></p>
<p>Hey Adrian, I know its been a while since you answered a question on this post, but I have one lingering curiosity. I have been trying to add members of my own family to the dataset so it can recognize them. Right now I have been having an issue with the labels as it still shows either ‘Adrian’ or ‘Trisha.’ Do you know how I can edit the labels so that the names of my family are there instead?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-523147">June 26, 2019 at 1:17 pm</a></p>
<p> I’m not sure what you mean by it’s been “awhile”. I regularly comment and help readers out on this post on a weekly basis. You should take a look at the recent comments before making such a statement 😉</p>
<p> As for adding family members you need to:</p>
<ol>
<li><p>Delete the “Adrian” and “Trisha” directories and add in your respective family members  </p>
</li>
<li><p>Extract the facial embeddings from your dataset  </p>
</li>
<li><p>Train the model</p>
<p>From there your family members names will show up.</p>
</li>
</ol>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/3120253ce41a8a43a06ccb06f34e65ee?s=48&d=mm&r=g">Mudassir Ahmed Khan</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-525142">July 10, 2019 at 4:01 am</a></p>
<p>What are command line arguments and their parsing?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-525171">July 10, 2019 at 9:32 am</a></p>
<p> You can read about command line arguments <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/03/12/python-argparse-command-line-arguments/">in this tutorial.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d7234df5deec5567e23c575aaf0f7dce?s=48&d=mm&r=g">Khang</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-525653">July 14, 2019 at 5:40 am</a></p>
<p>How do I apply facial landmarks on this tutorial to increase accuracy of recognition?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-527287">July 25, 2019 at 10:06 am</a></p>
<p> You can use them to perform <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/">face alignment.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/06f302054df1716601d88ef472798e08?s=48&d=mm&r=g">Rajath Bharadwaj</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-526297">July 19, 2019 at 3:57 am</a></p>
<p>I was wondering how to recognize multiple faces. Could you give me some leads on that? That’d be great and very helpful.</p>
<p>And thank-you for all your great tutorials and codes. Really, it’s helped me a lot!<br>Thanks once again.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-527259">July 25, 2019 at 9:42 am</a></p>
<p> This method does work with multiple faces so perhaps I’m not understanding your question?</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/4e33e2197422c731da797543d3108590?s=48&d=mm&r=g">Jose Fierro</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-526545">July 20, 2019 at 8:08 pm</a></p>
<p>Congrats!!, Great tutorial Master.</p>
<p>I just have a question, each time you add a new person, do need to train again the SVM or exists another way?</p>
<p>Thanks</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-527250">July 25, 2019 at 9:35 am</a></p>
<p> If you add a new person you will need to retrain the SVM.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5348c75918f8f0928d9af5cc4c9c8674?s=48&d=mm&r=g">Amos Cheung</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-527328">July 25, 2019 at 12:26 pm</a></p>
<p>Hi! First of all thanks for the tutorial. I just have one question. Is there anyway to construct the code so that all new faces will be recognized as “unknown” rather than having to add data in there. Thanks</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/5c0984f434f8857cd952f0fa1e34aad0?s=48&d=mm&r=g">Steve</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-569516">November 3, 2019 at 9:56 pm</a></p>
<p> It will already do this. Each image gets converted into an embedding (a bunch of numbers). Each person will have a pattern to their embeddings. If you have enough images, the SVM will pick up on those patterns. Since the “unknown” folder has a ton of random images in it, those embeddings will be all over the place — it won’t have an easy to model pattern. So it should learn, on its own, that if a face is “weird” it should be labelled unknown.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d01c231f9284829b458d6b305bcd4f62?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://www.practicallifereflection.com/">Pawan</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-527408">July 26, 2019 at 4:16 am</a></p>
<p>Hi adrian!! I am a big fan of your work and although it is too late i wish you a happy married life.I was wondering , can we combine your open cv with face recognition tutorial(this tutorial) with the pan-tilt motor based face recognition tutorial and enhance the fps with movidius ncs2 tutorial(on raspberry pi) to make a really fast people identification raspberry pi system which can then be utilized for further projects.I just wanted to know whether it can be done or not and if it can be done, how should i go ahead with it ?I have already applied and made these projects separately in different virtual environments, now i need to somehow integrate it.Thanks for your help in advance.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/8a83f2ab071ca1fd9c5d05ab55345c79?s=48&d=mm&r=g">Bruce Young</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-527609">July 27, 2019 at 3:32 pm</a></p>
<p>Thanks for your great tutorials. 🙂</p>
<p>I had an error while training that several others have had.</p>
<p>“AttributeError: ‘NoneType’ object has no attribute ‘shape’”</p>
<p>My path to the training images folder(s) was fine.</p>
<p>For my case at least, the issue was that I am doing the tutorials on a Linux machine but I collected the images using my Mac and then copied the folders across the network to the Linux machine. That process copies both the resource and data forks of the image files on the Mac as well as the Mac .ds_store file. Many of these files are hidden. Once I made fresh dataset image folders and copied the training images into them using the Linux machine, all was good.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-532776">August 7, 2019 at 1:04 pm</a></p>
<p> Thanks for sharing, Bruce!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/8d91404151760bd70274a67e8b5f64eb?s=48&d=mm&r=g">Fouk</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-528089">July 29, 2019 at 4:30 am</a></p>
<p>how to use face-recognition with gpio in raspi</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-532763">August 7, 2019 at 12:59 pm</a></p>
<p> That exact question is covered inside <a target="_blank" rel="noopener" href="https://pyimagesearch.com/raspberry-pi-for-computer-vision/">Raspberry Pi for Computer Vision.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/38cd39c8e11c0a3f7024dd2a6d775d3e?s=48&d=mm&r=g">Laxmi Kant</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-538205">August 15, 2019 at 2:46 am</a></p>
<p>Hi Adrian,<br>Thank you so much for developing it quick and easy stuff with OpenCV.<br>I am using it on Windows-10 machine, it worked great.<br>Thank you once again for creating it.<br>lax</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-539189">August 16, 2019 at 5:32 am</a></p>
<p> Thanks Laxmi!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5709ea0dc18a6509e9d31806eed7433c?s=48&d=mm&r=g">john</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-540550">August 20, 2019 at 3:22 am</a></p>
<p>Hi, Andrian!</p>
<p>U can help me to assign the picamera to on Jetson Nano for videostream face recognition?<br>The real issue is that I can install “picamera[array]” on my Jetson board.</p>
<p>Thanks!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a4f588f318c8fab8fa1daec1bf4e9338?s=48&d=mm&r=g">Ajmal</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-545037">August 25, 2019 at 4:04 am</a></p>
<p>Hello Adrian! Thanks a lot for these tutorials. Your tutorials have been my first intro to Computer Vision and I have fallen in love with the subject! I’m on my way to do my M.Sc. with a focus in computer vision now, and it all started from your tutorials! 😀</p>
<p>I’m aware this may not be the right place for this question, but wanted to know your take on it regardless:</p>
<p>How well does SVM scale? I tried to do a test with dummy vectors, and the training time seems to scale exponentially. Have you had any experiences in scaling this for large datasets (in the order of tens of thousands of classes perhaps)?</p>
<p>Also, what is your opinion on using Neural Networks for the classification of the embeddings as opposed to k-nn (perhaps with LSH) or SVM for scalability?</p>
<p>Any tips/information or links to resources would help me a lot! Thank you once again for these wonderful tutorials!</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/42cc35b843d37962c7e8a7916099ee75?s=48&d=mm&r=g">Pankaj</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-546127">August 31, 2019 at 1:21 pm</a></p>
<p>Hey Adrian. Thank you for this amazing tutorial. Loved it. I’m working on a project where I’m supposed to recognize the faces of the moving people. Like people approaching my front door or maybe people in a locality , given I have the dataset of that locality. I’m supposed to identify unknown person accurately for safety purposes. Can you please help me on this. How can I use this tutorial in doing that .</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-547160">September 5, 2019 at 11:09 am</a></p>
<p> That exact project is covered inside <a target="_blank" rel="noopener" href="https://pyimagesearch.com/raspberry-pi-for-computer-vision/">Raspberry Pi for Computer Vision.</a> I suggest you start there.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/ed1f7d13bcdc64c1ff00f62174d5e769?s=48&d=mm&r=g">Nick Kim</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-548945">September 8, 2019 at 10:40 pm</a></p>
<p>Hi Adrian,</p>
<p>Great post as usual but wondering why SVM is used for classifying rather than a fully connected neural network with softmax activation?</p>
<p>Thanks,</p>
<p>Nick</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-550348">September 12, 2019 at 11:42 am</a></p>
<p> You could create your own separate FC network but it would require more work and additional parameter tuning. It’s easier and more efficient to just use a SVM in this case.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5c3fe2975aadc5f22ed393d31e62c4a8?s=48&d=mm&r=g">Shashi Kiran</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-549844">September 11, 2019 at 4:15 am</a></p>
<p>Hi Adrian, Thanks for sharing such wonderful blogs on OpenCV, DL. Very useful, informative, educational and well presented in layman terms. I have learnt a few things so far thru your articles.</p>
<p>My question : Let’s say I have trained my engine with 20 or more images and some images were not well interpreted by the engine. How would I know that ?</p>
<p>Is there someway the engine will try to digest it and throw it out with some result that says image is 90% good, or 10% good/bad.( It could be image is blurred, image where the eyes are closed, streaks in images, rotation was not aligned and engine could not process, and other things that you mentioned in the “Drawbacks” section of your article – things like that )<br>These lead to bad probability which I can avoid by creating a second set of well crafted images.</p>
<p>Was hoping to hear your opinion on it.<br>I have about 40 images of a single person and yet I get 20 or 30% probability and sometimes wrong names during recognition !! so I am thinking my set is no good – but which image is bad in my set ? I need to be able to identify that so that I can train my engine with a better set of photos.</p>
<p>Again thanks for wonderful articles.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a3111b30d53917cb4ad0d15a38936614?s=48&d=mm&r=g">John</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-551635">September 15, 2019 at 7:35 pm</a></p>
<p>Hi Adrian, thanks for the tutorial. I have a question about processing speed. Is there any way that the forward() function speed can be improved or why does this take the most time? When running this on a Raspberry Pi, it seems to be the bottleneck of the recognition. Makes things especially harder when trying to recognize faces in frames from a live video stream.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-552810">September 19, 2019 at 10:06 am</a></p>
<p> Hey John — take a look at <a target="_blank" rel="noopener" href="https://pyimagesearch.com/raspberry-pi-for-computer-vision/">Raspberry Pi for Computer Vision</a> where I show you how to perform face recognition in real-time on the RPi.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/c1bc36fed66d402357a9aa23a9aa9c49?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="https://www.wahanajayagroup.com/">Kontraktor Kolam Renang</a></p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-587381">December 1, 2019 at 9:29 am</a></p>
<p> I also have a question similar to this. And thanks Adrian for the answer.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d2cf8e758c1f9bf09a3e68bea07ff384?s=48&d=mm&r=g">Abhishek Gupte</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-553505">September 22, 2019 at 4:44 pm</a></p>
<p>Hey Adrian,<br>I’m facing a rather weird problem. When I train one of my friend’s face BUT NOT MINE, the program still recognizes my face as my friend’s with an above 50% probability but when I train BOTH OUR FACES, it recognizes my face correctly as an almost equal probability as in the former case. What seems to be the problem?<br>P.S I also tried experimenting with different values of C but to no avail.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/17d4f287e7d456b2493ebeb721d3faf8?s=48&d=mm&r=g">Joe</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-557089">October 2, 2019 at 11:26 pm</a></p>
<p>Hi Adrian,</p>
<p>Can this work with greyscale images? I.e. can cv2.dnn.blobFromImage still be used with non RGB images. Asking this because I want the recognition to not be dependent on lighting (if lighting actually even affects this)</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-557424">October 3, 2019 at 12:16 pm</a></p>
<p> Just stack your grayscale image to form an RGB image representation:</p>
<p> <code>image = np.dstack([gray] * 3)</code></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d2cf8e758c1f9bf09a3e68bea07ff384?s=48&d=mm&r=g">Abhishek Gupte</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-563563">October 9, 2019 at 5:27 pm</a></p>
<p>Hey Adrian,<br>First of all great post, now since you reply to so many people, I won’t take much of your time.<br>I have just one question. Can an image size(resolution, size on disk) disparity between dataset and camera feed or between images in the dataset make a difference to the probability? I get varying like 20% difference whenever I run the webcam-recognizer script at different times, with the same trained face under same light conditions. With stranger faces it varies just as much and so i can’t set an exact threshold of probability. Some of the sizes on disk for the images is 5 Kb whereas some 200 Kb. Also the images coming from the feed each equal 70kb. Please comment as it’s causing the greatest hindrance. So close I am to building a face recognition system yet this gnawing problem.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-563643">October 10, 2019 at 10:10 am</a></p>
<p> The image size on disk doesn’t matter but the image resolution <em>does</em> matter. If your image is too small there won’t be enough information to accurately recognize the face. If the image is too large then there will be too many fine-grained details and it could potentially “confuse” your model. Typically input face recognition resolutions are in the 64×64 to 224×224 pixel range.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/3980fc782797298c6a79526211562850?s=48&d=mm&r=g">Abhishek Gupte</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-563669">October 10, 2019 at 11:42 am</a></p>
<p>  What is basically the difference between the resolutions of your camera feed and dataset(the one containing pics of you and your wife and the unknowns)?</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/2e8c2b2072f002ac31ed3094b837a61b?s=48&d=mm&r=g">Harshit</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-563742">October 11, 2019 at 3:59 am</a></p>
<p>Just One Question——</p>
<p>Why did we make a blob of the face first, could’nt we directly pass the image to the embedder after resizing it to fit the input_size of the first layer to the embedder.</p>
<p>Is it necessary to make the blob??</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-564823">October 17, 2019 at 8:07 am</a></p>
<p> OpenCV requires that we create the blobs when using models loaded using OpenCV’s “dnn” model. We need to blobs in this example:</p>
<ol>
<li>One blob when performing face detection  </li>
<li>We then create separate blobs for each detected face</li>
</ol>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/8acf0136329615c5cc834b792c1b9a80?s=48&d=mm&r=g">manideep</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-564062">October 14, 2019 at 1:53 am</a></p>
<p>Thank you so much bro</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-564807">October 17, 2019 at 7:57 am</a></p>
<p> You are welcome!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/17d4f287e7d456b2493ebeb721d3faf8?s=48&d=mm&r=g">Joe</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-564506">October 15, 2019 at 10:34 pm</a></p>
<p>Hi Adrian,</p>
<p>I am looking to improve the method and am starting with preprocessing of images, specifically face alignment. If face alignment is used to preprocess the images, is there an effect on classifying test images if the face in the image is not completely horizontal (i.e. the eyes lie in the same y-coordinates)?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-564798">October 17, 2019 at 7:50 am</a></p>
<p> It <em>can</em> effect the accuracy of the faces are not aligned. They don’t need to be perfectly aligned, but the more aligned they are, the better.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/17d4f287e7d456b2493ebeb721d3faf8?s=48&d=mm&r=g">Joe</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-564887">October 17, 2019 at 1:17 pm</a></p>
<p>  My question may have been unclear. If the training data is aligned, but the face in the test image is not aligned, is that an issue?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-567126">October 25, 2019 at 10:30 am</a></p>
<p>  The embedding models tend to be pretty robust so it’s not the “end of the world” but if you’re aligning the training data you should also try to align the testing data.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/a3111b30d53917cb4ad0d15a38936614?s=48&d=mm&r=g">John</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-564882">October 17, 2019 at 12:51 pm</a></p>
<p>Hello,</p>
<p>For the unknown dataset, is it better to have many pictures of a few people (say 6 different people with 10 pictures each) or as many random people as possible (say 60 different people rather than 6 sets of 10 pictures per person)?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-567127">October 25, 2019 at 10:31 am</a></p>
<p> That really depends on your application. I prefer to have examples of many different people but if you know for a fact there are people you are not interested in recognizing (perhaps coworkers in a work place) then you should consider gathering examples of <em>just</em> those people.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/7893dfdb872cc8d4bd3a0fab8a982764?s=48&d=mm&r=g">Tien</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-565231">October 18, 2019 at 10:47 pm</a></p>
<p>Hi Adrian,<br>Thanks for your tutorial, it helps me so much to start learning deep learning and face recognition. From this tutorial, i try to improve accuracy by using dlib’s embedding model but have a low accuracy. So i just want to ask one question that if i extract embeddings by using dlib and face_recognition with my dataset, i will use dlib and face_recognition again in step #3 to extract embeddings instead of the model used in this tutorial. It’s right?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-567122">October 25, 2019 at 10:28 am</a></p>
<p> Yes, you must use the <em>same</em> face embedding model that was used to extract embeddings from your training data.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d0ec7c58e225d7de85eaf121b25ea4fa?s=48&d=mm&r=g">Safi</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-567654">October 27, 2019 at 4:00 pm</a></p>
<p>Hi <em>Adrian</em>,</p>
<p>I’ve a questions if you have ever had times to wrote a tutorial about detecting faces using IP camera.<br>I’m working on something to detect multiple faces &amp; gender using IP camera (rtsp).</p>
<p>if you do have this tutorial please share the link with down below. thanks I really appreciate your effort.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5c0984f434f8857cd952f0fa1e34aad0?s=48&d=mm&r=g">Steve</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-569518">November 3, 2019 at 10:24 pm</a></p>
<p>If I put a ton of unknown images in the unknown folder, it starts predicting that everyone is unknown. I can fix this by either remove unknowns or by just copy/pasting all the images in my other folders so they’re roughly equal (or having it copy the embeddings multiple times for the same effect). Any thoughts on which is better?</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/3506b4c89abda39cf0956e12883973e1?s=48&d=mm&r=g">Yusra Shaikh</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-573746">November 15, 2019 at 3:54 am</a></p>
<p>Hi Adrian</p>
<p>This tutorial worked perfectly! i was amazed at how easy it was. All thanks to your detailed explanation. I wanted to extend this project to detect intruders, and raise an alert via SMS. Can you help me just a general overview of how this can be done?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-576863">November 21, 2019 at 9:24 am</a></p>
<p> Take a look at <a target="_blank" rel="noopener" href="https://pyimagesearch.com/raspberry-pi-for-computer-vision/">Raspberry Pi for Computer Vision</a> which covers that exact project — detecting intruders and sending an SMS alert.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/64a436a1f4a53de421d20cda470e064e?s=48&d=mm&r=g">Will</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-585843">November 28, 2019 at 5:31 am</a></p>
<p>Hi Adrian, thank you for the amazing tutorial.</p>
<p>I have a question and I would like to hear your oppion.</p>
<p>Do you think the reason of the “unknown” prediction when you was wearing a sunglasses is because OpenFace use eye-aligned for preprocessing the input image, so when we wear sunglasses OpenFace cannot align our face correctly that result in low accuracy?</p>
<p>What I mean hear is that although we add more data of people wearing sunglasses in the dataset, maybe the accuracy would not be improved because the OpenFace algorithm cannot perform eye-aligned.</p>
<p>If yes, how would you overcome this problem?</p>
<p>Again, thank you for the nice post. Have a nice day</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/172d4f00689d259a709b23a4c071028c?s=48&d=mm&r=g">Meg</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-587063">November 30, 2019 at 11:31 am</a></p>
<p>Hiii. Does this project detect faces in real time?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-589881">December 5, 2019 at 10:35 am</a></p>
<p> Yes, it certainly does.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/ae1288eb05c74189dd8c8f9236ddb6a3?s=48&d=mm&r=g">AKBAR HIDAYATULOH</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-591259">December 11, 2019 at 2:55 am</a></p>
<p>hi, thanks for the great tutorials. I want to ask, how can i capture face recognition only one time for detected faces as long as the faces are inside the frame, so the captured faces are not every frame, can you give some advice. thanks</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-591596">December 12, 2019 at 10:06 am</a></p>
<p> Try using <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/">basic centroid tracking.</a> Each bounding box will have a unique ID that you can use to keep track of each face.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/bc4a2e2d6f818701459f0b3d4a43ffcb?s=48&d=mm&r=g">Aonty</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-646991">January 13, 2020 at 12:50 pm</a></p>
<p>How can I use it for attendance system</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-656562">January 16, 2020 at 10:33 am</a></p>
<p> I would suggest you read <a target="_blank" rel="noopener" href="https://pyimagesearch.com/raspberry-pi-for-computer-vision/">Raspberry Pi for Computer Vision</a> which covers how to build a custom attendance system.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/7f17aa344c89591b5e7131de0d959d50?s=48&d=mm&r=g">Aadit</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-659456">January 17, 2020 at 2:38 pm</a></p>
<p>Hello Adrian,<br>Can we use this code in rasberry pi to implement face recognition?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-671610">January 23, 2020 at 9:32 am</a></p>
<p> If you want to perform face recognition on the RPi, <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/25/raspberry-pi-face-recognition/">read this tutorial</a> or my book, <a target="_blank" rel="noopener" href="https://pyimagesearch.com/raspberry-pi-for-computer-vision/">Raspberry Pi for Computer Vision.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/6702095f5a6461768ed0aabd6a05943c?s=48&d=mm&r=g">Vinit Shetye</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-686075">January 29, 2020 at 3:12 am</a></p>
<p>Hey Adrian. Thank you for this amazing tutorial. Loved it. I’m working on a project where I’m supposed to recognize the helmet when riders ride the bike. if helmet not recognize then kill switch off bike not working</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-688992">January 30, 2020 at 8:41 am</a></p>
<p> I would recommend using a deep learning-based object detector for that, such as Faster R-CNN, SSD, or RetinaNet. You can use those models to detect the helmet. I cover them inside <a target="_blank" rel="noopener" href="https://pyimagesearch.com/deep-learning-computer-vision-python-book/">Deep Learning for Computer Vision with Python.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/6e77761fad03a1e454df6815264f8442?s=48&d=mm&r=g">Martin</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-696742">February 3, 2020 at 12:21 pm</a></p>
<p>Hi Adrian<br>is it possible to use a Siamese network to recognize the face?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-703791">February 5, 2020 at 2:02 pm</a></p>
<p> Yes, absolutely. I would suggest you read up on siamese networks, triplet loss, and one-shot learning.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/578843c9c21df64ad2125b6e2f0a20c6?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="http://.../">faiz____</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-722611">February 12, 2020 at 5:27 am</a></p>
<p>thankyou so much, you are like hero to me.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-725624">February 13, 2020 at 10:58 am</a></p>
<p> Thank you for the kind words.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/f4511fd4e33e8f21cdfa8ca676cf35c2?s=48&d=mm&r=g">Sanju</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-722901">February 12, 2020 at 7:48 am</a></p>
<p>Hi Adrian,</p>
<p>What if we want to include the images which belong to some other person apart from the faces present in the dataset ? Do we have to train the model again to recognize that newly added face??</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/10a8bbb79635e9e760d01b81ba7e321b?s=48&d=mm&r=g">someone</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-728400">February 14, 2020 at 12:14 pm</a></p>
<p>This might be too broad of a question, but: how do I improve the rejection rate of unknown faces?<br>I currently have two faces trained, but, running some video data, other persons come very close to my comfort limit.<br>I have about 30-80 pictures trained for each face, but not aligned, in various lighting environments. They are pretty low res, trained from the same camera that does the recognition (~640-720p).</p>
<p>Perhaps over-training raises the risk of false positives? Should different lighting be trained with a different label? (e.g. IR vs daylight)</p>
<p>Should unknown persons be put into a different folder? Into several different folders? Currently I have no such folder in my training set, just the faces I want to detect.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-746995">February 20, 2020 at 9:42 am</a></p>
<p> If you have enough training data you may want to consider training a siamese network with triplet loss — doing so would likely improve the face recognition accuracy.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d15ec2cd8fac468e2fb54d5b77c95cec?s=48&d=mm&r=g">Satyam</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-728550">February 14, 2020 at 1:06 pm</a></p>
<p>Sir where and how to change the hyperparameters i.e; C value as mentioned in ur post to improve the system.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/288d90d121caeb77304b1ed2ff7adb84?s=48&d=mm&r=g">sinem</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-765982">March 11, 2020 at 8:38 am</a></p>
<p>Hi Adrian 🙂<br>How can I use Python and OpenCV to find facial similarity?</p>
<p>I’ve successfully used OpenCV and Python to extract faces from multiple photographs using Haar Cascades.</p>
<p>I now have a directory of images, all of which are faces of different people.</p>
<p>What I’d like to do is take a sample image, and then see which face it most looks like.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766034">March 11, 2020 at 4:43 pm</a></p>
<p> Yes, but I would recommend you follow <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/">this guide on face recognition.</a> Extract the 128-d feature vector for each face and then compute the Euclidean distance between the faces. Faces with smaller distances are considered “more similar”.</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/288d90d121caeb77304b1ed2ff7adb84?s=48&d=mm&r=g">sinem</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766253">March 12, 2020 at 6:33 pm</a></p>
<p>  Thank you very much Adrian. Can I do it using the LBPH algorithm, which is the facial recognition algorithm included in OpenCv?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p>  <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766939">March 19, 2020 at 10:02 am</a></p>
<p>  Yes, but your accuracy won’t be as good as using the deep learning-based face recognition methods covered here.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/24876b2979b22a6e65c5532910d6e9f2?s=48&d=mm&r=g">sepideh</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766350">March 14, 2020 at 5:42 am</a></p>
<p>Hi Adrian<br>Thank you very much for your complete code and description.<br>I wondering to khow , how many face can recognize by this code?<br>and store how many person identity?<br>is this good for a university recognition system?<br>I would be very happy if you could introduce a code or article that could recognition many faces (for university or big company)</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/1e23615ff3bb76e04e57182ce176f7a7?s=48&d=mm&r=g">Phil</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766833">March 18, 2020 at 8:27 pm</a></p>
<p>For those who are using sklearn v.0.22, there was a change in the library recently that yields the error: AttributeError: ‘SVC’ object has no attribute ‘_n_support’</p>
<p>An explanation is found here:<br><a target="_blank" rel="noopener" href="https://github.com/scikit-learn/scikit-learn/issues/15902">https://github.com/scikit-learn/scikit-learn/issues/15902</a></p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/1e23615ff3bb76e04e57182ce176f7a7?s=48&d=mm&r=g">Phil</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766834">March 18, 2020 at 8:28 pm</a></p>
<p> To fix the error, you can either revert to &lt;0.22 or retrain the model and then run the recognize[_video].py</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-766918">March 19, 2020 at 9:43 am</a></p>
<p> Thanks for sharing, Phil!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/57d78783fffdd7596755a2ef730f7171?s=48&d=mm&r=g">Venkat Mukthineni</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-767465">March 24, 2020 at 9:14 am</a></p>
<p>Hey Adrian! Great job</p>
<p>I would like to know how to extract the bounding boxes of recognized or unrecognized face from video stream/ image. I want the face in bounding box to get saved in a folder. I would like to extend the project with google reverse image search (on unrecognized faces)</p>
<p>Thank you</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-767646">March 25, 2020 at 1:34 pm</a></p>
<p> That’s absolutely possible, but I get the impression that you may be trying to “run before you walk”. Take the time to learn the basics of OpenCV first, including the “cv2.imwrite” function and basic NumPy array slicing/cropping. <a target="_blank" rel="noopener" href="https://pyimagesearch.com/practical-python-opencv/">Practical Python and OpenCV</a> will help you with just that.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/1bb95587f5169c3b2a69b3e092309778?s=48&d=mm&r=g">Ayoub</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-768266">March 29, 2020 at 5:28 am</a></p>
<p>Hello Adrian , please how can i create an embeddings.pickle and le.pickle for my dataset , thanks !</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-769546">April 1, 2020 at 9:35 am</a></p>
<p> Follow the steps in this tutorial as I show you how to run the Python scripts used to generate those files.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/e7723b7db5de3fe3440336123c921ed5?s=48&d=mm&r=g">Yasser</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-768284">March 29, 2020 at 1:25 pm</a></p>
<p>HI…</p>
<p>Are the openface and face_recognition different models? or they work together?<br>I am confused between them</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/23090779df0bfe147c3a96d6ab304cb5?s=48&d=mm&r=g"><a target="_blank" rel="noopener" href="https://www.motovegan.co.id/">Kaldu Jamur</a></p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-769416">March 31, 2020 at 10:14 pm</a></p>
<p>Thank you Adrian, this really helped me. Always success ..</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-769525">April 1, 2020 at 9:22 am</a></p>
<p> Thanks Kaldu!</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/b08290e8d027c109c801846e91820980?s=48&d=mm&r=g">Barry McQuain</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-769656">April 1, 2020 at 5:04 pm</a></p>
<p>Hi,</p>
<p>Great Post!! Everything works as planned, but I have a few questions:</p>
<ol>
<li><p>I get slightly different percentages compared to your demo (ie, 51.03% v 47.09%). It still works, but my results are not exactly identical using the zipped data and code with no changes. Any idea why that might be? It’s not a big deal, just curious.</p>
</li>
<li><p>I added my seven family members, 12 pictures of each, plus another 10 or so random unknowns to dataset to train and a few of each as the images to test. My results are about 50/50 in terms of identifying the correct person.</p>
</li>
<li><p>When it correctly identifies a person, the highest percentage I see is about 35%, and whether correct or not, I see all of my results in the 15% to 35% range. Is this to be expected? Without my new data, the original tutorial of just you and your wife seemed to give results more in the 40% to 60% range.</p>
</li>
<li><p>After adding the new dataset pics above, I do step #1 to extract the 128D data. Then step #2 to retrain. The retraining appears to happen almost instantly, takes less than 1 second. Is it really re-training? I would have expected the retraining to take longer?</p>
</li>
<li><p>You refer to a process whereby I can “retrain” or “fine-tune” – can you give a little more detail about how I can do that? I would like to create a sample of 30 people in my dataset and retrain on just those 30 (with of course a few random ones too). Is this possible?</p>
</li>
<li><p>I left the original pics of you and your wife in my dataset when I added my seven family members. But now, it doesn’t recognize you (adrian.jpg now only gives me a 17.29% and tells me you are now unknown).</p>
</li>
<li><p>I am a high school teacher, and I would like to show my class these results (and I believe a few are ready to learn this themselves.) I have 30 students. Approx how many training pictures of each do you think I will need? Per the above at the present time, I am only 50% correct in identifying the correct person given a dataset of 10 unique people with 12 pics of each.<br>That isn’t high enough for me to show them. Do you think it is possible, using this tutorial example, that I could get something closer to 95% correct identification of those 30 students (or in my test above, closer to 95% correct identification of my seven family members?</p>
</li>
</ol>
<p>Thanks v much!!</p>
<p>B</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-772538">April 9, 2020 at 9:32 am</a></p>
<p> Hey Barry, I’m happy to help, but as I’ve <a target="_blank" rel="noopener" href="https://pyimagesearch.com/faqs/">mentioned on my FAQs page</a>, kindly keep your comments to one question at a time. I receive 250+ emails per day and 100s of blog post comments. I try my best to get to them all, but multipart questions, especially seven of them, isn’t something I can do. If you want more detailed help <a target="_blank" rel="noopener" href="https://pyimagesearch.com/books-and-courses/">kindly become a customer first</a> and then I can help with these longer questions. I hope you understand that I’m trying to “do the most good” and get to as many people as possible, but I can’t possibly get to everyone with these types of comments.</p>
<p> Feel free to pick one of the above questions you want me to answer though.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d429d37d2c16d200b0158406ddcfe287?s=48&d=mm&r=g">Thabang</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-770766">April 6, 2020 at 10:09 am</a></p>
<p>Hi Adrian. Great tutorial. “deeply” enjoyed it. Thanks.</p>
<p>Is it possible to adapt the code to say; If the person in the frame is recognised then they have access to a room? How can i go about to do that?</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-772483">April 9, 2020 at 9:10 am</a></p>
<p> I would suggest looking into “smart locks”. Some have APIs that you can integrate with and could then “unlock” when a face is recognized.</p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/7101c780da72f6f509d2cfc7171b2501?s=48&d=mm&r=g">erol</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-774664">April 10, 2020 at 1:19 pm</a></p>
<p>hi adrian<br>I am doing a face recognition project with raspberry pi for my school project. The door will be unlocked as the face recognition status. I need to transfer the image taken from the camera to the computer and open and lock the door upon request. How can I make this connection</p>
<ul>
<li><p><img src="https://secure.gravatar.com/avatar/02743529311d3b8babbaf6935670ec9c?s=48&d=mm&r=g">Adrian Rosebrock</p>
<p> <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-785498">April 16, 2020 at 8:07 am</a></p>
<p> I suggest using <a target="_blank" rel="noopener" href="https://pyimagesearch.com/2019/04/15/live-video-streaming-over-network-with-opencv-and-imagezmq/">ImageZMQ.</a></p>
</li>
</ul>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/d4a406b3b129e421976211e1373f3f4c?s=48&d=mm&r=g">Jestin</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-786277">April 16, 2020 at 3:54 pm</a></p>
<p>Hi Adrian,</p>
<p>Firstly, thanks for all the amazing content!<br>I’d like to understand if there is any specific reason for using OpenCV’s face detector instead of dlib’s?<br>I’m curious since you have used dlib’s face detector in your other blog on training a custom dlib shape predictor.</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/b584aa14257c001aaefc221984ab71fa?s=48&d=mm&r=g">Nel</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-786405">April 16, 2020 at 5:04 pm</a></p>
<p>Thanks for the article Adrian,</p>
<p>I am working on a project about face recognition in an uncontrolled environment. So, I need a model to detect unknown person, who I don’t have any pictures of in the dataset. I mean, their picture isn’t even in the unknow folder and my model should label them “unknown”. How should I do that?</p>
<p>Thanks</p>
</li>
<li><p><img src="https://secure.gravatar.com/avatar/5141893eba3d2baa314e61d812e8fae0?s=48&d=mm&r=g">Talpe Loyange</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/2018/09/24/opencv-face-recognition/#comment-787370">April 17, 2020 at 6:46 am</a></p>
<p>Have you used both machine language and deep learning for this project and for what ?Can you explain about that</p>
</li>
</ol>
<h4 id="Comment-section"><a href="#Comment-section" class="headerlink" title="Comment section"></a>Comment section</h4><p>Hey, Adrian Rosebrock here, author and creator of PyImageSearch. While I love hearing from readers, a couple years ago I made the tough decision to no longer offer 1:1 help over blog post comments.</p>
<p>At the time I was receiving 200+ emails per day and another 100+ blog post comments. I simply did not have the time to moderate and respond to them all, and the sheer volume of requests was taking a toll on me.</p>
<p>Instead, my goal is to <em>do the most good</em> for the computer vision, deep learning, and OpenCV community at large by focusing my time on authoring high-quality blog posts, tutorials, and books/courses.</p>
<p><strong>If you need help learning computer vision and deep learning, <a target="_blank" rel="noopener" href="https://pyimagesearch.com/books-and-courses/">I suggest you refer to my full catalog of books and courses</a></strong> — they have helped tens of thousands of developers, students, and researchers <em>just like yourself</em> learn Computer Vision, Deep Learning, and OpenCV.</p>
<p><a target="_blank" rel="noopener" href="https://pyimagesearch.com/books-and-courses/">Click here to browse my full catalog.</a></p>

      

      
        <div class="page-reward">
          <a href="javascript:;" class="page-reward-btn tooltip-top">
            <div class="tooltip tooltip-east">
            <span class="tooltip-item">
              赏
            </span>
            <span class="tooltip-content">
              <span class="tooltip-text">
                <span class="tooltip-inner">
                  <p class="reward-p"><i class="icon icon-quo-left"></i>谢谢你请我吃糖果<i class="icon icon-quo-right"></i></p>
                  <div class="reward-box">
                    
                    
                  </div>
                </span>
              </span>
            </span>
          </div>
          </a>
        </div>
      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/Log//" class="article-tag-list-link color4">Log</a>
        		</li>
      		
		</ul>
	</div>


      

      
        
<div class="share-btn share-icons tooltip-left">
  <div class="tooltip tooltip-east">
    <span class="tooltip-item">
      <a href="javascript:;" class="share-sns share-outer">
        <i class="icon icon-share"></i>
      </a>
    </span>
    <span class="tooltip-content">
      <div class="share-wrap">
        <div class="share-icons">
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="icon icon-weibo"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="icon icon-weixin"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="icon icon-qq"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="icon icon-douban"></i>
          </a>
          <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a>
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="icon icon-facebook"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="icon icon-twitter"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="icon icon-google"></i>
          </a>
        </div>
      </div>
    </span>
  </div>
</div>

<div class="page-modal wx-share js-wx-box">
    <a class="close js-modal-close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//pan.baidu.com/share/qrcode?url=http://example.com/2022/02/27/20220227/" alt="微信分享二维码">
    </div>
</div>

<div class="mask js-mask"></div>
      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

  
<nav id="article-nav">
  
    <a href="/2022/03/03/20220303/" id="article-nav-newer" class="article-nav-link-wrap">
      <i class="icon-circle-left"></i>
      <div class="article-nav-title">
        
          20220303
        
      </div>
    </a>
  
  
    <a href="/2022/02/26/20220226/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">20220226</div>
      <i class="icon-circle-right"></i>
    </a>
  
</nav>


<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
        <div class="toc-container tooltip-left">
            <i class="icon-font icon-category"></i>
            <div class="tooltip tooltip-east">
                <span class="tooltip-item">
                </span>
                <span class="tooltip-content">
                    <div class="toc-article">
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Enable-two-finger-scroll-via-Settings-in-Windows-10"><span class="toc-number">1.</span> <span class="toc-text">Enable two-finger scroll via Settings in Windows 10</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#OpenCV-Ref"><span class="toc-number">2.</span> <span class="toc-text">OpenCV Ref</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Extract-faces-from-images"><span class="toc-number">3.</span> <span class="toc-text">Extract faces from images</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Extracting-faces-using-OpenCV-Face-Detection-Neural-Network"><span class="toc-number">3.1.</span> <span class="toc-text">Extracting faces using OpenCV Face Detection Neural Network</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Aim"><span class="toc-number">3.1.1.</span> <span class="toc-text">Aim</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Project"><span class="toc-number">3.1.2.</span> <span class="toc-text">Project</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Import-libraries"><span class="toc-number">3.1.2.1.</span> <span class="toc-text">Import libraries</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Define-paths-and-load-model"><span class="toc-number">3.1.2.2.</span> <span class="toc-text">Define paths and load model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Create-directory"><span class="toc-number">3.1.2.3.</span> <span class="toc-text">Create directory</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Read-images"><span class="toc-number">3.1.2.4.</span> <span class="toc-text">Read images</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Detect-faces"><span class="toc-number">3.1.2.5.</span> <span class="toc-text">Detect faces</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Create-boxes-around-faces"><span class="toc-number">3.1.2.6.</span> <span class="toc-text">1. Create boxes around faces</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Extracting-faces"><span class="toc-number">3.1.2.7.</span> <span class="toc-text">2. Extracting faces</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusion"><span class="toc-number">3.1.3.</span> <span class="toc-text">Conclusion</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#OpenCV-Face-Recognition"><span class="toc-number">3.2.</span> <span class="toc-text">OpenCV Face Recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Looking-for-the-source-code-to-this-post"><span class="toc-number">3.2.0.0.1.</span> <span class="toc-text">Looking for the source code to this post?</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#OpenCV-Face-Recognition-1"><span class="toc-number">3.2.1.</span> <span class="toc-text">OpenCV Face Recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#How-OpenCV%E2%80%99s-face-recognition-works"><span class="toc-number">3.2.1.1.</span> <span class="toc-text">How OpenCV’s face recognition works</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Our-face-recognition-dataset"><span class="toc-number">3.2.1.2.</span> <span class="toc-text">Our face recognition dataset</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Project-structure"><span class="toc-number">3.2.1.3.</span> <span class="toc-text">Project structure</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-1-Extract-embeddings-from-face-dataset"><span class="toc-number">3.2.1.4.</span> <span class="toc-text">Step #1: Extract embeddings from face dataset</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#import-the-necessary-packages"><span class="toc-number">3.3.</span> <span class="toc-text">import the necessary packages</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#construct-the-argument-parser-and-parse-the-arguments"><span class="toc-number">3.4.</span> <span class="toc-text">construct the argument parser and parse the arguments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-detector-from-disk"><span class="toc-number">3.5.</span> <span class="toc-text">load our serialized face detector from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-embedding-model-from-disk"><span class="toc-number">3.6.</span> <span class="toc-text">load our serialized face embedding model from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#grab-the-paths-to-the-input-images-in-our-dataset"><span class="toc-number">3.7.</span> <span class="toc-text">grab the paths to the input images in our dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#initialize-our-lists-of-extracted-facial-embeddings-and"><span class="toc-number">3.8.</span> <span class="toc-text">initialize our lists of extracted facial embeddings and</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#corresponding-people-names"><span class="toc-number">3.9.</span> <span class="toc-text">corresponding people names</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#initialize-the-total-number-of-faces-processed"><span class="toc-number">3.10.</span> <span class="toc-text">initialize the total number of faces processed</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#loop-over-the-image-paths"><span class="toc-number">3.11.</span> <span class="toc-text">loop over the image paths</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dump-the-facial-embeddings-names-to-disk"><span class="toc-number">3.12.</span> <span class="toc-text">dump the facial embeddings + names to disk</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-2-Train-face-recognition-model"><span class="toc-number">3.12.0.1.</span> <span class="toc-text">Step #2: Train face recognition model</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#import-the-necessary-packages-1"><span class="toc-number">3.13.</span> <span class="toc-text">import the necessary packages</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#construct-the-argument-parser-and-parse-the-arguments-1"><span class="toc-number">3.14.</span> <span class="toc-text">construct the argument parser and parse the arguments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-the-face-embeddings"><span class="toc-number">3.15.</span> <span class="toc-text">load the face embeddings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#encode-the-labels"><span class="toc-number">3.16.</span> <span class="toc-text">encode the labels</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#train-the-model-used-to-accept-the-128-d-embeddings-of-the-face-and"><span class="toc-number">3.17.</span> <span class="toc-text">train the model used to accept the 128-d embeddings of the face and</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#then-produce-the-actual-face-recognition"><span class="toc-number">3.18.</span> <span class="toc-text">then produce the actual face recognition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#write-the-actual-face-recognition-model-to-disk"><span class="toc-number">3.19.</span> <span class="toc-text">write the actual face recognition model to disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#write-the-label-encoder-to-disk"><span class="toc-number">3.20.</span> <span class="toc-text">write the label encoder to disk</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-3-Recognize-faces-with-OpenCV"><span class="toc-number">3.20.0.1.</span> <span class="toc-text">Step #3: Recognize faces with OpenCV</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#import-the-necessary-packages-2"><span class="toc-number">3.21.</span> <span class="toc-text">import the necessary packages</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#construct-the-argument-parser-and-parse-the-arguments-2"><span class="toc-number">3.22.</span> <span class="toc-text">construct the argument parser and parse the arguments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-detector-from-disk-1"><span class="toc-number">3.23.</span> <span class="toc-text">load our serialized face detector from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-embedding-model-from-disk-1"><span class="toc-number">3.24.</span> <span class="toc-text">load our serialized face embedding model from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-the-actual-face-recognition-model-along-with-the-label-encoder"><span class="toc-number">3.25.</span> <span class="toc-text">load the actual face recognition model along with the label encoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-the-image-resize-it-to-have-a-width-of-600-pixels-while"><span class="toc-number">3.26.</span> <span class="toc-text">load the image, resize it to have a width of 600 pixels (while</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#maintaining-the-aspect-ratio-and-then-grab-the-image-dimensions"><span class="toc-number">3.27.</span> <span class="toc-text">maintaining the aspect ratio), and then grab the image dimensions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#construct-a-blob-from-the-image"><span class="toc-number">3.28.</span> <span class="toc-text">construct a blob from the image</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#apply-OpenCV%E2%80%99s-deep-learning-based-face-detector-to-localize"><span class="toc-number">3.29.</span> <span class="toc-text">apply OpenCV’s deep learning-based face detector to localize</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#faces-in-the-input-image"><span class="toc-number">3.30.</span> <span class="toc-text">faces in the input image</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#loop-over-the-detections"><span class="toc-number">3.31.</span> <span class="toc-text">loop over the detections</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#show-the-output-image"><span class="toc-number">3.32.</span> <span class="toc-text">show the output image</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#BONUS-Recognize-faces-in-video-streams"><span class="toc-number">3.32.0.1.</span> <span class="toc-text">BONUS: Recognize faces in video streams</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#import-the-necessary-packages-3"><span class="toc-number">3.33.</span> <span class="toc-text">import the necessary packages</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#construct-the-argument-parser-and-parse-the-arguments-3"><span class="toc-number">3.34.</span> <span class="toc-text">construct the argument parser and parse the arguments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-detector-from-disk-2"><span class="toc-number">3.35.</span> <span class="toc-text">load our serialized face detector from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-our-serialized-face-embedding-model-from-disk-2"><span class="toc-number">3.36.</span> <span class="toc-text">load our serialized face embedding model from disk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#load-the-actual-face-recognition-model-along-with-the-label-encoder-1"><span class="toc-number">3.37.</span> <span class="toc-text">load the actual face recognition model along with the label encoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#initialize-the-video-stream-then-allow-the-camera-sensor-to-warm-up"><span class="toc-number">3.38.</span> <span class="toc-text">initialize the video stream, then allow the camera sensor to warm up</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#start-the-FPS-throughput-estimator"><span class="toc-number">3.39.</span> <span class="toc-text">start the FPS throughput estimator</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#loop-over-frames-from-the-video-file-stream"><span class="toc-number">3.40.</span> <span class="toc-text">loop over frames from the video file stream</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#stop-the-timer-and-display-FPS-information"><span class="toc-number">3.41.</span> <span class="toc-text">stop the timer and display FPS information</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#do-a-bit-of-cleanup"><span class="toc-number">3.42.</span> <span class="toc-text">do a bit of cleanup</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Drawbacks-limitations-and-how-to-obtain-higher-face-recognition-accuracy"><span class="toc-number">3.42.0.1.</span> <span class="toc-text">Drawbacks, limitations, and how to obtain higher face recognition accuracy</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#You-may-need-more-data"><span class="toc-number">3.42.0.1.1.</span> <span class="toc-text">You may need more data</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Perform-face-alignment"><span class="toc-number">3.42.0.1.2.</span> <span class="toc-text">Perform face alignment</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Tune-your-hyperparameters"><span class="toc-number">3.42.0.1.3.</span> <span class="toc-text">Tune your hyperparameters</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Use-dlib%E2%80%99s-embedding-model-but-not-it%E2%80%99s-k-NN-for-face-recognition"><span class="toc-number">3.42.0.1.4.</span> <span class="toc-text">Use dlib’s embedding model (but not it’s k-NN for face recognition)</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Did-you-encounter-a-%E2%80%9CUSAGE%E2%80%9D-error-running-today%E2%80%99s-Python-face-recognition-scripts"><span class="toc-number">3.42.0.2.</span> <span class="toc-text">Did you encounter a “USAGE” error running today’s Python face recognition scripts?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Alternative-OpenCV-face-recognition-methods"><span class="toc-number">3.42.0.3.</span> <span class="toc-text">Alternative OpenCV face recognition methods</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#What%E2%80%99s-next-I-recommend-PyImageSearch-University"><span class="toc-number">3.42.0.4.</span> <span class="toc-text">What’s next? I recommend PyImageSearch University.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary"><span class="toc-number">3.42.1.</span> <span class="toc-text">Summary</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Download-the-Source-Code-and-FREE-17-page-Resource-Guide"><span class="toc-number">3.42.1.0.1.</span> <span class="toc-text">Download the Source Code and FREE 17-page Resource Guide</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#About-the-Author"><span class="toc-number">3.42.1.0.2.</span> <span class="toc-text">About the Author</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reader-Interactions"><span class="toc-number">3.42.2.</span> <span class="toc-text">Reader Interactions</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#pip-install-OpenCV"><span class="toc-number">3.42.2.1.</span> <span class="toc-text">pip install OpenCV</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Install-OpenCV-4-on-your-Raspberry-Pi"><span class="toc-number">3.42.2.2.</span> <span class="toc-text">Install OpenCV 4 on your Raspberry Pi</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#359-responses-to-OpenCV-Face-Recognition"><span class="toc-number">3.42.2.3.</span> <span class="toc-text">359 responses to: OpenCV Face Recognition</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#do-a-bit-of-cleanup-1"><span class="toc-number">3.43.</span> <span class="toc-text">do a bit of cleanup</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Comment-section"><span class="toc-number">3.43.0.1.</span> <span class="toc-text">Comment section</span></a></li></ol></li></ol></li></ol></li></ol>
                    </div>
                </span>
            </div>
        </div>
        
    </div>
</aside>



  
  
  

  

  

  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2022 Hongtu Liu
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>



<script src='https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js'></script>

<script>
    if (window.mermaid) {
        mermaid.initialize({"startOnload":true});
    }
</script>


    </div>
    <script>
	var yiliaConfig = {
		mathjax: false,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&laquo; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &raquo;</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>



{% if theme.mermaid.enable %}
  <script src='https://unpkg.com/mermaid@{{ theme.mermaid.version }}/dist/mermaid.min.js'></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({{ JSON.stringify(theme.mermaid.options) }});
    }
  </script>
{% endif %}

    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">All Items</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">Links</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">About Me</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">QR Code</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">周易大师</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Hexo Note</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">PHMG</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Smallpdf</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">p62 Draft</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">HPV30</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Nrf2</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">HPV</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">离线语音助手</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">FTP connection</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Tags in Hexo</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Special symbols</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Exiftool</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">FTP</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">English Correction online</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Image attached</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Legado</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Document Clean-up</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Document Organization</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Root Explorer</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">RE</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">ES File Explorer</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">SRA</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">integrity</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">vdb-validate</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Macro in Linux WPS</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Hantavirus evolution</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">LibreOffice</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">OpenOffice</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Apache OpenOffice</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">zcat</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">ICTV</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">virus</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">International Committee on Taxonomy of Viruses (ICTV)</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">hexo index</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">hexo content</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Comments in Rmd</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">BaiduNetDisk</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">SRA analysis</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">HLA-G analysis</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">HEV Sequence Download</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Yin Wenjiao</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Guo Yifan</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">E-MTAB-1733</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">AnyDesk update</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Usage of my mobile device</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Windows Docker Machine</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">container</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Tips of Hexo GitHub Pages</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Build Github Pages With Jekyll</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">SDS-PAGE</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Protein Marker</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">txid</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">ETE/ETE3</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Minoxidil</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">KinhDown</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">pan.baidu.com</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">SRA tumor cell lines</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Zhang Li</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">p62</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">E6</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">E7</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">wget</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Ji Tianjiao</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Size reduction of PDF</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">tBHQ</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Tert-Butylhydroquinone</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Authentication Required</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Gene Report</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">BioGPS</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">rsync</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">SRA-Tools</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Endometrial Cancer</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Wang Jiao</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">miRBase</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">RNA-Seq</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Sex Determination</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">endometrial cancer</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">HLA-G</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">RGB color</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">VPN</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">AppImage</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Outline</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Watch charging</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">代码的魅力</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Flatpak</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Shotcut</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">NSFC</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">E-MTAB-173</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Essential</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Kaiju</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Phage RnD</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Mobile phone</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">NCBI Datasets</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Hexo categories</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">COVID-19 Data Download</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Glossary</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">QQ浏览器</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Word</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Flu Biosafety</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">EBV Detection</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Hexo Categories</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">HEV</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">pdftk</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">PDF password</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">MUSCLE</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">MSA</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">ACNUC</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Microsoft Office</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Bacteriophage</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">ContraFect</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Intralytix</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">phagelux</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">菲吉乐科</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">COVID-19  Vaccine</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Backup</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Nrf2-HSV</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">HSV Exosome cn draft</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">B95-8_Nrf2 project</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">B95-8-Nrf2 project</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Offline map</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Google Patents</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">File manager</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Reactive Oxygen Species</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Herpesviridae sequence download</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">HFMD</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">STGCN Model</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Offline Wikipedia</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">flatpak</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Installation Log in X1</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Virus proliferation cell free system</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">CEB</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">COVID-19 Vaccine</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">PDF_Learning</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">R update</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">CRAN update</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Cairo-Dock</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Scheme of README documents</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Bacteriophage database</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Thesis Nrf2</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">凉山项目</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">凉山</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Hexo</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">HSV</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">CRAN</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Monkey B virus</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">中文语音识别</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">speech recognition</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">山西疾控</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">审计</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Antibody Purification</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Water</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">scrcpy</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">ADB</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">七千人大会</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">算命</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Long COVID-19</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">COVID-19</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Vaccine</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">恢复真实容量</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Pen drive Encrypting</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Figma</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Chinese conversion</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">OpenCC</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">中文简繁转换</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Collison</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Nebulizer</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">气溶胶发生器</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">openvpn</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">openvpn3</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">WINE</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">疾病负担</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">BOD</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Burden of Disease</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">pH meter</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Potassium Chloride</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">KCl</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Android File Transfer</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">miR</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">HSV-1</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Exosome</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">miR prediction</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Primer organization</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">R</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Gist</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">国家自然科学基金</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">NFS</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Kiwix</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Wikipedia</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">HGNC</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">file extension</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">RNA-Seq pipeline</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">HEV sequence download</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">HEVnet</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">CSV</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Excel</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Sheet</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Tree structure</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Markdown syntax</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">西部世界</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">IME in Ubuntu</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Markdown Mermaid examples</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Markdown mermaid</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">flowchart</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">MarkText</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Small RNA</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">PDF Note</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Markdown</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Text Editor</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Obsidian</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Gingko</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Manuskript</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">FocusWriter</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Typora</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">TreeSheets</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Zettlr</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Protein-Protein Docking</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">PPI</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Margarine</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Protein Assay</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Proteomics</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">GanttProject</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Tasker</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Shortcuts</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Agent</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">氢原子</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">氢自由基</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">My Linux</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Qv2ray</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">HIV</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Scheme</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">HD Management</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">README</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">HD Structure</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="https://www.ncbi.nlm.nih.gov/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>Old entry NCBI</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.nejm.org/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>The New England J of Medicine</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.thelancet.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>Lancet</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.nature.com/nature/current-issue" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>Nature</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.sciencemag.org/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>Science</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.cell.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>Cell</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.pnas.org/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>PNAS</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.jbc.org/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>JBC</a>
            </li>
          
            <li class="search-li">
              <a href="https://rupress.org/jcb" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>JCB</a>
            </li>
          
            <li class="search-li">
              <a href="https://clinicaltrials.gov/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>ClinicalTrials.gov</a>
            </li>
          
            <li class="search-li">
              <a href="http://192.168.1.10:8000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>MarginNote 3</a>
            </li>
          
            <li class="search-li">
              <a href="https://my.pcloud.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>pCloud</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">Step by step.</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>